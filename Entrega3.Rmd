---
title: "Census Income Dataset"
author: "Carles Cidraque i Eduard Cidraque"
date: \today
output:
  word_document:
    toc: yes
    toc_depth: '4'
  html_document:
    toc: no
    toc_depth: '4'
  pdf_document:
    latex_engine: pdflatex
    number_sections: yes
    toc_depth: 4
    toc: yes
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 9pt
subtitle: 'Data Quality Assessment'
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Lab0_LoadingData
##Loading data
```{r, echo=FALSE}
setwd("C:/Users/charl/OneDrive/Documents/ADEI")

df<-read.table("adult.data",header=F, sep=",", fill=FALSE,strip.white=TRUE,na.string="?")

summary(df) #Here we have 32561 obs. of 15 variables
dim(df)
head(df)

```
# Load samples
```{r}
#Load samples
###Use one birthday of a group member
set.seed(14031997) 
llista<-sample(1:nrow(df),5000)
llista<-sort(llista)
llista[1:10]

df<-df[llista,]

rm(list=c("llista"))
save.image("E1.RData")
```


#Lab1_PreparingData
```{r, echo=FALSE}
# Load libraries - Set global variables

options(contrasts=c("contr.treatment","contr.treatment"))
install.packages("pscl", repos = "https://cran.rstudio.com")
options(repos="https://cran.rstudio.com" )
requiredPackages<-c("car","FactoMineR","missMDA","chemometrics")
install.packages(requiredPackages)
lapply(requiredPackages, require, character.only = TRUE)
```

```{r, include=FALSE, echo=FALSE}
# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","effects","FactoMineR","car","factoextra","RColorBrewer","ggplot2","dplyr","knitr","chemometrics")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]

if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)

```

```{r, include=FALSE}
# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("ROCR","effects","FactoMineR","car","lmtest", "factoextra","ggplot2","knitr")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]

if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)

```

##Load sample
```{r}
# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

setwd("C:/Users/charl/OneDrive/Documents/ADEI")
filepath<-"C:/Users/charl/OneDrive/Documents/ADEI/"
load(paste0(filepath,"E1.RData"))

# Give meaningful column names
summary(df)
names(df)[c(1:15)]<-c("age","workclass","fnlwgt","education","education.num","marital.status","occupation","relationship","race","sex","capital.gain","capital.loss","hours.per.week","native.country","Y.bin")
```
##Load Some useful functions for data quality assessment

```{r, echo=FALSE}

# Some useful functions
calcQ <- function(x) {
  s.x <- summary(x)
  iqr<-s.x[5]-s.x[2]
  list(souti=s.x[2]-3*iqr, mouti=s.x[2]-1.5*iqr, min=s.x[1], q1=s.x[2], q2=s.x[3], 
       q3=s.x[5], max=s.x[6], mouts=s.x[5]+1.5*iqr, souts=s.x[5]+3*iqr ) }

countNA <- function(x) {
  mis_x <- NULL
  for (j in 1:ncol(x)) {mis_x[j] <- sum(is.na(x[,j])) }
  mis_x <- as.data.frame(mis_x)
  rownames(mis_x) <- names(x)
  mis_i <- rep(0,nrow(x))
  for (j in 1:ncol(x)) {mis_i <- mis_i + as.numeric(is.na(x[,j])) }
  list(mis_col=mis_x,mis_ind=mis_i) }

countX <- function(x,X) {
  n_x <- NULL
  for (j in 1:ncol(x)) {n_x[j] <- sum(x[,j]==X) }
  n_x <- as.data.frame(n_x)
  rownames(n_x) <- names(x)
  nx_i <- rep(0,nrow(x))
  for (j in 1:ncol(x)) {nx_i <- nx_i + as.numeric(x[,j]==X) }
  list(nx_col=n_x,nx_ind=nx_i) }

```

# Univariate descriptive analysis
## Define new factors by recoding and grouping the original levels
## Factor preparation
### workclass
```{r}
##EDA
summary(df$workclass)
```
#Alhora de fer agrupacions per noves categories en les diferents variables hem de tenir en compte tant el sentit lògic de la variable com la relació de cada variable amb el target numèric(intentar no perdre la relació amb el target numèric).S'intentarà sempre que sigui llògic agrupar categories que tinguin mean hours.per.week semblants.
```{r}
table(df$workclass)
#tant per cent de les categories
100*table(df$workclass)/nrow(df)
#Graphic tools
barplot(table(df$workclass),main="Workclass observations",col="Red")
##Ara pasem a la definició de nou factor
df$f.type<-5
tapply(df$hours.per.week,df$workclass,mean)
```
# Never-worked removed from the sample ja que no ens interesen amb el nostre target numèric hours.per.week
```{r}
ll<-which(df$workclass=="Never-worked");length(ll)
if( length(ll)>0) df<-df[-ll,]

# New levels: 1-civil, 2-private, 3-self-emp-inc, 4-self-emp-not-inc, 5-Other
levels(df$workclass)
ll<-which(df$workclass=="Private");length(ll)
df$f.type[ll]<-2

ll<-which((df$workclass=="Self-emp-inc"));length(ll)  # Direct

ll<-which(as.numeric(df$workclass)%in%c(5));length(ll) # Faster
df$f.type[ll]<-3

ll<-which(as.numeric(df$workclass)%in%c(6));length(ll) # Faster
df$f.type[ll]<-4

ll<-which(as.numeric(df$workclass)%in%c(1,2,7));length(ll) # Faster
df$f.type[ll]<-1


df$f.type<-factor(df$f.type,labels=paste0("f.typ-",c("civil","private","self-emp-inc","self-emp-not-inc","other")))
summary(df$f.type)
```

### relationship
```{r}
##EDA
table(df$relationship)
#tant per cent de les categories
100*table(df$relationship)/nrow(df)
#Graphic tools
barplot(table(df$relationship),main="relationship observations",col="Red")
##Ara pasem a la definició de nou factor
df$f.RelType<-4
tapply(df$hours.per.week,df$relationship,mean)
```
Creem 4 nivells agrupant les categories per significat pero també respecte el comportament amb el target
```{r}
# New ralationship levels: 1-rel-husband, 2-rel-WifeOther, 3-rel-Child, 4-Rel-NotFamily
ll<-which(df$relationship=="Husband");length(ll)
df$f.RelType[ll]<-1

ll<-which((df$relationship=="Wife")|((df$relationship=="Other-relative")));length(ll) 
df$f.RelType[ll]<-2

ll<-which(df$relationship=="Own-child");length(ll)
df$f.RelType[ll]<-3

ll<-which((df$relationship=="Not-in-family")|((df$relationship=="Unmaried")));length(ll)
df$f.RelType[ll]<-4

df$f.RelType<-factor(df$f.RelType,labels=paste0("f.Reltyp-",c("rel-husband","rel-WifeOther","rel-Child","Rel-NotFamily")))
summary(df$f.RelType)
```
### native.country
```{r}
##EDA
summary(df$native.country)
#tant per cent de les categories
100*table(df$native.country)/nrow(df)
#Graphic tools
barplot(table(df$native.country),main="native.country observations",col="Red")
```
##Ara pasem a la definició de nou factor
#Els paisos que no tenen cap observació no tenen sentit pel nostre estudi, llavors fem:

```{r}
df$native.country<-factor(df$native.country)
df$f.CountryType<-2
# New native.country levels: 1-USA, 2-other
tapply(df$hours.per.week,df$native.country,mean)

ll<-which(df$native.country=="United-States");length(ll)
df$f.CountryType[ll]<-1

ll<-which((df$native.country=="Canada") | (df$native.country=="Cuba")| (df$native.country=="Dominican-republic")| (df$native.country=="Ecuador")| (df$native.country=="El-Salvador")| (df$native.country=="Guatemala")| (df$native.country=="Haiti")| (df$native.country=="Honduras")| (df$native.country=="Jamaica")| (df$native.country=="Mexico")| (df$native.country=="Outlying-US(Guam-USVI-etc)")| (df$native.country=="Nicaragua")| (df$native.country=="Peru")| (df$native.country=="Puerto-Rico")| (df$native.country=="Trinadad&Tobago")| (df$native.country=="Columbia")|(df$native.country=="England")|(df$native.country=="France")|(df$native.country=="Germany")|(df$native.country=="Greece")|(df$native.country=="Hungary")|(df$native.country=="Ireland")|(df$native.country=="Italy")|(df$native.country=="Poland")|(df$native.country=="Portugal")|(df$native.country=="Scotland")|(df$native.country=="Yugoslavia")|(df$native.country=="Cambodia")|(df$native.country=="Iran")|(df$native.country=="India")|(df$native.country=="Laos")|(df$native.country=="Japan")|(df$native.country=="Philippines")|(df$native.country=="South")|(df$native.country=="Taiwan")|(df$native.country=="Thailand")|(df$native.country=="Vietnam"));length(ll)
df$f.CountryType[ll]<-2

barplot(table(df$f.CountryType),main="f.CountryType observations",col="Red")

df$f.CountryType<-factor(df$f.CountryType,labels=paste0("f.Countrytyp-",c("USA","other")))
summary(df$f.CountryType)
```
### education
```{r}
##EDA
summary(df$education)
#tant per cent de les categories
100*table(df$education)/nrow(df)
#Graphic tools
barplot(table(df$education),main="education observations",col="Red")
##Ara pasem a la definició de nou factor
df$f.EduType<-8
tapply(df$hours.per.week,df$education,mean)
# New education levels: 1-Dropout, 2-Associates, 3-HS-Graduate, 4-Prof-school, 5-Masters, 6-Bachelors, 7-Colleges, 8-Doctorate

ll<-which((df$education=="10th")|(df$education=="11th")|(df$education=="12th")|(df$education=="9th")|(df$education=="11th")|(df$education=="1st-4th")|(df$education=="5th-6th")|(df$education=="7th-8th")|(df$education=="Preschool"));length(ll)

df$f.EduType[ll]<-1

ll<-which((df$education=="Assoc-acdm")|(df$education=="Assoc-voc"));length(ll)

df$f.EduType[ll]<-2

ll<-which((df$education=="HS-grad"));length(ll)

df$f.EduType[ll]<-3

ll<-which((df$education=="Prof-school"));length(ll)

df$f.EduType[ll]<-4

ll<-which((df$education=="Masters"));length(ll)

df$f.EduType[ll]<-5

ll<-which((df$education=="Bachelors"));length(ll)

df$f.EduType[ll]<-6

ll<-which((df$education=="Some-college"));length(ll)

df$f.EduType[ll]<-7

ll<-which((df$education=="Doctorate"));length(ll)

df$f.EduType[ll]<-8

df$f.EduType<-factor(df$f.EduType,labels=paste0("f.Edutyp-",c("Dropout","Associates","HS-Graduate","Prof-school","Masters","Bachelors","Colleges","Doctorate")))
summary(df$f.EduType)

```
### marital.status
```{r}
##EDA
summary(df$marital.status)
tapply(df$hours.per.week,df$marital.status,mean)
#tant per cent de les categories
100*table(df$marital.status)/nrow(df)
#Graphic tools
barplot(table(df$marital.status),main="marital.status observations",col="Red")
```
##Ara pasem a la definició de nou factor
##Juntem Divorced i Separeted amb també Married-spouse-absent perque a part de que tenint en compte el target tenen similituds, amb la terminologia, un spouse-absent es aquella persona que no viu amb la parella.
##Veiem que la categoria Married-AF-spouse només té una observació i a més te una relació amb el target de hours.per.week de només 14 hores, molt inferior a totes les altres categories, cosa que faria erronia el agrupar aquesta observació amb alguna altre categoria. Al haver-hi només una decidim eliminar-la de la mostra.
```{r}
ll<-which(df$marital.status=="Married-AF-spouse");length(ll)
if( length(ll)>0) df<-df[-ll,]
```

```{r}
df$f.MaritalStatusType<-4
# New marital.status levels: 1-Married-civ-spouse, 2-Never-Married, 3-Not-with-partner, 4-Widowed
ll<-which((df$marital.status=="Married-civ-spouse"));length(ll)
df$f.MaritalStatusType[ll]<-1

ll<-which((df$marital.status=="Never-married"));length(ll)
df$f.MaritalStatusType[ll]<-2

ll<-which((df$marital.status=="Divorced")|(df$marital.status=="Separated")|(df$marital.status=="Married-spouse-absent"));length(ll)

df$f.MaritalStatusType[ll]<-3

ll<-which((df$marital.status=="Widowed"));length(ll)
df$f.MaritalStatusType[ll]<-4

df$f.MaritalStatusType<-factor(df$f.MaritalStatusType,labels=paste0("f.MaritalStatustyp-",c("Married-civ-spouse","Never-Married","Not-with-partner","Widowed")))

summary(df$f.MaritalStatusType)

```
### occupation
```{r}
##EDA
summary(df$occupation)
#tant per cent de les categories
100*table(df$occupation)/nrow(df)
#Graphic tools
barplot(table(df$occupation),main="occupation observations",col="Red")
```
##Ara pasem a la definició de nou factor
#NA's no es tenen en compte en la factorització, tenim 9 OccupationType, pero no ubiquem els NA en cap OccupationType
```{r}
df$f.OccupationType<-9
# New Occupation levels: 1-Admin, 2-Emp-BaixCarrec, 3-Military, 4-Other-Ocupation, 5-Professional, 6-Sales, 7-Services, 8-Emp-AltCarrec
ll<-which((df$occupation=="Adm-clerical"));length(ll)
df$f.OccupationType[ll]<-1

ll<-which((df$occupation=="Craft-repair")|(df$occupation=="Farming-fishing")|(df$occupation=="Handlers-cleaners")|(df$occupation=="Machine-op-inspct")|(df$occupation=="Transport-moving"));length(ll)

df$f.OccupationType[ll]<-2

ll<-which((df$occupation=="Armed-Forces"));length(ll)
df$f.OccupationType[ll]<-3

ll<-which((df$occupation=="Tech-support")|(df$occupation=="Protective-serv"));length(ll)

df$f.OccupationType[ll]<-4

ll<-which((df$occupation=="Prof-specialty"));length(ll)

df$f.OccupationType[ll]<-5

ll<-which((df$occupation=="Sales"));length(ll)

df$f.OccupationType[ll]<-6

ll<-which((df$occupation=="Priv-house-serv")|(df$occupation=="Other-service"));length(ll)

df$f.OccupationType[ll]<-7

ll<-which((df$occupation=="Exec-managerial"));length(ll)

df$f.OccupationType[ll]<-8

df$f.OccupationType<-factor(df$f.OccupationType,labels=paste0("f.Occupationtyp-",c("Admin","Emp-BaixCarrec","Military","Other-Ocupation", "Professional","Sales","Services","Emp-AltCarrec")))

summary(df$f.OccupationType)
```
### race
```{r}
##EDA
summary(df$race)
tapply(df$hours.per.week,df$race,mean)
#tant per cent de les categories
100*table(df$race)/nrow(df)
#Graphic tools
barplot(table(df$race),main="race observations",col="Red")
##Ara pasem a la definició de nou factor
df$f.RaceType<-3
levels(df$race)
```
# New Occupation levels: 1-White, 2-Black, 3-Other
#No fem el factor junt de Black i Other pq creiem que la població Black se l'ha de separar pq te una hours.per.week en la que veiem que es troba per sota de les altres categories que no es diferencient tant entre elles. Encara que les diferencies entre White i altres races que no siguin Black la hours.per.week es similar pero com a levels no les podriem juntar perque tenen un significat diferent en aquesta categoria race.
```{r}
ll<-which((df$race=="White"));length(ll)

df$f.RaceType[ll]<-1

ll<-which((df$race=="Black"));length(ll)

df$f.RaceType[ll]<-2

ll<-which((df$race=="Amer-Indian-Eskimo")|(df$race=="Asian-Pac-Islander")|(df$race=="Other"));length(ll)

df$f.RaceType[ll]<-3

df$f.RaceType<-factor(df$f.RaceType,labels=paste0("f.Racetyp-",c("White","Black","Other")))

summary(df$f.RaceType)
tapply(df$hours.per.week,df$f.RaceType,mean)
```
### Sex
```{r}
#Unicament fem EDA, no creem nou factor
summary(df$sex)
#tant per cent d'homes i dones
100*table(df$sex)/nrow(df)


#Graphic tools
barplot(table(df$sex),main="Gender observations",col="Red")
```

##Aqui ja hem acabat de factoritzar les variables categoriques (no numeriques)

## Define new factors by discretization of numeric variables
```{r}
vars_con<-names(df)[c(1,5,11,12,13)]#variables numeriques a discretitzar
vars_con
```
### Age
```{r}
summary(df$age)
# sd(df$age) = standard deviation - var(): variance
sd(df$age)
```
#standard deviation = 13.46751, com més alta sigui sd vol dir que les dades s'extenen sobre un rang de valors més ampli.
```{r}
hist(df$age,freq=F,breaks=30,col="orange",main="Histograma Age") 
```
# Proportions
##Gràcies a l'hist veiem que no segueix una distribució normal
#Veiem que es una distribució asimètrica perque la mean i la median no són la mateixa, visualment ho podem observar perque té una cua més llarga per la dreta. Podem afirmar que no és una variable que segueix una distribució normal(si fos normal el histograma ens sortiria amb forma de campana de Gauss).
```{r}
mm<-mean(df$age);ss<-sd(df$age);mm;ss
```
##Ara volem solapar la mean i median d'age com si fós normal, el que volem fer es que sobre el perfil (de distribució de prop.) volem veure si aquest perfil es compatible amb una distr.normal, doncs solapem el perfil que haurien de tenir unes dades normals amb la mean i sd que tenim.
```{r}
#curve(dnorm(x,mm,ss),col="red",lwd=2,add=T)
```
#Veiem que el perfil de age no s'ajusta al perfil d'una normal estandard. No segueix una distribució simetrica, i així es imposible encaixar-ho en un perfil normal.
#la x indica que sha de fer amb el rang de valors de la variable original

#info amb els percentils (seq crea una llista q comença en el 0, acaba en el 1 i va de 0.1 en 0.1)
```{r}
quantile(df$age,probs=seq(0,1,0.1))
```
 # decils (de 10 en 10)
#Veiem que entre 58 anys i 90 anys representen el 10%, per tant seria útil agrupar-los com fem a continuació, per quantiles no per percentiles(es poc informatiu amb percentil per veureho de manera visual i explicativa, han d'estar ben distribuits els quantils, amb el màxim de igual distribució entre quantils). Els intervals han d'estar igualment plens, no ens interesa tenir una mala distribució dels intervals

#Definim una nova variable f.age per tal de visualitzar millor la mostra
```{r}
df$f.age<-factor(cut(df$age,breaks=quantile(df$age),include.lowest = T))
```
# Customization: it depends on variable meaning
##Veiem que ens donen breaks amb rangs de nombres poc memorístics relatius a la edat. Millor posar-ne uns de més lògics.
```{r}
df$f.age<-factor(cut(df$age,breaks=c(17,30,40,50,90),include.lowest = T))
```
##Ara ens preguntem quina es la mean de les edats en cada un dels grups definits del factor. Apliquem tapply i mirem 
```{r}
tapply(df$age,df$f.age,median)
```
##Podem veure que els intervals estan bastant equilibrats, això ens agrada, sobretot en els tres primers, l'últim ((50,90])) pero ho no està tant d'equilibrat.

#Redefinim les etiquetes en qüestio
```{r}
df$f.age<-factor(df$f.age,labels=paste0("f.age-",levels(df$f.age)))
summary(df$f.age)
```
### capital.gain
```{r}
summary(df$capital.gain)
souts<-summary(df$capital.gain)[5] #outlier superior
souts 
souti<-summary(df$capital.gain)[2] #outlier inferior
souti
quantile(df$capital.gain,seq(0,1,by=0.1))

#Desviacio tipus
sd(df$capital.gain)

#Variança
var(df$capital.gain)

#Coefficient of variation
sd(df$capital.gain)/mean(df$capital.gain)

#Graphical tools
hist(df$capital.gain,freq=FALSE,30,main="capital.gain histogram")
#Solapem amb normal
curve(dnorm(x,mean(df$capital.gain),sd(df$capital.gain)),col="red", lwd=3,add=TRUE)
```
#Com podem veure capital.gain es una variable bastant particular, ja que fent les tres sentencies anteriors veiem que tenim valors molt alts com els de 99999 (valor que no es creible), pot ser una codificació de dada faltant que hem d'identificar i hem de posar-li el codi de NA.
```{r}
#Outlier Detection
boxplot(df$capital.gain,main="Boxplot capital.gain")
outsev<-souts+3*(souts-souti)
outsua<-souts+1.5*(souts-souti)
outsev
outsua

abline(h=outsua,col="blue",lwd=2,lty=2)
abline(h=outsev,col="red",lwd=2,lty=2)
quantile(df$capital.gain,probs=seq(0,1,0.1))


ll<-which(df$capital.gain==99999);length(ll)
```
#En aquesta llista ll hi tenim tots aquells capital.gain que tenen com a valor 99999, a la nostra mostra en concret hi tenim 25; les codifiquem com a NA.
```{r}
df$capital.gain[ll]<-NA

summary(df$capital.gain)
##La mean no ens diu massa cosa (hi ha masses 0s), doncs mirem els percentils.
quantile(df$capital.gain,probs=seq(0,1,0.1),na.rm=T)
```
##Ara ja no tenim problemes amb els NA's ja que no els tenim en compte (na.rm=T), veiem que el 90% tenen un capital.gain = 0, i un 10% que han de tenir un capital.gain > 0. Tot això fa que aquesta variable sigui de difícil discretització. Fer 4 o 5 categories com hem fet amb altres variables no té cap sentit, hi haurà una categoria(la de capital.gain=0) que serà molt pesant. Per tant millor en aquest cas fer dues categories, dient que el capital.gain=0 o capital.gain>0.
# It seems 2 levels is enough (capital.gain=0 o capital.gain>0)
```{r}
df$f.cgain<-factor(cut(df$capital.gain,breaks=c(0,1,50000),include.lowest = T))
summary(df$f.cgain)
df$f.cgain<-factor(df$f.cgain,labels=c("f.cgain-No","f.cgain-Yes"))
plot(df$f.cgain)
```
### capital.loss
```{r}
summary(df$capital.loss)
quantile(df$capital.loss,probs=seq(0,1,0.1))
```
##Per la variable capital.loss seguirem un procediment similar al que hem fet amb capital.gain. Es semblant, pero en aquest cas no tenim missings (NA's). Per tant ens dona menys feina a fer i es una discretització més ràpida.

# It seems 2 levels is enough
```{r}
df$f.closs<-factor(cut(df$capital.loss,breaks=c(0,1,4000),include.lowest = T))
summary(df$f.closs)
df$f.closs<-factor(df$f.closs,labels=c("f.closs-No","f.closs-Yes"))
plot(df$f.closs)
```
### Defining new variable (capital.var)
#Després de la discretització de capital.gain i capital.loss, podem veure una característica interesant en aquestes dades i es que si calculem la taula de contingència corresponent a capital.gain i capital.loss veiem que no hi ha cap persona en la mostra que tingui capital.gain>0 i capital.loss>0.
```{r}
table(df$f.cgain,df$f.closs)
```
#Per tant, o guanyen o perden diners. Alhesores fa que la creació d'una nova variable tingui sentit; la variable variació de capital (tindrà valor negatiu o positiu).

##Definim aquesta nova variable numerica capital.var
```{r}
df$capital.var<-df$capital.gain-df$capital.loss
```
## Discretization: f.capvar  3 levels (els que guanyen, els que perden i els que es queden igual de capital)
```{r}
quantile(df$capital.var,probs=seq(0,1,0.1),na.rm=T)
df$f.cvar<-factor(cut(df$capital.var,breaks=c(-3000,-0.00001,0,40000),include.lowest = T))
df$f.cvar<-factor(df$f.cvar,labels=c("f.cvar.loss","f.cvar.equal","f.cvar.gain"))
summary(df$f.cvar)
##Veiem que a la categoria on tenim més observacions es a en la que tenen un capital.var = 0
```
### education.num
```{r}
quantile(df$education.num,probs=seq(0,1,0.1))
```
#Veiem que hi ha aprox. un 50% d'observacions que es troben entre un education.num de 9 i 10. I la mean es de 10.07. Calculem ara la seva sd
```{r}
sd(df$education.num)
```
#Una sd baixa indica que els data points tendeixen a estar a prop del mean value. En education.num tenim una sd de 2.596706, podem afirmar que s'acosta més a una distribució normal del que ho fa per exemple la variable "age".
```{r}
hist(df$education.num,freq=F,breaks=20,col="orange",main="Histograma Education.num")
mm<-mean(df$education.num);ss<-sd(df$education.num);mm;ss
#Fem la comparació amb la normal tal i com hem fet amb la variable "age"
curve(dnorm(x,mm,ss),col="red",lwd=2,add=T)
```
# Customization: it depends on variable meaning
##Veiem que la variable education.num te relació amb la variable education; es pot observar que la majoria d'observacions 
```{r}
df$f.education.num<-factor(cut(df$education.num,breaks=quantile(df$education.num),include.lowest = T))
summary(df$f.education.num)
```
#Veiem que hi ha intervals sobrecarregats comparats amb altres, intentem que estiguin més o menys igualats.
```{r}
df$f.education.num<-factor(cut(df$education.num,breaks=c(1,8,9,12,16),include.lowest = T))
df$f.education.num<-factor(df$f.education.num,labels=c("f.education.num(1-8)","f.education.num(9)","f.education.num(10-12)","f.education.num(13-16)"))
```

### hours.per.week
```{r}
summary(df$hours.per.week)
```
##Els 99 semblen codificacions d'error
##Gràcies al histograma veiem que la majoria d'individus està sobre les 40 hours.per.weeek.
```{r}
hist(df$hours.per.week,30)
```
##Fem un histograma amb bastanats intervals i veiem que es més una variable discreta que una de continua. Veiem valors com el 99 que són codificacions errors o missings, l'hem de detectar i suprimir. Aqui no podem posar NA, ja que estem tractant la variable target numerica, no li podem aplicar un procediment d'imputació, seria falsejar el target, que es molt més crític que les variables explicatives del nostre estudi.
```{r}
ll<-which(df$hours.per.week==99);length(ll)
if( length(ll)>0) df<-df[-ll,]
quantile(df$hours.per.week,probs=seq(0,1,0.1))
```
#Només prenem com a data errònia els hours.per.week = 99, les altres les hem considerat casos posibles.
##Veiem que hi ha aprox. un 65% d'observacions amb un hours.per.week entre 40 i 55.
```{r}
df$f.hours.per.week<-factor(cut(df$hours.per.week,c(1,39,45,98),include.lowest = T)) 
levels(df$f.hours.per.week)<-paste0("hours.per.week-",levels(df$f.hours.per.week))
```

## Numeric summary and graphic support
## Per variables numeriques
Les descriptive variables s'ha fet el numeric summary i graphic support dins l'apartat anterior de Factor preparation, aqui només tractem les variables numeriques.
Encara que hem vist alguns dels indicadors numerics o gràfics de les variables que veurem a continuació en l'apartat de discretització de variables numeriques, aqui els tornem a mostrar i n'afegim algún més per tal d'estudiar millor la variable.
A més a més es fa un petit apartat en cada variable per la detecció inicial d'outliers abans d'entrar en l'apartat de Data Quality posterior.
### age
```{r}
#Numeric indicators - statistics
summary(df$age)
```
#A continuació el que fem es agafar el 3rd Q. i el 1st Q. Aquests ens serviran més tard per calcular el iqr i poder detectar outliers.
```{r}
souts<-summary(df$age)[5] #outlier superior
souts 
souti<-summary(df$age)[2] #outlier inferior
souti
quantile(df$age,seq(0,1,by=0.1))

#Desviacio tipus
sd(df$age)

#Variança
var(df$age)
```
#Coefficient of variation: A major valor de coeficient de variació major heterogeneïtat dels valors de la variable; i a menor coeficient de variació, major homogeneïtat en els valors de la variable.
#Només utilitzarem amb variables que sempre siguin positives i per tant que la seva mitjana també ho sigui
```{r}
sd(df$age)/mean(df$age)

#Graphical tools
hist(df$age,freq=FALSE,30,main="Age histogram")

#Solapem amb normal
curve(dnorm(x,mean(df$age),sd(df$age)),col="red", lwd=3,add=TRUE)

#Outlier Detection
boxplot(df$age,main="Boxplot age", ylim=c(0,120))
outsev<-souts+3*(souts-souti)
outsua<-souts+1.5*(souts-souti)
outsev
outsua

abline(h=outsua,col="blue",lwd=2,lty=2)
abline(h=outsev,col="red",lwd=2,lty=2)
```
#Calculem els valors a partir dels quals trobarem els outliers severes i outliers suaus. Per als suaus fem la mesura típica de multiplicar 1.5 per el iqr, que es souts-souti, i per trobar els severes multipliquem per 3.

### capital.gain

#Més ja s'ha realitzat l'estudi de la variable capital.gain

### capital.loss
```{r}
#Numeric indicators - statistics

summary(df$capital.loss)
souts<-summary(df$capital.loss)[5] #outlier superior
souts 
souti<-summary(df$capital.loss)[2] #outlier inferior
souti
quantile(df$capital.loss,seq(0,1,by=0.1))

#Desviacio tipus
sd(df$capital.loss)

#Variança
var(df$capital.loss)

#Coefficient of variation
sd(df$capital.loss)/mean(df$capital.loss)

#Graphical tools
hist(df$capital.loss,freq=FALSE,30,main="capital.loss histogram")

#Solapem amb normal
curve(dnorm(x,mean(df$capital.loss),sd(df$capital.loss)),col="red", lwd=3,add=TRUE)

#Outlier Detection
boxplot(df$capital.loss,main="Boxplot capital.loss")
outsev<-souts+3*(souts-souti)
outsua<-souts+1.5*(souts-souti)
outsev
outsua

abline(h=outsua,col="blue",lwd=2,lty=2)
abline(h=outsev,col="red",lwd=2,lty=2)
```
### education.num
```{r}
#Numeric indicators - statistics

summary(df$education.num)
souts<-summary(df$education.num)[5]
souts
souti<-summary(df$education.num)[2]
souti
quantile(df$education.num,seq(0,1,by=0.1))

#Desviacio tipus
sd(df$education.num)

#Variança
var(df$education.num)

#Coefficient of variation
sd(df$education.num)/mean(df$education.num)

#Graphical tools
hist(df$education.num,freq=FALSE,30,main="education.num histogram", col=rainbow(30))
mitjana<-mean(df$education.num)
desv_est<-sd(df$education.num)
mitjana
desv_est

#FER VALORACIO DE SI SEGUEIX UNA DISTRIBUCIO NORMAL
curve(dnorm(x,mean=mitjana,sd=desv_est),col="magenta", lwd=3,add=TRUE)

#Outlier Detection
boxplot(df$education.num,main="Boxplot education.num")

outsev<-souts+3*(souts-souti)
outsua<-souts+1.5*(souts-souti)
outsev
outsua

abline(h=outsua,col="orange",lwd=2,lty=2)
abline(h=outsev,col="red",lwd=2,lty=2)
```

### hours.per.week
```{r}
#Numeric indicators - statistics

summary(df$hours.per.week)
souts<-summary(df$hours.per.week)[5]
souts
souti<-summary(df$hours.per.week)[2]
souti
quantile(df$hours.per.week,seq(0,1,by=0.1))

#Desviacio tipus
sd(df$hours.per.week)

#Variança
var(df$hours.per.week)

#Coefficient of variation
sd(df$hours.per.week)/mean(df$hours.per.week)

#Graphical tools
hist(df$hours.per.week,freq=FALSE,30,main="hours.per.week histogram", col=rainbow(30))
mitjana<-mean(df$hours.per.week)
desv_est<-sd(df$hours.per.week)
mitjana
desv_est

#FER VALORACIO DE SI SEGUEIX UNA DISTRIBUCIO NORMAL
curve(dnorm(x,mean=mitjana,sd=desv_est),col="magenta", lwd=3,add=TRUE)

#Outlier Detection
boxplot(df$hours.per.week,main="Boxplot hours.per.week")
summary(df$hours.per.week)

outsev<-souts+3*(souts-souti)
outsua<-souts+1.5*(souts-souti)
outsev
outsua

abline(h=outsua,col="orange",lwd=2,lty=2)
abline(h=outsev,col="red",lwd=2,lty=2)
```

# Data Quality
## Comptem el nombre d'NA's per variable
```{r}
#Primer es calcula el numero total de missings del df.
miss<-countNA(df)
```
#countNA ens genera dues llistes que a continuació utilitzarem, la llista miss_ind i la miss_col
```{r}
totalMiss<-sum(miss$mis_ind)##Numero total de missing values a la mostra
totalMiss ##De moment 755
attributes(miss)
```
#un cop tenim el numero total de missings del df, mitjançant mis_col ens permet veure el numero de missings per cada una de les variables.
#Podem veure el resum en la taula de continuació (les variables que tenen més missing són workclass, education amb 282 missings)
```{r}
miss$mis_col
table(miss$mis_col)
```
##Veiem per exemple que hi ha un total de 282 individus que tenen missings en 2 variables.
```{r}
summary(miss$mis_col)
```
## Comptem el nombre d'NA's per individu
```{r}
#Primer es calcula el numero total de missings del df.
miss<-countNA(df)
attributes(miss)
```
#Mitjançant mis_ind aconseguim el numero de missings per individu, el output que obtenim es el següent, on cada element de la llista es el numero de missings per individu (les 1000 primeres observacions).
```{r}
miss$mis_ind
```
##Gracies a la comanda table podem obtenir el resulat d'una forma més visual i simplifocada, ens surten el numero d'individus que tenen 0,1,2,3,4 o 5 missings en el general de les seves variables.
```{r}
table(miss$mis_ind)
```
##inicials desde aquest punt (arribats en aquest punt ja s'han tractat alguns com capital.gain = 99999), després en el tractament d'erros i outliars en poden aparèixer més
#   0    1    2    3    4    5 
#4596   86  278    4   24    1 
#Podem veure que hi ha un total de 393 individus que tenen almenys 1 missing.
```{r}
summary(miss$mis_ind)
```

## Tractament d'outliers, errors i missings per variable
Els outliers només seràn tractats per les variables numeriques.
## Outliers i Errors: creem dos vectors(un per les variables i l'altre per els individus), per tal de comptar els outliers i els errors. Les j's son per les variables i les i's per els individus.
```{r}
iout<-rep(0,nrow(df)) #numero d'outliers
jout<-rep(0,length(vars_con))

ierr<-rep(0,nrow(df))
jerr<-rep(0,ncol(df))

```
##Utilitzarem el boxplot de la llibreria car
##Els outliers severs depenent de la variable es poden esborrar, tot i que per regla els convertim en NA.
```{r}
library(car)
```

### hours.per.week (numeric target)
```{r}
boxplot(df$hours.per.week,main="Boxplot de hours.per.week",col="red")
barplot(table(df$hours.per.week))
calcQ(df$hours.per.week)
```
#calQ calcula els tresholds pels outliers suaus i els severs, de totes maneres aqui sota els calculem d'una manera una mica més rudimentaria, aixi també ens ajuda a nosaltres a entendre millor el càlcul i el significat d'aquests valors.
```{r}
summ<-summary(df$hours.per.week)
summ
iqr<-summ[5]-summ[2]
#calculem els superiors
outsevS<-summ[5]+3*(iqr) #limit outliers severs superiors
outsuaS<-summ[5]+1.5*(iqr) #limit outliers suaus superiors
outsevS
outsuaS
#calculem els inferiors
outsevI<-summ[2]-3*(iqr) #limit outliers severs inferiors
outsuaI<-summ[2]-1.5*(iqr) #limit outliers suaus inferiors
outsevI
outsuaI
```
## Ja vam detectar i eliminar algun outlier/error de hours.per.week a "simple vista" anteriorment; aquells hours.per.week = 99.
```{r}
ll<-which((df$hours.per.week < outsuaI)|(df$hours.per.week > outsuaS)) #calcul del numero de outliers tant suaus com severs
length(ll)
#Calcul només d'outliers severs
ll<-which((df$hours.per.week < outsevI)|(df$hours.per.week > outsevS))
length(ll)
#Num outliers de hours.per.week:
out.hours.per.week<-length(ll)
out.hours.per.week
```
#Veiem que outsevI es 25 i outsevS es 60. Creiem que aquest aquest interval no s'ajusta a la realitat actual i pot deixar fora a moltes observacions que en realitat si que hauriem de tractar i poden ser d'interès. Per tant decidim crear un nou interval per la detecció d'outliers de hours.per.week.
```{r}
ll<-which((df$hours.per.week < 5)|(df$hours.per.week > 75) & (is.na(df$hours.per.week)!=TRUE))
length(ll)
```
#Esborrem els outliers detectats com a errors ja que és la variable target:
```{r}
df<-df[-ll,]
```
#Veiem que la mostra pasa de tenir 4989 obs a 4913 obs.

##Actualitzem jout o iout amb els outliers detectats
```{r}
jout[13]<-length(ll)

#El nombre d'outliers de la variable és:
jout[13]
##Actualitzem el numero d'outliers dels individus
iout[ll]<-iout[ll] + 1
```
##Els errors detectats son els 11 individus que tenien hours.per.week = 99 detectats en un apartat anterior.

#Calcul de missings
```{r}
na.hours.per.week<-is.na(df$hours.per.week)
table(is.na(df$hours.per.week))
na.hours.per.week<-which(na.hours.per.week!=FALSE)
na.hours.per.week<-length(na.hours.per.week)

#El número de missings de hours.per.week és:
na.hours.per.week

boxplot(df$hours.per.week,main="Boxplot de hours.per.week",col="red")
#tracem sobre el plot, tenir en compte que hem variat els limits severs inferiors i superiors a 5 i 75 respectivament.
abline(h=outsuaS,col="blue",lwd=2,lty=2)
abline(h=outsevS,col="red",lwd=2,lty=2)
abline(h=outsuaI,col="blue",lwd=2,lty=2)
abline(h=outsevI,col="red",lwd=2,lty=2)
```
### age
```{r}
summ<-summary(df$age)
summ
boxplot(df$age,main="Boxplot de age",col="red")
barplot(table(df$age))
##Detecció outliers
calcQ(df$hours.per.week)

iqr<-summ[5]-summ[2]
iqr

#calculem els superiors
outsevS<-summ[5]+3*(iqr) #limit outliers severs superiors
outsuaS<-summ[5]+1.5*(iqr) #limit outliers suaus superiors
outsevS
outsuaS
#calculem els inferiors
outsevI<-summ[2]-3*(iqr) #limit outliers severs inferiors
outsuaI<-summ[2]-1.5*(iqr) #limit outliers suaus inferiors
outsevI
outsuaI

#calcul del numero de outliers totals,suaus i severs:

ll<-which((df$age < outsuaI)|(df$age > outsuaS))
length(ll)
outage<-length(ll)

#Calculem només outliers severs:
ll<-which((df$age < outsevI)|(df$age > outsevS))
length(ll)

#num outliers de age:
outage
```
#Veiem que outsevI es -29 i outsevS es 104. Per tant veiem que prenent aquests dos valors com a llindars pel delimitar outliers es absurd per aquesta variable, a més a més com es obvi obtenim 0 observacions fora d'aquest interval.Creiem que aquest interval no s'ajusta a la realitat de la variable i hauriem d'acotar més l'interval per detectar aquells outliers amb sentit per la nostra mostra. Per tant decidim crear un nou interval per la detecció d'outliers d'age.
```{r}
ll<-which((df$age < 16)|(df$age > 80) & (is.na(df$age)!=TRUE))
length(ll)
##Posem com a NA's els outlier considerats entre 16 i 80:
df$age[ll]<-NA
```
##Detectem un total de 10 observacions que es troben fora l'interval d'entre 17 a 80 anys, que ja els hem posat com a NA's

```{r}
boxplot(df$age,main="Boxplot de age",col="red")
#tracem sobre el plot, tenir en compte que hem variat els limits severs inferiors i superiors a 17 i 80 respectivament. Veiem que només teniem suaus superiors.
abline(h=outsuaS,col="blue",lwd=2,lty=2)
abline(h=outsevS,col="red",lwd=2,lty=2)
abline(h=outsuaI,col="blue",lwd=2,lty=2)
abline(h=outsevI,col="red",lwd=2,lty=2)
```
### education.num
```{r}
summ<-summary(df$education.num)
summ
boxplot(df$education.num,main="Boxplot de education.num",col="red")
barplot(table(df$education.num))
##Detecció outliers
calcQ(df$education.num)

iqr<-summ[5]-summ[2]

#calculem els superiors
outsevS<-summ[5]+3*(iqr) #limit outliers severs superiors
outsuaS<-summ[5]+1.5*(iqr) #limit outliers suaus superiors
outsevS
outsuaS
#calculem els inferiors
outsevI<-summ[2]-3*(iqr) #limit outliers severs inferiors
outsuaI<-summ[2]-1.5*(iqr) #limit outliers suaus inferiors
outsevI
outsuaI

ll<-which((df$education.num < outsuaI)|(df$education.num > outsuaS)) #calcul del numero de outliers entre severs i suaus totals
length(ll)
outedu<-length(ll)
#Calcul d'outliers severs inferiors i superiors
ll<-which((df$education.num < outsevI)|(df$education.num > outsevS))
length(ll) #tenim 0 severs
#Es pot veure que no hi ha cap error pq el max es 16 i el min es 1 que no violen les restriccions de la variable education.num
#Tot i que trobem outliers suaus no considerem posar-los com a NA ja que creiem que es necessari tractar-los i son dades necesaries.
#Num outliers de education.num:
outedu 

boxplot(df$education.num,main="Boxplot de education.num",col="red")
abline(h=outsuaS,col="blue",lwd=2,lty=2)
abline(h=outsevS,col="red",lwd=2,lty=2)
abline(h=outsuaI,col="blue",lwd=2,lty=2)
abline(h=outsevI,col="red",lwd=2,lty=2)

##Actualitzem jout o iout amb els outliers detectats
jout[5]<-length(ll)

#El nombre d'outliers de la variable és:
jout[5]
##Actualitzem el numero d'outliers dels individus
iout[ll]<-iout[ll] + 1

#Calcul de missings

na.education.num<-is.na(df$education.num)
table(is.na(df$education.num))
na.education.num<-which(na.education.num!=FALSE)
na.education.num<-length(na.education.num)

#El numero de missings de la variable education.num és:
na.education.num
```
### capital.gain
```{r}
summ<-summary(df$capital.gain)
summ
```
##Amb el summary veiem que ja tenim 25 NA's alguns d'ells classificats com a tal en l'apartat de discretització de la variable. (En concret aquells de valor 99999, que es veien a simple vista)
```{r}
boxplot(df$capital.gain,main="Boxplot de capital.gain",col="red")
barplot(table(df$capital.gain))
##Detecció outliers
calcQ(df$capital.gain)

iqr<-summ[5]-summ[2]
#calculem els superiors
outsevS<-summ[5]+3*(iqr) #limit outliers severs superiors
outsuaS<-summ[5]+1.5*(iqr) #limit outliers suaus superiors
outsevS
outsuaS
#calculem els inferiors
outsevI<-summ[2]-3*(iqr) #limit outliers severs inferiors
outsuaI<-summ[2]-1.5*(iqr) #limit outliers suaus inferiors
outsevI
outsuaI

ll<-which((df$capital.gain < outsuaI)|(df$capital.gain > outsuaS)) #calcul del numero de outliers entre severs i suaus totals
length(ll)
outedu<-length(ll)
#Calcul d'outliers severs inferiors i superiors
ll<-which((df$capital.gain < outsevI)|(df$capital.gain > outsevS))
length(ll)
#No val la pena aplicara aquest procediment a capital.gain, detectarà molts outliers, en detectem un total de 375 i tots ells son severs. Hem d'adaptar el criteri, no n'hi poden haver tants.
summary(df$capital.gain)

boxplot(df$capital.gain,main="Boxplot de capital.gain",col="red")
abline(h=outsuaS,col="blue",lwd=2,lty=2)
abline(h=outsevS,col="red",lwd=2,lty=2)
abline(h=outsuaI,col="blue",lwd=2,lty=2)
abline(h=outsevI,col="red",lwd=2,lty=2)
```
##Per sentit comú aquesta detecció d'outliers utilitzant aquest interval no es lògica i en definim un altre que tingui més sentit com hem fet amb altres variables anteriorment.

#num outliers de capital.gain
```{r}
outgain<-length(ll)
outgain 

###Detecció outliers amb nou interval
summ
##COMENTAR INTERVAL OUTLIERS
quantile(df$capital.gain,seq(0,1,by=0.1),na.rm=T)
ll<-which((df$capital.gain <0)|(df$capital.gain > 34095) & (is.na(df$capital.gain)!=TRUE))
length(ll) #Veiem doncs tenim outliers amb els nous limits generats per nosaltres de forma més lògica

##Actualitzem jout o iout amb els outliers detectats
jout[11]<-length(ll)

#El nombre d'outliers de la variable és:
jout[11]
##Actualitzem el numero d'outliers dels individus
iout[ll]<-iout[ll] + 1

#Calcul de missings

na.capital.gain<-is.na(df$capital.gain)
table(is.na(df$capital.gain))
na.capital.gain<-which(na.capital.gain!=FALSE)
na.capital.gain<-length(na.capital.gain)

#El numero de missings de la variable capital.gain és:
na.capital.gain
```
### capital.loss
```{r}
summ<-summary(df$capital.loss)
summ
boxplot(df$capital.loss,main="Boxplot de capital.loss",col="red")
barplot(table(df$capital.loss))
##Detecció outliers
calcQ(df$capital.loss)

iqr<-summ[5]-summ[2]
#calculem els superiors
outsevS<-summ[5]+3*(iqr) #limit outliers severs superiors
outsuaS<-summ[5]+1.5*(iqr) #limit outliers suaus superiors
outsevS
outsuaS
#calculem els inferiors
outsevI<-summ[2]-3*(iqr) #limit outliers severs inferiors
outsuaI<-summ[2]-1.5*(iqr) #limit outliers suaus inferiors
outsevI
outsuaI

ll<-which((df$capital.loss < outsuaI)|(df$capital.loss > outsuaS)) #calcul del numero de outliers entre severs i suaus totals
length(ll)
outedu<-length(ll)
#Calcul d'outliers severs inferiors i superiors
ll<-which((df$capital.loss < outsevI)|(df$capital.loss > outsevS))
length(ll)

summary(df$capital.loss)

boxplot(df$capital.loss,main="Boxplot de capital.loss",col="red")
abline(h=outsuaS,col="blue",lwd=2,lty=2)
abline(h=outsevS,col="red",lwd=2,lty=2)
abline(h=outsuaI,col="blue",lwd=2,lty=2)
abline(h=outsevI,col="red",lwd=2,lty=2)

quantile(df$capital.loss,seq(0,1,by=0.01))
summ
```
##Encara que la gent que te capital.loss > 0 sigui aprox. 5% de la mostra creiem que l'hem de tenir en compte pel nostre anàlisi i no poden ser ignorats, per que a més pot tenir repercusions en altres variables com capital.var. Apliquem el mateix procediment que hem aplicat a capital.gain pel nou calcul dels limits d'outliers.
```{r}
##Definim el nou interval
ll<-which((df$capital.loss < 0)|(df$capital.loss > 2824)) #calcul del numero de outliers entre souts i 2824
length(ll)
outloss<-length(ll)
outloss #num outliers de capital.loss

##Actualitzem jout o iout amb els outliers detectats
jout[12]<-length(ll)

#El nombre d'outliers de la variable és:
jout[12]
##Actualitzem el numero d'outliers dels individus
iout[ll]<-iout[ll] + 1

#Calcul de missings
na.capital.loss<-is.na(df$capital.loss)
table(is.na(df$capital.loss))
na.capital.loss<-which(na.capital.loss!=FALSE)
na.capital.loss<-length(na.capital.loss)

#El numero de missings de la variable capital.loss és:
na.capital.loss
```

# Imputation
Per la imputació el que farem es donar valors raonables a les dades que siguin missings.
## Numeric variables
Comencem amb la imputació de les numeric variables
```{r}
#library que fa imputació es missMDA
library(missMDA)
#no podem imputar observacions erroneas o outlayers en el target pq la estariem corrompent, falsejant
vars_con<-names(df)[c(1,5,11,12,13,26)]#variables numeriques, tenim NA a age, capital.gain i capital.var
vars_con
names(df)
summary(df[,vars_con])
```
#inputPCA li pasem  el conjunt de totes les variables que son numeriques pq la imputacio es fara de manera que sigui consistent amb tota la multidimensionalitat, es a dir, amb totes les caracteristiques que tenen els individus globalment.
```{r}
res.impn<-imputePCA(df[,vars_con])
```
#Aquests procediments per molt fonamentats que estiguin no deixa de ser una imputacio, es a dir, posarli un valor que es el mes probable segons les caracteristiques del colectiu, es a dir multivariant que te aquella observacio, pero shan de verificar perque a vegades poden donar coses extranyes.
```{r}
summary(res.impn$completeObs) #Ja no tenim NA's

vars_con_miss<-vars_con[c(1,3,6)]
# Check that you have missing data
summary(df[,vars_con_miss]) # Before, missings en age, capital.gain i capital.var (25 NA's cada una)
summary(res.impn$completeObs[,vars_con_miss]) # After, veiem que els NA's han desaparegut perque ja s'ha fet la imputació
```
#Per totes les variables que tenien missings i ara no, sha de veure l'abans i el després. Quin ha sigut el procediment d'imputació. Que els percentils no han canviat massa i que el perfil de la variable no ha canviat massa tampoc.
# Numeric tools EDA univariate for capital.gain
```{r}
quantile(df$capital.gain,seq(0,1,0.1),na.rm=T)# Before
quantile(res.impn$completeObs[,"capital.gain"],seq(0,1,0.1),na.rm=T) # After
# Graphics for EDA univariate for capital.gain
par(mfrow=c(1,2))#customitzar la sortida grafica per posar una sola fila i dues columnes perque surtin un al costat de l'altre el before i l'after
hist(df$capital.gain)
hist(res.impn$completeObs[,"capital.gain"])
par(mfrow=c(1,1))#Fem tornar al format original

# Numeric tools EDA univariate for capital.var
quantile(df$capital.var,seq(0,1,0.1),na.rm=T)# Before
quantile(res.impn$completeObs[,"capital.var"],seq(0,1,0.1),na.rm=T) # After

# Graphics for EDA univariate for capital.var
par(mfrow=c(1,2))
hist(df$capital.var)
hist(res.impn$completeObs[,"capital.var"])
par(mfrow=c(1,1))

df[,vars_con_miss]<-res.impn$completeObs[,vars_con_miss]
summary(df[,vars_con]) # After

```

## Factor imputation
```{r}
#Primer de tot mirem si tenim missings.
vars_dis<-names(df)[c(2,4,6:10,14,15:25,27:29)]
summary(df[,vars_dis])
vars_dis
vars_dis_miss<-names(df)[c(16,21,18,24,27)] ##f.workclass, f.occupation, f.CountryType, f.cgain, f.cvar
```
#Pels factor s'utilitza el imputMCA (multiple correspondance analysis)
#Per la imputació utilitzem totes les variables discretes, perque volem que sigui una imputació consistent.
```{r}
res.impf<-imputeMCA(df[,vars_dis],ncp=30)
```
#També es pot fer manualment, com per exemple amb la variable native.country, es més evident el que s'hi ha de posar.
# Check consistency of imputation
# For each factor in vars_dis_miss EDA tools (num,graphics)
#Veure la diferencia d'NA's i cap a on han anat
```{r}
#occupation i factor
#table(df$f.OccupationType,useNA="always") # Before
table(res.impf$completeObs[,"f.OccupationType"],useNA="always") # After

#table(df$occupation,useNA="always") # Before
table(res.impf$completeObs[,"occupation"],useNA="always") # After

#native.country i factor
table(df$f.CountryType,useNA="always") # Before
table(res.impf$completeObs[,"f.CountryType"],useNA="always") # After

table(df$native.country,useNA="always") # Before
table(res.impf$completeObs[,"native.country"],useNA="always") # After

#workclass i factor
table(df$f.type,useNA="always") # Before
table(res.impf$completeObs[,"f.type"],useNA="always") # After

table(df$workclass,useNA="always") # Before
table(res.impf$completeObs[,"workclass"],useNA="always") # After

#f.cgain
table(df$f.cgain,useNA="always") # Before
table(res.impf$completeObs[,"f.cgain"],useNA="always") # After

#f.cvar
table(df$f.cvar,useNA="always") # Before
table(res.impf$completeObs[,"f.cvar"],useNA="always") # After

# barplot occupation (AFTER)
barplot(table(df$occupation),main="occupation observations",col="Red")

# barplot native.country (AFTER)
barplot(table(df$native.country),main="native.country observations",col="Red")

# barplot workclass (AFTER)
barplot(table(df$workclass),main="workclass observations",col="Red")

# barplot f.cgain (AFTER)
barplot(table(df$f.cgain),main="f.cgain observations",col="Red")

# barplot f.cvar (AFTER)
barplot(table(df$f.cvar),main="f.cvar observations",col="Red")

# Once imputation has been validated for all factors
df[,vars_dis_miss]<-res.impf$completeObs[,vars_dis_miss]

```
## Multivariate outlier detection: per variables numeriques
No ho tractem en el nostre dataset perque es poc complex, amb pocs valors dintre de una variable molt diferents entre ells i les dades numeriques son "males numeriques", tenim molts valors iguals i per tant, no té gaire sentit.
De totes maneres amb la funció moutlier sobre algunes de les nostres variables numeriques. Ens proporciona dos diagramas, un que mostra per cadascuna de les observacions que tenim a la mostra quina es la seva distancia de normal, i l'altre de la Robust Mahalanobis. Si trobem que hi ha serioses discrepancies entre la que es robusta i la que no es que estem davant d'un outlier multivariant.

# Profiling

## Target numeric: hours per week
```{r}
library(FactoMineR)
#condes(df,num.var=13) # variable target.Dificult interpretation

vars_condes<-names(df)[c(1,2,5,8:10,11,12,13,15:25,27:29)] 
vars_condes  # Check number position
#condes(df[,vars_condes],num.var=9,proba=0.01) #pvalue a partir de 0.01
```
els apartats del output son els seguents:

quanti: vol dir associacio global, de les variables numeriques amb el target hours.per.week. Veiem que hi ha una correlacio de pearson, que es l'estadistic que s'utilitza, i es sempre positiu, vol dir que a més educació aleshores mes hours.per.week, a més edat veiem que també, pero veiem que la intensitat d'aquest coeficient de correlació es molt petita per totes les variables que no son education.num. Ens surten ordenats per p-value, la h0 es que la correlació es = 0. Mirem el p-value que ens surt. Si pvalue es molt petit vol dir que la h0 no es creible, es rebutja, vol dir que tenen relació.

quali: vol dir associacio global entre els cada un dels factors i la variable target. La h0 que s'utilitza en aquest cas es la mitjana de les hores treballades es la mateixa que per totes les categories del factor, si no es compleix vol dir que existeix algun/s que son diferents. Ovbiament el f.hours.per.week es el més alt, no el tenim en compte perque es el seu factor. En el factor relationship (f.relationship) veiem que la mitjana de les hours.per.week es diferent segons el rol que ocupa l'individu a la familia, pvalue molt baix, practicament 0, es rebutja la h0. La llista que ens apareix ens mostra les variables ordenades segons aquestes variables factors segons les diferencies que introdueixen en la mitjana de les hours.per.week.

Important anar mirant cap i cua llista $category, els que estan més per sobre de la mitjana i els que estan més per sota de la mitjana. Per exemple veiem que els que guanyen més de 50K l'any treballen 3.0189035 hours.per.week més per sobre de la mitjana i els que guanyen menys de 50k treballen -3.0189035 hours.per.week per sota de la mitjana.

Amb el condes aconseguim identificar totes les variables numeriques que estan relacionades amb el target numeric globalment, totes les variables factor que estan relacionades globalment amb el target. I després de manera especifica també aconseguim saber quines categories son les que afecten al rebuig d'aquesta h0.
# Manual check. Com a exemple fem amb f.RelType
```{r}
tapply(df$hours.per.week,df$f.RelType,mean)
```
Veiem per exemple cada categoria de del factor f.RelType la mitjana de hours.per.week. Veiem que els que més hours.per.week treballen son els husband.


## Profile categorical target: y.bin
```{r}
summary(df$f.MaritalStatusType)
# vars_catdes or vars_condes is already valid
vars_condes
#catdes(df[,vars_condes],num.var=which(vars_condes=="Y.bin"),proba=0.01)
```
Per exemplificar l'output que ens proporciona en fixem amb marital.status h0: Y.bin i marital.status son caracteristiques independents, si son independents no podem utilitzar marital.status per predir Y.bin, si rebutjem h0 si. h0 es rebutja (pvalue molt proxim a 0, f.MaritalStatusType 3.455187e-199), per tant marital.status està relacionat amb Y.bin globalment. Llavors la qüestió es si hi ha alguna categoria en el marital.status que sigui la que es diferent a la resta.
Relationship, el factor de education.num, el factor de education, l'age com a factor. Totes aquestes variables globalment estan relacionades amb Y.bin (guanyar + o - de 50K $).
El següent pas es descriure cada cluster per les categories. Cluster dels que guanyen + de 50k i els de - de 50k. Ens fixem amb els que guanyen més de 50k $ l'any, ja sabem que marital.status es globalment relacionat, pero hi ha alguna categoria de marital.status que sigui marcadament més caracteritzadora del que li pasa a Y.bin? Si, els que estan casats (Married) el 43% guanyen mes de 50k$ l'any.
Mirar el education.num també pot ser interessant, un 77% del Doctorate cobren més de 50k, en canvi l'altre cas extrem serien els Dropout que només ho fan aprox un 5%.
Home i dones(sex), els homes predominen en guanyar +50K, ja que veiem que els homes son un total del voltant del 30% que guanyen més de 50k i les dones només un 11% d'elles ho fan.

#2 Deliverable

#PCA analysis
# PCA

You have to use FactoMineR library and PCA method for Principal Component Analysis. Output for names in data.frame and vars_con list for numeric variables is shown below.

> names(df)
 [1] "age"            "type.employer"  "fnlwgt"         "education"      "education.num"  "marital"        "occupation"    
 [8] "relationship"   "race"           "sex"            "capital.gain"   "capital.loss"   "hr.per.week"    "country"       
[15] "y.bin"          "f.temployer"    "f.educ"         "f.marital"      "f.ocu"          "f.relship"      "f.race"        
[22] "f.ori"          "f.age"          "f.edunum"       "f.cgain"        "f.closs"        "f.cap"          "capital.change"
[29] "f.hpw"          "mout"          
> vars_con
[1] "age"            "education.num"  "capital.gain"   "capital.loss"   "hr.per.week"    "capital.change"

```{r}
names(df)
vars_con
```

Basic PCA analysis has to be conducted by defining the numeric target hr.per.week as a supplementary qualitative variable.

PCA seria considerar totes les variable numeriques i definir la variable que es el target, que en vars_con(variables numeriques) es el hours.per.week es el 5. Quantitativa suplementaria es el 5 llavors. Pq el target sempre ha de ser suplementari.
```{r}
res.pca<-PCA(df[,vars_con],quanti.sup=5)
summary(res.pca, nb.dec = 2, ncp = 4, nbelements = 3, nbind=0)
```
 I. Eigenvalues and  axes. How many axes we have to interpret? 
Eigenvalues
                      Dim.1  Dim.2  Dim.3  Dim.4  Dim.5
Variance               2.07   1.13   0.96   0.84   0.00
% of var.             41.35  22.61  19.20  16.84   0.00
Cumulative % of var.  41.35  63.96  83.16 100.00 100.00

Utilitzem el criteri de Kaiser
Veiem la primera dimensio, la primera component principal fa una feina com dues de les variables originals. I captura el 41,35% de la variabilitat de les dades. La segona dimensio fa una feina com el 1.13 vegades una variables de les originals, i capta un 22.61% adicional de la variabilitat. Per tant amb les dues primeres dimensions capturem un 63,96% de la variabilitat.
Ara bé, com que es una mica just (el 63,96%) aleshores tenint en compte que la dimensio 3 té un valor propi, un eigenvalue (0,96) que l'hem d'interpretar com la representitivitat que te aquesta variable i llavors la té casi igual que a una de les variables originals, aleshores també l'hauriem de retenir. Per tant retenim les primeres 3 dimensions.
```{r}
res.pca$eig
summary(res.pca) #summary només amb vars_con
barplot(res.pca$eig[,1],main="Eigenvalues",names.arg=paste("dim",1:nrow(res.pca$eig)))
```
Veiem que si sumem els valors propis aleshores ens dona el numero de variables, això es degut a que estem en components principals normalitzades, és a dir, el que estem factoritzant es la matriu de variances i covariances.
 
 
```{r}
sum(res.pca$eig[,1])

# Use modern ggplot facilities

fviz_eig(res.pca, addlabels = TRUE)
fviz_eig(res.pca, choice = "eigenvalue", addlabels=TRUE)

```

Aquestes components principals no son del tot fàcils d'interpretar, per afavorir la interpretació el que fem es incloure variables suplementaries.
Incloem els factors com a variables suplementaries. Coloquem com a factors suplementaris que segur que shan de fer servir, son el el target binari, el f.hours.per.week, i després entre 2 i 4 factors dels que nosaltres hem identificat que estan molt relacionats amb la variable target numerica hours.per.week (f.age, f.MaritalStatusType, f.RelType i f.EduType)

Les variables quali seràn:"Y.bin","f.hours.per.week","f.age","f.MaritalStatusType","f.RelType","f.EduType"
La variable quanti.sup serà: "hours.per.week"
```{r}
names(df)
vars_con
```
ncp es el nombre de dimensions que agafem en els resultat de PCA, per tant posem ncp=3.
```{r}
res.pca<-PCA(df[,c(vars_con,"Y.bin","f.hours.per.week","f.age","f.MaritalStatusType","f.RelType","f.EduType")],quanti.sup=5, quali=7:12,ncp=3 )
```
En el summary es absurd que intentem veure tots els individus on estan projectats (pq en tenim masses), lo més pràctic i perque no ens doni feina la edició es posar nbind=0. I en el nombre de elements (nbelements) es on controlem el nombre de categories de suplementaries que ens fan falta. Més menys de l'ordre de 30.
```{r}
summary(res.pca, nb.dec = 2, ncp = 3, nbelements = 30, nbind=0)
# Let us understand how to customize pca object plotting
plot(res.pca,choix="ind", cex=0.5, col.ind="grey80",label="quali")
#bvbox(cbind(res.pca$ind$coord[,1:3]),pch=2,add=T)
plot(res.pca,choix="var")
#fem la lletra més petita per millorar la llegibilitat
plot(res.pca,choix="var", cex=0.75)
```
Valorem summary i gràfic
El gràfic d'individus veiem en color magenta el que tenim son els centres de gravetat corresponents a cadascun dels nivells de les variables factor, amb les respectives etiquetes d'aquestes variables qualitatives. 

El següent gràfic es el de relacions entre les variables i les components principals (PCA graph variables). Aleshores el que veiem es que dues variables que tenen un angle petit entre elles vol dir que estàn molt correlacionades positivament, si l'angle que fan es de 180 també estàn molt correlacionades, pero inversament. Aleshores en aquest cas el que veiem es que capital.gain i capital.var doncs estàn molt relacionades amb la primera dimensió factorial i que gairabé ortogonal a elles trobem el capital.loss, que té força sentit.
Per altre banda, si mirem la education.num i l'age veiem que també estàn correlacionades (PCA graph of variables). Veiem que en el eix y (DIM 2, que té una explicavilitat del 22,61% de la variabilitat) les variables age i education.num estàn bastant relacionades amb ell. 
Per altre banda si anem al gràfic de graph of individuals es veu que, d'una forma visua,les diferents categories contraposades d'una variable com es la de f.EduType es troben situades una quasi a dalt de tot de l'eix DIM2 i l'altre quasi abaix de tot (per exemple les categories f.EduType-Doctorate i la f.EduType-Dropout respectivament). Podem fer el mateix raonament fer f.age, tot i que les diferencies entre les posicions de categories contraposades es més curta que la de f.EduType ja que la variable age numèrica en si es troba menys representada pels dos eixos ja que veiem que la seva fletxa es curta.
Si les fletxes son molt curtes vol dir que les dues components principals expliquen poc de les caracteristiques d'aquesta variable. La variable age seria la variable amb la fletxa més curta en aquest cas.

PARLAR DE HOURS.PER.WEEK: veiem que hours.per.week es troba poc representada segons la llargaria de la seva fletxa en el primer graph (PCA graph variables), tot i això podem veure que en la part de l'eix DIM1 i DIM2 positius trobem els hours.per.week més alts, en canvi en la part negativa dels eixos DIM1 i DIM2 trobem les més baixes, totes aquelles categories de factors que trobem en aquests quadrants i mé aprop del centre de gravetat de les diferents categories de f.hours.per.week estaràn més relacionades. Podem dir que els que més hours.per.week tenen estàn relacionats positivament amb categories de més edat i més education.num.

 II.  Individuals point of view
 Are they any individuals "too contributive"       
Un cop ja tenim triades el nombre de dimensions factorials que es necesari per explicar una raonable proporció de la variabilitat de les dades, aleshores el que fem es analitzar aquestes dades fent us de les variables suplementaries que en general ens ajuden en la interpretació.
Primer fem un gràfic on volem veure els individus pero sense etiquetes (label="none"), tampoc surten les etiquetes de les categories magentes d'abans.
```{r}
plot.PCA(res.pca,choix=c("ind"),invisible=c("ind.sup"),label="none",cex=0.8)
```
Ara fem un gràfic on veiem els 5 individus més contributius (select="contrib 5") i veurem quin es el seu rowname. Obviament els individus més contributius són els que tenen les coordenades més grans. I això ens dona una ventatge, els individus que tenen les coordenades més grans poden ser outliers, és a dir també seria una manera de identificar-los, pero també ens poden a ajudar a identificar el significat del eix. Si agafem aquests individus i intentem mirar quines caracteristiques tenen de les variables, aleshores aixo ens dona una pista de perque aquests individus estan tan ben caracteritzats en aquests eix, es a dir, aconseguirem interpretar el significat de l'eix, que es un punt important al que li hem de donar resposta.
```{r}
plot.PCA(res.pca,choix=c("ind"),cex=0.8,select="contrib 5",axes=c(1,2,3)) 
# To better understand the axes through the extreme individuals
#Primer ordenem les coordenades del primer eix (DIM1)
rang<-order(res.pca$ind$coord[,1],decreasing = T)
#Els hem ordenat per coordenades decreixents.
length(rang)
#Mirem els 10 primers, que son els majors en la primera dimensió
res.pca$ind$coord[rang[1:10],1]
#Si ens itereses, també podriem veure si son outliers fent un boxplot:
Boxplot(res.pca$ind$coord[,1],labels=row.names(df))
#Podem veure les característiques d'aquests individus que ens poden interesar per explicar el significat de l'eix dim 1.
df[rang[1:10],vars_con]
```
Veiem que hi ha un individu una mica especial en aquests 10 individus obtinguts; es l'individu que te age=20, (el 17040), que te education.num=10 que es una mica justa per la seva age, un hours.per.week=10, que es molt poc, i que ha tingut un capital.gain de 34095. 
Si anem mirant tots aquests 10 individus son individus que tenen un capital.gain molt elevat, cosa que ja es coherent abans amb la interpretacio dels gràfics, ja que el capital.gain i el capital.var hem mencionat que estaven molt correlacionades amb la primera dimensió.
Aquests individus que tenen el capital.gain tant elevat son els que ens estàn definint la primera dimensió factorial.
Si ens fixem en el target hours.per.week i com ja hem avançat una mica abans amb la interpretació dels gràfics, es una variable molt mal representada en el primer pla factorial, ja que veiem que hi ha individus amb hours.per.week de 10 i 20 i d'altres amb 60. Per tant l'hours.per.week es una variable que està molt poc relacionada amb les variabes numeriques de les que disposem.

Ara pasem a fer la interpretació dels eixos amb dim2
Fem exactament el mateix procediment que en la dimensió per intentar agafar els 10 primers individus que ens ajudin a explicar els significat d'aquest eix en concret.
```{r}
rang<-order(res.pca$ind$coord[,2],decreasing = T)
length(rang)
res.pca$ind$coord[rang[1:10],2]
Boxplot(res.pca$ind$coord[,2])
df[rang[1:10],vars_con]
```
Veiem que els individus que tenen les coordenades més grans en l'eix y, en la dim2, son aquells que tenen el major capital.loss i un capital.var més negatiu. Si analitzem la variable target hours.per.week pasa una cosa similar a l'eix dimensió 1, no es veu ven representada(hi hours.per.week de 20 i de 75).
```{r}
summary(df$age)
```
Podem veure que veien la mean de la variable age que te la nostra mostra es de aprox  38 anys , aquests 10 individus, la majoria d'ells, superen per molts anys aquesta mitja. Podem veure que comparant amb els resultats dels 10 individus obtinguts en l'analisi de la dimensio 1(que s'apropen mes a la mean)les edats d'aquests son molt més elevades. Podem dir, encara que per poc, que l'eix dimensió 2 representaria millor la variable age que el que ho fa la dimensió 1, tot i que no ho hem de tenir gaire en compte ja que la longitud de la fletxa en el PCA graph of variables de age es força curta.
Exactament pasa el mateix amb education.num pero en aquest cas la fletxa d'education.num es una mica més llarga i per tant l'hauriem de tenir en més consideració (comentat anteriorment amb les categories Dropout i Doctorate de f.EduType).

Ara pasem a fer la interpretació dels eixos amb dim3
Fem exactament el mateix procediment que en les altres dues dimensions per intentar agafar els 10 primers individus que ens ajudin a explicar els significat d'aquest eix en concret.
```{r}
rang<-order(res.pca$ind$coord[,3],decreasing = T)
length(rang)
res.pca$ind$coord[rang[1:10],3]
Boxplot(res.pca$ind$coord[,3])
df[rang[1:10],vars_con]
summary(df$age)
summary(df$education.num)
```
Veiem que els individus que tenen les coordenades més grans en l'eix dim3, son aquells que tenen una age molt elevada respecte la mitja (tots superiors a 65 anys). També veiem que la education.num es molt baix (eduaction.num=4 o inferior).Podem dir que la dimensio 3 està bastant definida per individus amb una edat elevada i amb escasa eduació.



 III. Interpreting the axes:  Variables point of view
 coordinates, quality of representation, contribution of the variables  ###


Si tornem a fer el sumary al inici de l'estudi, veiem que comfirmem el que hem estat comentant anteriorment sobre l'analisi de cada dimensió.
Aquí tenim la part del bolcat del summary que ens interesa mirar, on veiem que la dimensió està molt definida per capital.gain i capital.var, que la dimensió 2 ho està per capital.loss i que la dimensió 3 ho està per la age i la educatio.num inversament. I finalment que la variable target hours.per.week no es veu clarament relacionat per cap dels eixos que hem agafat (el que més es la dim2 i amb un 0.14).
```{r}
summary(res.pca, nb.dec = 2, ncp = 3, nbelements = 30, nbind=0)
#Variables
#                                        Dim.1   ctr  cos2   Dim.2   ctr  cos2  
#age                                   |  0.18  1.61  0.03 |  0.44 16.74  0.19 |
#education.num                         |  0.24  2.81  0.06 |  0.62 33.81  0.38 |
#capital.gain                          |  0.98 46.11  0.95 |  0.02  0.04  0.00 |
#capital.loss                          | -0.21  2.13  0.04 |  0.74 48.62  0.55 |
#capital.var                           |  0.99 47.33  0.98 | -0.09  0.79  0.01 |
#                                      Dim.3   ctr  cos2  
#age                                    0.86 76.18  0.73 |
#education.num                         -0.46 22.02  0.21 |
#capital.gain                          -0.05  0.22  0.00 |
#capital.loss                          -0.12  1.51  0.01 |
#capital.var                           -0.03  0.07  0.00 |

#Supplementary continuous variable
#                                        Dim.1  cos2   Dim.2  cos2   Dim.3  #cos2  
#hours.per.week                        |  0.10  0.01 |  0.14  0.02 | -0.02  0.00 |
```
Amb la següent comanda podem veure el cosinus2 i la contribució de cadascuna de les variables age, education.num, capital.gain, capital.loss i capital.var en 3 diferents eixos. Seguim confirmant el que hem explicat en els apartats anteriors.
```{r}
round(cbind(res.pca$var$cos2[,1:3],res.pca$var$contrib[,1:3]),2)
# dimdes easies this description from the variables
#Es posible fer una descripció dels eixos amb dimdesc, analitzem l'output:
#dimdesc(res.pca,axes=1:3)
sumari<-dimdesc(res.pca,axes=1:3,proba=0.01)
#$quali
sumari$'Dim 1'$quali
sumari$'Dim 2'$quali
sumari$'Dim 3'$quali
#$quanti
sumari$'Dim 1'$quanti
sumari$'Dim 2'$quanti
sumari$'Dim 3'$quanti
#$category
sumari$'Dim 1'$category
sumari$'Dim 2'$category
sumari$'Dim 3'$category
```
De la DIM1 veiem que esta relacionada amb les variables quantitatives que ja hem vist, capital.var, capital.gain, eductaion.num, etc. I amb el capital.loss de manera inversa. 

Per la part de quali amb la que més relacionada està es amb Y.bin (el nostre target binari), després també amb el factor f.EduType, f.age, etc.

Per l'apartat de category, per cadascuna de les categories dels factors quin es el valor de la coordenada. Per exemple els que guanyen més de 50k l'any (Y.bin=>50K) tenen un valor estimat en aquesta dimensió 1 que es de 0.54 unitats per sobre del 0, que el 0 es la mitjana global (estàn representats positivament en aquesta dimensió al voltant del 0,5). 
Similarment veiem que també estàn representats positivament en aquesta dimensió els f.Edutyp-Doctorate (0.61082269), els f.Edutyp-Masters (0.53315901) i els f.Edutyp-Prof-school(1.01600904) i també els f.age-(50,90] (0.29018990) que el seu valor es superior a les altres categories de f.age. Podem dir que aquestes serien algunes de les qualitats que tenen els individus que en general guanyen més diners (major capital.gain). Per contra, els que guanyen menys de 50k l'any tenen coordenades negatives en aquesta dimensió 1. Similarment pasa amb el que tenen més baix nivell d'educació (f.Edutyp-Dropout es de -0.85568375) i menys edat (f.age-[17,30] es de -0.39586719).

De la DIM2 veiem que esta relacionada amb les variables quantitatives que ja hem vist, sobretot amb capital.loss i després també amb eductaion.num, age, etc. I amb el capital.var de manera inversa.

Per la part de quali amb la que més relacionada està es amb f.EduType i després també amb Y.bin, f.age, etc.

Per l'apartat de category, per cadascuna de les categories dels factors quin es el valor de la coordenada. Per exemple els que guanyen més de 50k l'any (Y.bin=>50K) tenen un valor estimat en aquesta dimensió 2 que es de 0.458 unitats per sobre del 0. 
Similarment veiem que també estàn representats positivament en aquesta dimensió els f.Edutyp-Doctorate (1.69182306), els f.Edutyp-Masters (0.61630254) i els f.Edutyp-Prof-school(0.94722126) i també els f.age-(50,90] (0.47268134) que el seu valor es superior a les altres categories de f.age. Podem dir que aquestes serien algunes de les qualitats que tenen els individus que en general perden més diners (major capital.loss). Per tant podem dir que comparant-lo amb la dimensió 1, veiem que els que guanyen més diners en general també perden més diners, cosa que te sentit perque si no tens diners la cantitat de diners perduts dificilment serà alta. Per contra, els que guanyen menys de 50k l'any tenen coordenades negatives (-0.45889032) en aquesta dimensió 2. Similarment pasa amb el que tenen més baix nivell d'educació (f.Edutyp-Dropout es de -1.66849470) i menys edat (f.age-[17,30] es de -0.65089403).

De la DIM3 veiem que esta relacionada amb les variables quantitatives que ja hem vist, sobretot molt amb age (0.85520303) i després encara que no tant (-0.45977674), també amb eductaion.num però de manera inversa.

Per la part de quali amb la que més relacionada està es amb f.age i després també amb f.MaritalStatusType, f.EduType, etc.

Per l'apartat de category, per cadascuna de les categories dels factors quin es el valor de la coordenada. Per exemple els que tenen un age de f.age-(50,90] tenen un valor estimat en aquesta dimensió 3 que es de 1.23169694 unitats per sobre del 0, això es molt per sobre de la mitjana. 
Similarment veiem que també estàn representats positivament en aquesta dimensió els f.Edutyp-Dropout (0.95369334), els f.MaritalStatustyp-Widowed  (1.05252793), gent d'edat més avançada, per tant son els que tenen més la caracteristica de widowed i per altre banda amb pocs estudis. Per altre banda, negativament relacionats hi veuriem les persones més joves (f.age-[17,30]), els f.MaritalStatustyp-Never-Married i els f.Edutyp-Bachelors.

Veiem enls tres primers eixos factorials les variables que esàn més correlacionades.
```{r}
plot.PCA(res.pca,choix=c("var"))
plot.PCA(res.pca,choix=c("var"),axes=c(1,2,3))
```
 Use modern ggplot facilities
Ara mitjaçant els gràfics de cos2 i de contribució, podem veure d'una forma global per les dimensions 1, 2 i 3 quines son les variables que estàn més relacionades amb aquestes dimensions.
Veiem que la contribució per sobre de la mitjana en aquestes tres dimensions estàn el capital.var, capital.gain i age. Per altre banda veiem que s'hi acosten força les variables education.num i capital.loss.
```{r}
fviz_cos2(res.pca, choice = "var", axes = 1:3)+theme_bw()
fviz_contrib(res.pca, choice = "var", axes = 1:3)+theme_bw()
```
En aquest gràfic de continuació (Variables-PCA) veiem el gràfic ja mostrat abans (PCA graph variables), pero ara amb una llegenda que indica el cos2 de cada variable en les dues primeres dimensions (no ens acaba de ser útil perque la variable age surt amb un cos2 baix quan en la tercera dimensió no és així i per tant en el conjunt de les tres primeres dimensions tampoc).
```{r}
fviz_pca_var(res.pca, col.var="cos2",repel=TRUE)+
    scale_color_gradient2(low="green", mid="blue", 
    high="red", midpoint=0.75)+theme_bw()
```

 Clustering
Veiem que dins els que es el clustering tenim dues families diferents, la supervisada, que de fet es un partitioning, assignar la millor de les particions a cada individu, pero això si, tenint sempre el numero de grups definits a priori, l'algorisme per fer-ho es el K-Mean.
Després tenim el que es la clasificació no supervisda que per nosaltres es el clustering jeràrquic, aleshores aquest el farem amb el FactorMiner.
 K-Nearest neighboors ---> edu ho sap fer en principi
K-Means Classification
K-Mean amb PCAs
Per fer els K-Means el realitzem sobre les components principals. Agafem la comanda PCA, agafem 3 components principals (les 3 dimensions que hem seleccionat anteriorment).
```{r}
names(df)
vars_con
res.pca<-PCA(df[,c(vars_con,"Y.bin","f.hours.per.week","f.age","f.MaritalStatusType","f.RelType","f.EduType")],quanti.sup=5, quali=7:12,ncp=3 )
my_data <- res.pca$ind$coor[,1:3 ]   # Take scores (principal components) as many dimensions as selected

#Amb les library factoextra i NbClust ens ajudaran a determinar quin es el numero de clusters apropiat.
library("factoextra")
fviz_nbclust(my_data, kmeans, method = "gap_stat")
library("NbClust") # It takes a lot ....
set.seed(123)
#res.nbclust <- NbClust(my_data, distance = "euclidean",
#                  min.nc = 2, max.nc = 10, 
#                  method = "complete", index ="all") 
#res.nbclust
```
Un cop executat NbClust ens retorna que el millor número de clusters es 3, l'execució d'NbClust ens ha bolcat la següent sortida:
******************************************************************* 
* Among all indices:                                                
#* 6 proposed 2 as the best number of clusters 
#* 9 proposed 3 as the best number of clusters 
#* 1 proposed 4 as the best number of clusters 
#* 1 proposed 6 as the best number of clusters 
#* 5 proposed 8 as the best number of clusters 
#* 1 proposed 9 as the best number of clusters 
#* 1 proposed 10 as the best number of clusters 

                  ***** Conclusion *****                            
 
* According to the majority rule, the best number of clusters is  3 
 
 
******************************************************************* 

 One common method of choosing the appropriate cluster solution is to compare the sum of squared error (SSE) for a number of cluster solutions. SSE is defined as the sum of the squared distance between each member of a cluster and its cluster centroid. Thus, SSE can be seen as a global measure of error. In general, as the number of clusters increases, the SSE should decrease because clusters are, by definition, smaller. A plot of the SSE against a series of sequential cluster levels can provide a useful graphical way to choose an appropriate cluster level. 
 Use the kmeans cluster as a suplementary variable in PCA
```{r}
kcla <- kmeans(my_data,3)
```
Aqui agafem 3 clusters tal i com ens ha indicat NbClust, triem els centres a l'atzar.
```{r}
#kcla
```
Ens ensenya quants individus hi ha a cada cluster, a quin cluster li correspon a cadascuna de les nostres observacions, això es pot veure en el clustering vector.
K-means clustering with 3 clusters of sizes 195, 247, 4470;
Ens diu també les coordenades del centre de gravetat de cada cluster.
I també informació sobre la suma de quadrats total, la suma de quadrats within, tot.withinss, etc (Available components).
La qualitat l'hem de mesurar com la suma de quadrats entre clusters (kcla$betweenss) dividit per la suma de quadrats total(kcla$totss); 
```{r}
kcla$betweenss/kcla$totss  
```
Percentage of inertia explains by the partition
veiem que una partició en 3 clusters d'aquestes dades ens presuposa una explicavilitat, és a dir, recull el 52,3% de la variabilitat de les dades.
Veiem que es un a explicavilitat força baixa, ens agradaria poder assolir-ne més, per tant, tenint en compte els resultats que ha anat calculant NbCluster hem decidit finalment utilitzar 5 clusters.
```{r}
kcla <- kmeans(my_data,5)
#kcla
```
K-means clustering with 5 clusters of sizes 1311, 1147, 193, 238, 2023
```{r}
kcla$betweenss/kcla$totss
```
Una partició amb 5 clusters d'aquestes dades es recull el 72,37% de la variabilitat de les dades. 
Per conservar el cluster, ens el guardem a les nostres dades.
```{r}
df$kmclu<-kcla$cluster # Retain the cluster for every observation
df$kmclu<-factor(df$kmclu,labels=paste("kmclu",sep="-",levels(df$kmclu)))#fem factor dels clusters
```
Per caracteritzar els diferents clusters utilitzem el catdes.
```{r}
catdes(df,num.var=which("kmclu"==colnames(df)))
```
Description of each cluster by the categories

CLUSTER1
Si mirem el cluster 1, veiem que hi ha una sobrerepresentació de gent entre f.age[50-90], veiem que a la mostra global representen un 19,56% mentres que els individus que estàn dins aquest cluster un 58,12% d'ells es troba dins d'aquest rang d'edat. A més també veiem que un 79,29% dels individus de la nostra mostra total que tenen f.age[50-90] es troben dins el cluster 1.
Si ens fixem, tenim un 0% del total d'individus de la mostra amb f.age[17-30] que es troba en aquest cluster.
Per la banda de f.EduType veiem que el f.EduType-Dropout tenim que globalment representen un 12,96%, però en aquest cluster1 els tenim sobrerepresentats ja que tenim un 20,59% dels individus amb aquest f.EduType. A més, en aquest cluster hi tenim el 42,38% dels individus totals de la mostra que tenen f.EduType-Dropout.
Si ens fixem en el f.MaritalStatusType el 74,26% dels que son f.MaritalStatustyp-Widowed de la mostra total es troben dins d'aquest cluster i els tenim sobrerepresentats.
Per la part del target binari Y.bin no hi ha res a destacar, es troba sobre la mitja global.
CLUSTER2
El cluster 2 no es un cluster gaire interesant a comentar, únicament i per destacar alguna cosa, podem veure que els f.Edutyp-Bachelors estàn sobrerepresentats en aquesta cluster, veiem que sobre la mostra global representen un 16,43% mentres que en aquest cluster representen un 55%, a part el 78% dels f.Edutyp-Bachelors totals de la mostra van a parar en aquest cluster 2.
El Y.bin per altre banda, que ens interesa bastant tenir-lo en compte ja que es el target binari, en aquest cluster 2 trobem que el Y.bin=>50K està sobrerepresentat, en la mostra global representen un 23% mentres dins d'aquest cluster representen un 36%.
CLUSTER3
En el cluster 3 mirant el target binari Y.bin=>50K veiem que està sobrerepresentat ja que en la mostra global només representen un 23% i en canvi en aquest cluster representen un 93,78%. Per altre banda també cal dir que aquest cluster hi ha poques observacions i que en el Cla/Mod ens mostra un Y.bin=>50K 15% ja que aquesta dada depèn també bastant del tamany del cluster.
Podem comentar també f.cgain=f.cgain-Yes  que en la mostra global representen un 7% mentres que en aquest cluster el 100% dels individus es troben dins d'aquesta categoria. 
CLUSTER4
El cluster 4 segueix amb la mateixa linia en que es un cluster amb pocs individus i en que els individus que guanyen més de 50k estàn sobrerepresentats, ja que en aquest cluster representen un 53,78% mentres en la general un 23%. Per altre banda, a diferencia del cluster 3, aqui tenim que el f.cvar.gain al nostre cluster el 100% dels individus tenen aquesta caracteristica, mentres que en cluster 3 era exactament al contrari.
CLUSTER5
En el cluster 5 veiem que l'Y.bin<=50k està sobrerepresentada, ja que en el global representen un 76% i en aquest cluster un 92,14%. Per altre banda també tenim sobrerepresentats els individus amb f.age-[17,30], ja que en el global representen un 31,57% i en aquest cluster un 59%. També veiem que també estàn sobrerepresentats els f.MaritalStatustyp-Never-Married que te sentit ja que en aquest cluster estàn sobrereprsentats els individus de menor edat.

Description of each cluster by quantitative variables
Es comenta una mica més ràpid perque ja s'han comentat els cluster mitjançant les categories dels factors que es van crear de les variables originals.
CLUSTER1
La age està per sobre de la mitjana global, la hours.per.week gairabé per sobre de la mitjana global.
CLUSTER2
El education.num està per sobre de la mitjana global i el target hours.per.week també està tres hores per sobre de la mitjana global.
CLUSTER3
Trobem que capital.gain i capital.var es troben molt per sobre de la mitjana global i per altre banda el hour.per.week es troba al voltant de 5 hores per sobre de la mitjana global.
CLUSTER4
Tenim que el capital.loss està molt per sobre de la mitjana global i el hours.per.week lleugerament també per sobre.
CLUSTER5
A destacar veiem que tan el capital.gain com la age dels individus d'aquest cluster estàn força per sota de la mitjana global.
Per veure més clarament els comentaris fets al target hours.per.week sobre la seva mitjana en cadascun dels clusters i sobre la mitjana global, mostrem a continuació els %'s en cada cluster per cadascuna de les categories de f.hours.per.week:
```{r}
prop.table(table(df$f.hours.per.week,df$kmclu),2)
```

 Hierarchical Clustering

 HCPC (Hierarchical Clustering on Principal Components)
```{r}
res.hcpc<-HCPC(res.pca,nb.clust=-1,order=TRUE)
```
Ens suggereix una partició en 5 clusters.
Quina es la qualitat? depèn del nombre de clusters que considerem, en el nostre cas 5.
Calculem la fracció que representa la suma de quadrats between dividida entre la suma de quadrats total ens dirà el que aquesta partició captura, si en comptes d'utilitzar els 4912 individus actuals de la mostra, sintetitzes i només utilitzes el centre de gravetat dels 5 clusters aleshores amb aquest centre de gravetat recolliria, de la variabilitat total, el percentatge que calculem a continuació (70,57% de la variabilitat de les dades)
Goodness of fit: BetweenSS/TotalSS
```{r}
(res.hcpc$call$t$within[1]- res.hcpc$call$t$within[5])/res.hcpc$call$t$within[1]
```
Interprete the results of clustering
L'objecte obtingut del HCPC conté:  "data.clust": conté totes les dades originals més una columna on s'hi identifica el cluster, "desc.var",  "desc.axes",  "desc.ind",   "call".
```{r}
names(res.hcpc)
```
data.clust 
The original data with a supplementary row containing the partition
```{r}
summary(res.hcpc$data.clust)
```
Veiem el summary de totes les variables originals que hem utilitzat en la definició dels Principal Components (totes les numèriques, més els factors que nosaltres li hem posat com a suplementaris i que ens ajuden a llegir).
```{r}
colnames(res.hcpc$data.clust)
```
Comentaris sobre el cluster jeràrquic:
Veiem que el cluster numero 5 està molt buit (84 obs), el 1 (240 obs) i el 4 (198 obs) també ho estàn força, mentres que el 2 està molt ple (2748 obs).

Counts of individuals in each cluster
```{r}
summary(res.hcpc$data.clust$clust)
```
desc.var
A. The description of the clusters by the variables
```{r}
names(res.hcpc$desc.var)
```
Tenim quatre llistes diferents per fer la descripció dels clusters per variables: "test.chi2"  "category"   "quanti.var" "quanti"

desc.var$test.chi2
 A.1. The categorical variables which characterizes the clusters 
Associació global amb els factors:
```{r}
res.hcpc$desc.var$test.chi2
```
Veiem que el factor més relacionat es f.age, tambe ho estàn molt el f.MaritalStatusType, Y.bin, f.RelType, f.EduType i per últim f.hours.per.week en aquest ordre.
Ens interesa anar mirant doncs lo representatius que son aquests clusters en relació amb les variables target que ens interesen.
desc.var$category 
A.2. The description of each cluster by the categories 
Associació de les categories amb cadascún dels clusters:
```{r}
res.hcpc$desc.var$category
```
CLUSTER1
El Y.bin=>50K estàn sobrerepresentats ja que la mitja global es  23.33% i en aquest cluster hi ha un 53.33% d'individus que guanyen més de 50k. També estàn sobrerepresentats els f.MaritalStatustyp-Married-civ-spouse, els f.Edutyp-Doctorate, els f.Reltyp-rel-husband i hours.per.week-(45,98].
CLUSTER2
En aquest cluster estàn sobrerepresentats els més joves de la mostra, es a dir, els f.age-[17,30] ja que en aquest cluster representen un 53,45% dels individus mentres que en la global en representen un 31,57%. També estàn sobrerepresentats els f.MaritalStatustyp-Never-Married, Y.bin=<=50K entre d'altres.
CLUSTER3
Veiem sobrerepresentats la gent de major edat de la mostra, ja que f.age-(50,90] representen un 50,66% d'aquest cluster mentres que de global representen un 19,56%. També estàn sobrerepresentats factors com f.MaritalStatustyp-Widowed, f.age-(40,50]   i f.Edutyp-Dropout entre d'altres.
CLUSTER4
Veiem que la gent que guanya més de 50k (Y.bin=>50K)està molt sobrereprsentada en aquest cluster, ja que en el cluster un 68,18% guanyen més de 50k mentres que en la global un 23,33%. També estaràn sobrerepresentats f.MaritalStatustyp-Married-civ-spouse, f.Edutyp-Masters i hours.per.week-(45,98] entre d'altres. 
Tenir en compte en aquets cluster amb menys individus el Cla/Mod ja que serà obviament més baix.
CLUSTER5
El cluster 5 conte poques observacions, per tant el Cla/Mod es força baix.
Veiem que, com en el cluster 4, els individus amb Y.bin=>50K estàn sobrerepresentats, ja que en el cluster representen un 96,42% i globalment un 23,33%.També estàn sobrerepresentades altres categories com hours.per.week-(45,98] i f.MaritalStatustyp-Married-civ-spouse.

desc.var$quanti.var 
A.3. The quantitative variables which characterizes the clusters 
Relació de les variable quantitatives, doncs les que estàn més relacionades són:
```{r}
res.hcpc$desc.var$quanti.var
```
Les que estàn més relacionades son: age, capital.gain, capital.loss, capital.var (que ve determinada per les dues anteriors), education.num i hours.per.week en aquest ordre.

desc.var$quanti
A.4. The description of each cluster by the quantitative variables
Relació de les variables quantitatives:
```{r}
res.hcpc$desc.var$quanti
```
CLUSTER1
En el perfil del primer cluster veiem que les perdues de capital, en promig, son de 1891.02083, mentres que en la global son de 95.27. Després education.num, hours.per.week i age es troben lleugerament per sobre de la overall mean pero es significatiu.
CLUSTER2
En aquest cluster 2 veiem que les hours.per.week està sobre la overall mean, el capital.gain està bastant per sota del overall mean i la mean d'age dels individus d'aquest cluster està també per sota de la mitjana global.
CLUSTER3
En aquest cluster estàn sobrerepresentades les persones d'edat més gran, ja que la mitja d'edat en aquest cluster supera per bastant la mitjana global de 38 anys de la mostra. La mitjana de capital.gain dels individus d'aquest cluster està molt per sota de la mitjana així com també ho està per sota, encara que lleugerament l'education.num.
CLUSTER4
En aquest cluster la mitjana de capital.gain es molt més elevada que la overall mean, la age i el target hours.per.week també estàn força per sobre de la mitjana global.
CLUSTER5
En aquest cluster la mitjana de capital.gain es molt més elevada que la overall mean, la age i el target hours.per.week també estàn força per sobre de la mitjana global.

desc.axes 
B. The description of the clusters by the axes 
```{r}
names(res.hcpc$desc.axes)
res.hcpc$desc.axes$quanti.var
res.hcpc$desc.axes$quanti
```
Mirem com estàn de representats cadascun dels eixos (nosaltres tenim 3 dimensions) en cadascun dels clusters. Veiem que concorda força amb el que varem descriure de com es caracteritzava cada dimensió amb la caracterització dels clusters que hem fet en els apartats anteriors.

desc.ind 
C. The description of the clusters by the individuals
Per fer la descripció dels clusters segons els individus ens interesen els parangons. Els parangons s'obtenen per cadascun dels clusters les 5 observacions que estàn més a prop del centre de gravetat i les 5 observacions que estàn més llunys dels altres clusters. Per identificar quines són ho hem de fer per tots els clusters amb els que hem partit les nostres dades (5 clusters) i ho podrem posar gràficament.
```{r}
names(res.hcpc$desc.ind)
res.hcpc$desc.ind$para
res.hcpc$desc.ind$dist
```
Characteristic individuals
Mirem les característiques d'aquets individus que son els parangons:
```{r}
para1<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[1]]))
para2<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[2]]))
para3<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[3]]))
para4<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[4]]))
para5<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[5]]))

para1
df[63, ]
para2
df[33, ]
para3
df[499, ]
para4
df[504, ]
para5
df[259, ]
```
Hem agafat un parangó de cada cluster per veure el valor de les seves variables. Veiem que concorden amb els valors de les seves dades amb l'anàlisi que hem fet anteriorment de les característiques de cada cluster.
```{r}
dist1<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[1]]))
dist2<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[2]]))
dist3<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[3]]))
dist4<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[4]]))
dist5<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[5]]))
```
Mirem el valor de les variables dels para's i dels dist's de cadascun dels clusters. Veiem que concorden molt els valors de les seves dades amb l'anàlisi que hem fet anteriorment de les característiques de cada cluster. La relació amb les caracteristiques dels clusters apareix més accentuada amb aquests individus ja que són els individus de cada cluster més allunyat de tots els altres clusters restants.
```{r}
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$para[[1]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$dist[[1]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$para[[2]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$dist[[2]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$para[[3]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$dist[[3]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$para[[4]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$dist[[4]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$para[[5]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$dist[[5]])),]
```
Mostrem gràficament els para's i dist's de cada un dels clusters, en blau i en taronja respectivament.
```{r}
plot(res.pca,label="none",invisible=c("quali","ind.sup")) 
plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2],col="grey80",cex=0.5,pch=16)
points(res.pca$ind$coord[para1,1],res.pca$ind$coord[para1,2],col="blue",cex=2,pch=16)
points(res.pca$ind$coord[dist1,1],res.pca$ind$coord[dist1,2],col="orange",cex=2,pch=16)
points(res.pca$ind$coord[para2,1],res.pca$ind$coord[para2,2],col="blue",cex=2,pch=16)
points(res.pca$ind$coord[dist2,1],res.pca$ind$coord[dist2,2],col="orange",cex=2,pch=16)
points(res.pca$ind$coord[para3,1],res.pca$ind$coord[para3,2],col="blue",cex=2,pch=16)
points(res.pca$ind$coord[dist3,1],res.pca$ind$coord[dist3,2],col="orange",cex=2,pch=16)
points(res.pca$ind$coord[para4,1],res.pca$ind$coord[para4,2],col="orange",cex=2,pch=16)
points(res.pca$ind$coord[dist4,1],res.pca$ind$coord[dist4,2],col="orange",cex=2,pch=16)
points(res.pca$ind$coord[para5,1],res.pca$ind$coord[para5,2],col="orange",cex=2,pch=16)
points(res.pca$ind$coord[dist5,1],res.pca$ind$coord[dist5,2],col="orange",cex=2,pch=16)
```
Per veure com els nostres clusters estàn relacionats amb el nostre target binari (Y.bin), podem calcular quines son les proporcions dels que guanyen més o menys de 50k $ en cadascun dels clusters, podem fer les proporcions per files o per columnes.
Veiem que els clusters 4 i 5 son els que contenien més proporció d'individus que guanyen més de 50k, tot i que com ja hem comentat anteriorment son clusters amb poques observacions. Per altre banda, també veiem en el cluster 2 que hi ha una gran proporció d'individus que guanyen menys de 50k, que curiosament es en el cluster on hi ha més proporció de gent que treballa menys hores com veurem ara més abaix quan analitzem el target hours.per.week per cada cluster.
```{r}
prop.table(table(df$Y.bin,res.hcpc$data.clust$clust),2)
#Fem el mateix pel clustering de K-means
prop.table(table(df$Y.bin,df$kmclu),2)
#Per fer comparacions amb la mitjana mirem les proporcions globals de Y.bin, en sí, ho hem estat fent al llarg de tots els anàlisis anteriors.
prop.table(table(df$Y.bin))
```
Mirem com els nostres clusters estàn relacionats amb el nostre target hours.per.week. Destaquem que el cluster 2 es el cluster on hi ha més proporció de gent que treballa menys hores, seguit d'aprop pel cluster 3. Després per altre banda, el cluster 5 es el cluster on hi tenim més proporció de gent que treballa més hores.
```{r}
prop.table(table(df$f.hours.per.week,res.hcpc$data.clust$clust),2)
```
En tot moment hem anat mirant les mitjes globals de cadascuna de les categories de hours.per.week per veure si en cada cluster estaven sobrereprsentats o no. A continuació les mostrem:
```{r}
prop.table(table(df$f.hours.per.week))
# this is for checking the consistency between kmeans partition and HCPC clustering
table(df$kmclu,res.hcpc$data.clust$clust)

```

#CA analysis 
```{r}
# CA
names(df)
tt<-ftable(xtabs(~f.hours.per.week+f.OccupationType,data=df))
tt
prop.table(table(df$f.hours.per.week))
```
Veiem que la majoria de gent, un 55,5% treballa entre 39 i 45 hores, un 24,2% treballa entre 1-39 hores i que un 20,25% son els que treballen més hores, entre 45-98 hores. Aquestes mitjanes les utilitzarem per comparar amb les proporcions en cada categoria.
```{r}
round(prop.table(tt,1),dig=2)
prop.table(table(df$f.OccupationType))
```
Veiem que el grup més ple d'individus es el Emp-BaixCarrec amb un 30,58%, que son per exemple els operaris o serveis de netaja i els del sector primari. Dels que menys cantitat hi ha a la mostra son els Emp-AltCarrec i els Militars. 
```{r}
round(prop.table(tt,2),dig=2)
```
Veiem que els que més hores treballen son aquells que estàn dins la categoria de f.Occupationtyp-Services seguit pels f.Occupationtyp-Other-Ocupation i finalment també per els f.Occupationtyp-Emp-BaixCarrec. Bàsicament aquests son els que treballen més hores.
Els perfils que treballen menys hores son: els f.Occupationtyp-Emp-AltCarrec (un 50% treballen entre 1-39hores), després els f.Occupationtyp-Sales i finalment els f.Occupationtyp-Admin.

sembla que els perfils columna no son homogenis aleshores si no son homogenis vol dir que potser si que tenen alguna relació amb la variable hours.per.week. com que son dependents aleshores ens pot aportar alguna informació.
El que fem ara es comparar les proporcions en global de cada individu que hi ha en cadascun dels nivells que tenim a hours.per.week amb la proporció de les diferents categories de f.Occupationtyp. Veiem que les categories que estàn per sobre de la proporció global de cada un dels nivells de hours.per.week son les que hem estat comentant just fa un moment.

aquestes proporcions marginals ens serveixen per fer la base de comparació. 
deprés també tenim la opció de preguntarnos si son independents, ho fem mitjançant el test de la chi2.
H0: files i columnes son independents. (f.OccupationType i f.hours.per.week son indendents)
el p-value ens surt p-value < 2.2e-16, llavors les files i columnes no son independents.
es a dir, podem utilitzar la f.OccupationType per fer una prediccio alhora calcular el bloc d'hores treballat. 
```{r}
chisq.test(tt)
```
fins aqui ho hem anat fent amb eines basiques
```{r}
names(df)
table(df[,c(29,21)])
summary(df[,c(29,21)])
```
si ens interessa calcular les CA anem a la llibreria FactoMineR, i que faci l'analisi corresponent a les files i columnes en aquest cas.
```{r}
res.ca<-FactoMineR::CA(table(df[,c(29,21)]))
summary(res.ca, nb.dec = 2, ncp = 2)
par(cex=0.8)
```
el factorminer utilitza 2 dimensions perque les dimensions es el maxim d' eixos factorials necessaris per fer una representacio d'una taula de contingencia: es el minim del nombre de categories menys 1 del factor que tingui menys categories. En aquest cas ens treu 2 dimensions perque les f.hour.per.week te 3 categories en el nostre cas i menys 1 doncs ens dona 2.

El primer planol factorial que esta composat per la primera i segona dimensio te una representativitat del 100%. La primera dimensió ja reuniex un 77,35%.

aqui tenim la informacio caracteristica de les files i columnes que es poden sobreposar perfectament sense cap problema, tenim els %'s explicats, i tenim quina es la coord en cadascun dels eixos 1er i 2on, la contribucio i els cos2(mesura de la qualitat de la representacio que te una categoria en el eix que estiguem mirant).
Per exemple en la dimensió 1 veiem que la representació que te la categoria hours.per.week-[1,39] es molt bona, d'un 0.93. La categoria hours.per.week-(39,45] pasa el mateix pero amb la dimensió 2, ja que cos2=0.88
Per part de les columnes, es a dir, el f.OccupationType, veiem que el f.Occupationtyp-Emp-AltCarrec la qualitat de la representació es molt bona ja que té un cos2=0.98 en la primera dimensió (cosa que quadra amb el que hem comentat abans de que els f.Occupationtyp-Emp-AltCarrec són dels que menys hores treballen, ja que en la dimensió ens explicaria bastant això).
Per altre banda en la dimensió 2, veiem que estàn els f.Occupationtyp-Professional  i els f.Occupationtyp-Military, que tenen un cos2=1 i cos2=0.99 respectivament. Veiem doncs que fent referencia a lo analitzat per rows, aquests serien els que treballen una proporció d'hores més propera a hours.per.week-(39,45].

mirem el CA factor map: 
Si tenim dues categories que estan properes vol dir que estan relacionades, és a dir, que es donen simultaneament observacions que pertanyen a la categoria de files(f.hours.per.week) i a la categoria de columnes(f.OccupationType). 
Notem el efecte Goodman en la curvatura que dibuixen les tres categories de f.hours.per.week.
Podem dir que la gent que es dedica als f.Occupationtyp-Services son els que estàn més propers del punt de hours.per.week-(45,98]. Podem dir que majoritariament són els que treballen més de 45 hores.per.week. 
Després tenim a les persones que treballen a f.Occupationtyp-Emp-BaixCarrec que són les que s'acosten més al punt de hours.per.week-(39,45] que estaria una mica per sobre de la mitja. Veiem que els f.Occupationtyp-Admin els trobem entre hours.per.week-(39,45] i hours.per.week-(1,39] sense estar polaritzat en cap de les dues. I finalment podem veure a les persones amb f.Occupationtyp-Emp-AltCarrec que son les que s'acosten més al volum de hours.per.week més baix (hours.per.week-(1,39])
Trobem una polarització força clara entre els que treballen mes de 45hores i f.OccupationTyp-Services, després les que queden més a prop d'aquest número d'hours.per.week elevat però no amb una polarització son el sector f.Occupationtyp-Professional, f.Occupationtyp-Other-Ocupation i els f.Occupationtyp-Emp-BaixCarrec.
A la taula de continuació ens poden recolzar per validar els comentaris que hem fet anteriorment, on veiem el número d'individus de cada f.OccupationType i les hours.per.week que treballen:
```{r}
table(df[,c(29,21)])
```
other plots #Per veure més clarament les dades podem utilitzar els següents plots; no els necessitem ja que ja ens es suficientment clar amb els gràfics mostrats anteriorment.
```{r}
plot.CA(res.ca,invisible=c("row"))
plot.CA(res.ca,invisible=c("col"))
plot.CA(res.ca,selectRow = "contrib 3",invisible=c("col","col.sup"))
plot.CA(res.ca,selectCol="cos2 3",invisible="row")
plot.CA(res.ca,selectCol="coord 2",invisible="row")
```
Segon anàlisi CA
```{r}
names(df)
tt<-ftable(xtabs(~f.hours.per.week+f.EduType,data=df))
tt
round(prop.table(tt,1),dig=2)
prop.table(table(df$f.EduType))
```
Veiem que el grup més ple d'individus es el f.Edutyp-HS-Graduate amb un 32,7%. Dels que menys cantitat hi ha a la mostra son els f.Edutyp-Doctorate.
```{r}
round(prop.table(tt,2),dig=2)
```
Veiem que els que més hores treballen son aquells que estàn dins la categoria de f.Edutyp-Prof-school seguit pels f.Edutyp-Doctorate.
Els perfils que treballen menys hores son: els f.Edutyp-Dropout (un 33% treballen entre 1-39hores, que es el percentatge més alt d'aquesta categoria), després els f.Edutyp-Colleges també amb un 33%.

sembla que els perfils columna no son homogenis aleshores si no son homogenis vol dir que potser si que tenen alguna relació amb la variable hours.per.week.
El que fem ara es comparar les proporcions en global de cada individu que hi ha en cadascun dels nivells que tenim a hours.per.week amb la proporció de les diferents categories de f.EduType. Veiem que les categories que estàn per sobre de la proporció global de cada un dels nivells de hours.per.week son les que hem estat comentant just fa un moment.

Deprés també tenim la opció de preguntarnos si son independents, ho fem mitjançant el test de la chi2.
H0: files i columnes son independents. (f.EduType i f.hours.per.week son indendents)
el p-value ens surt p-value < 2.2e-16, llavors les files i columnes no son independents.
es a dir, podem utilitzar la f.EduType per fer una prediccio alhora calcular el bloc d'hores treballat. 
```{r}
chisq.test(tt)

names(df)
table(df[,c(29,19)])
summary(df[,c(29,19)])

res.ca<-FactoMineR::CA(table(df[,c(29,19)]))
summary(res.ca, nb.dec = 2, ncp = 2)
par(cex=0.8)
```
El primer planol factorial que esta composat per la primera i segona dimensio te una representativitat del 100%. La primera dimensió ja reuniex un 69,97%.

Aqui per exemple en la dimensió 1 veiem que la representació que te la categoria hours.per.week-(45,98] es molt bona, d'un 0.93. La categoria hours.per.week-(39,45] pasa el mateix pero amb la dimensió 2, ja que cos2=0.92
Per part de les columnes, es a dir, el f.EduType, veiem que el f.Edutyp-Bachelors la qualitat de la representació es molt bona ja que té un cos2=0.97 en la primera dimensió. També es veuen olt ven representats en la dim1 els f.Edutyp-Dropout i els f.Edutyp-Masters.
Per altre banda en la dimensió 2, veiem que estàn els f.Edutyp-Associates  i els f.Edutyp-HS-Graduate, que tenen un cos2=1 i cos2=0.95 respectivament. Veiem doncs que fent referencia a lo analitzat per rows, aquests serien els que treballen una proporció d'hores més propera a hours.per.week-(39,45].


mirem el CA factor map: INSERIR FACTOR MAP 
Si tenim dues categories que estan properes vol dir que estan relacionades, és a dir, que es donen simultaneament observacions que pertanyen a la categoria de files(f.hours.per.week) i a la categoria de columnes(f.EduType). 

Podem dir que la gent que es dedica als f.Edutyp-Masters estàn propers al punt de hours.per.week-(45,98].A més, també podem dir que les categories f.Edutyp-Doctorate i f.Edutyp-Prof-school la majoria d'individus treballen dintre aquesta categoria de hours.per.week-(45,98] però com que aquestes categories son poc freqüents queden lluny del centre de gravetat.
```{r}
summary(df$f.EduType)
```
Després tenim a les persones que pertanyen a f.Edutyp-HS-Graduate són les que s'acosten més al punt de hours.per.week-(39,45] que estaria una mica per sobre de la mitja juntament amb f.Edutyp-Associates.
I finalment podem veure a les persones amb f.Edutyp-Dropout i f.Edutyp-Colleges son les categories que tenen un percentatge més elevat d'individus que s'acosten més al volum de hours.per.week més baix (hours.per.week-(1,39]).

A la taula de continuació ens poden recolzar per validar els comentaris que hem fet anteriorment, on veiem el número d'individus de cada f.EduType i les hours.per.week que treballen:
```{r}
table(df[,c(29,19)])
```
other plots per veure més clarament les dades podem utilitzar els següents plots; no els necessitem ja que ja ens es suficientment clar amb els gràfics mostrats anteriorment.
```{r}
plot.CA(res.ca,invisible=c("row"))
plot.CA(res.ca,invisible=c("col"))
plot.CA(res.ca,selectRow = "contrib 3",invisible=c("col","col.sup"))
plot.CA(res.ca,selectCol="cos2 3",invisible="row")
plot.CA(res.ca,selectCol="coord 2",invisible="row")

```

# Multiple Corresponde Analysis
```{r}
names(df)
```
Quan estem fent correspondencies multiples podem fer igual que feiem en les correspondencies simples, considerar el cas mes basic que es considerar tot variables factor actives.Tot aixo no es un tractament adecuat tenint en compte que tinc variables factors que de fet son les que volem explicar, com el factor binari, o el cas de factor hours.per.week que no les podem posar com a actives.

En la següent comanda tenim en compte qualitatives suplementaries i quantitaves suplementaries.
Considerem com a variables qualitatives suplementaries el target binari i el target numeric discretitzat(f.hours.per.week).I com a quantitatives suplementaries el target que es numèric, es a dir, les hores de feina com a numèrica. 
Les posicions d'aquestes variables son la 13(hores de feina treballades numèrica) i 29(f.hours.per.week), i despres de 15(Y.bin). Les hours.per.week numèrica, que es la 13, la posem com a quant.sup=1. quali.sup son la 2 i la 3, son la 29 i la 15, f.hours.per.week i Y.bin respectivament.
```{r}
cex=0.4
res.mca <- MCA(df[,c(13,29,15:25,27:28)],quali.sup=2:3,quanti.sup = 1)
```
Ens dona 4 gràfics.
Anem a veure aquesta descripció, pero abans de res i abans de passar als gràfics, anem a analitzar el summary.
D'entrada com que tenim gairabé 5000 observacions no volem que ens digui res dels individus, nbind=0, de les dimensions ens interessa treballar amb un nombre de dimensions, amb un nbelements, que son les categories de totes les variables que estem utilitzant(posem una quantitat que sigui prou elevada, com ja hem fet moltes altres vegades).Per aquest summary posem ncp=2 ja que volem reduïr la sortida i els gràfics només donen les primeres dues dimensions(el primer pla factorial).
```{r}
summary(res.mca, nb.dec = 2, ncp = 2, nbelements = 30, nbind=0 )
```
Veiem que tenim els valors propis(Eigenvalues) de cada una de les dimensions. La qüestio es quantes dimensons hem de considerar per tenir una bona representació.
```{r}
round(res.mca$eig[,1],4)
mean(res.mca$eig[,1])
```
Hem de fer el criteri de Kaiser extés, és a dir, més general, calculem quina es la mitjana del valor propi i ens diu que es 0.083, aleshores ens quedem amb totes les dimensions que tinguin un valor propi superior a aquest valor.
Veiem que ens quedem fins la dimensio 17. Per tant es queda amb 17 dimensions principals. Reunim una explicavilitat de 73.40 de la variabilitat de la inèrcia.
 Barplot Eigenvalues
```{r}
barplot(res.mca$eig[,1],main="Eigenvalues",names.arg=1:nrow(res.mca$eig))
```
Els apartats del summary, el primer bloc son els valors propis, després tenim la relació de les categories de les variables i les diferents dimensions que haguem triat indicant quina es la contribucio i quina es la relacio de qualitat. Deprés tenim també la eta2 que es el coeficient de correlacio al quadrat entre els factors i les dimensions factorials. Aleshores veiem que les que estan més relacionades amb la dim1 son  la f.EduType, f.education.num seguides de f.OccupationType i f.RelType.
En la dim2 segueixen tenint molta relació f.EduType i f.education.num seguides de f.OccupationType i f.MaritalStatusType.
```{r}
sum(res.mca$eig[,1])
```
La inercia total explicada per aquestes dades en correspondencies multiples. La inercia total es la suma dels valors propis. La inercia total d'aquesta taula de correspondencies multiples es de 3,083.
Aleshores com més gran es la inèrcia vol dir que es mostren dades que estan més polaritzades, es a dir, que expliquen més coses les unes de les altres i hi ha més relació entre les variables.

Depres tenim les categories i variables suplementaries aixo es interessant. Ho veiem amb la relació amb els grafics.
representació dels eigenvalues
```{r}
barplot(res.mca$eig[,1],main="Eigenvalues",names.arg=1:nrow(res.mca$eig))
```
comentem els gràfics que ens ha donat per defecte al executar la comanda: res.mca <- MCA(df[,c(13,29,15:27)],quali.sup=2:3,quanti.sup = 1)

GRAFIC 1 FACTOR MAP
En aquest gràfic trobem les relacions entre les variables a partir de les relacions entre les categories. I quan tenim categories properes en la projecció vol dir que hi ha individus que comparteixen simultaneament aquelles caracteristiques.
En aquest cas aquest es el grafic més important de tots els que treballen les correspondencies multiples, perque en aqui el que podem veure es, categories que estan molt lluny del centre de gravetat(pertanyen a categories dels factors que son molt rares),(com per exemple els f.Edutyp-Doctorate o els que tenen capital.gain ).
Deprés segona de les lectures, te a veure les categories que estan mapejades juntes. Aleshores, si veiem els f.education.num i els f.EduType que son variables força realacionades veiem que hi ha casos obvis com per exemple el f.EduType-Collages i els f.education.num(10-12) estàn molt junts, com també ho estàn per exemple els f.EduTyp-Bachelor, Master i Doctorate amb el f.education.num més elevat.
Aleshores també si mirem persones que tenen una relacio de Husband, també resulta que estan casats, molt a prop de lo de f.MaritalStatusType Married (obvi i te sentit). 
A la variables superior esquerra el f.RelType-NotFamily te relacio amb el grup més jove d'edat.(obvi).I tenen un f.MaritalStatusType-NeverMarried.
També queda realativament a prop el Rel-Child.
A baix a la dreta tenim els del capital.gain positiu i els que tenen una variacio de capital positiva, aleshores evidenment son els mateixos. Després també tenim els self-emp-inc o autònoms que normalment son casats, tenen rol de husband. Aixo son les categories en color vermell, que son les categories activas.

Per si ens interessa la variable binaria Y.bin, en aquest cas el target binari ens diu que esta correlacionat en la dimensio primera, en canvi no te res a veure amb la dimensio segona, perque la coordenada en la dimensio segona es practicament 0, la dimensio primera, doncs, es la que ens interessa.
En les hour.per.week no esta tant clar, ja sabem que les hours.per.week no esta massa relacionada amb cap de les variables que tenim.

Aleshores les categories que son verdes corresponen a variables suplementaries que son qualitatives, és a dir, aqui podriem distingir quin es el bloc de numero de hores treballades. Aleshores veuriem que cap a l'esquerra tindriem els que treballen menys i cap a la dreta els que treballen més.
Veiem també que els que guanyen més de 50k es troben a la part dreta del gràfic i els que cobren menys de 50k a la part esquerre. Ho veiem en el següent gràfic:

Supplementary factors
```{r}
plot(res.mca,choix="ind",invisible=c("ind","var","ind.sup"))
plot.MCA(res.mca, choix="ind",invisible="ind.sup",habillage=3,label="quali.sup")
```
GRAFIC 2 FACTOR MAP
Aqui veiem els individus en el gràfic. Aqui els individus poca cosa, amb 5000 obs veiem, com a molt podem intentar veure quins son els individus més contributius i aleshores analitzar el perque. Els individus més contributius normalment son els que estaran més lluny del centre de gravetat i vol dir que son els més raros en quant a les característiques que tenen dins el conjunt d'observacions en general.
En canvi els individus que estan a prop del centre diriem que responen a les característiques del individu mig. Una altre cosa es que si agafem un factor, per exemple el maritalStatus i ens fixem en totes les categories d'quest factor, aquetes categories estan escampades de manera que el promig es el centre de gravetat.
```{r}
plot.MCA(res.mca, choix="ind", col.ind="grey80", label=c("var","quali.sup"),cex=0.7)
```
Analisi d'individus contributius i dels que estàn a prop del centre:
Els individus contributius serien els que estàn més allunyats de l'eix central i que s'allunyen de les característiques d'un individu mig, per tant son aquells individus que pertanyen a categories amb poques observacions. Per exemple el Doctorate, Dropout, els que tenen capital.gain-Yes, etc. 
Els menys contributius o els que tenen característques que segueixen la linia general de la mostra vindrien a ser els individus de raça White, els que tenen f.Countrytyp-USA, els que tenen f.cgain-No, etc.

GRAFIC 4 SUMPLEMENTARY QUANTITATIVE VARIABLES
 Supplementary numeric variables
```{r}
plot(res.mca,choix=c("quanti.sup"))
```
Veiem que les hours.per.week diu que esta relacionada amb la primera dimensio factorial i poc amb la segona (ho veiem gràcies al angle que forma la fletxa). I te una tendencia que amb més hours.per.week aleshores més a la dreta de la primera dimensio, i més negativa de la dimensio segona. Podem veure que en aquest quadrant on la hours.per.week va augmentant hi trobem per exemple els individus amb característiques de casats, husband, autonoms, i també es trobarien aquells que tenen capital.gain-Yes.

AXIS DESCRIPTION
```{r}
sumari<-dimdesc(res.mca,proba=0.01)
```
relacio entre les variables qualitatives amb la primera dimensio factorial

$quali
```{r}
sumari$'Dim 1'$quali
```
Els factors més relacionats amb la dim1 son com ja hem repetit diversos cops: Y.bin, f.RelType, f.EduType, f.MaritalStatusType, f.OccupationType, f.age i f.education.num.
```{r}
sumari$'Dim 2'$quali
```
Els factors més relacionats amb la dim2 son com ja hem repetit diversos cops principalment: f.EduType, f.OccupationType i f.education.num.

$quanti
```{r}
sumari$'Dim 1'$quanti
sumari$'Dim 2'$quanti
```
Veiem que com ja hem comentat i vist en el gràfic de sumplementary quantitative variables anteriorment, hours.per.week te més correlació amb la dim1, a més, ho és positivament, és a dir, a mesura que avancem positivament en l'eix x, també augmentem les hours.per.week. En canvi en la dimensió 2 la correlació es menor i negativa. 

$category
```{r}
sumari$'Dim 1'$category
```
Veiem que per la dim1 les categories que més relacionades són f.education.num=f.education.num(13-16), f.MaritalStatusType=f.MaritalStatustyp-Married-civ-spouse, f.RelType=f.Reltyp-rel-husband i Y.bin=>50K, ho estàn positivament; Per altre banda tenim les ho estarien negativament que serien els f.age=f.age-[17,30] i els Y.bin=<=50K.
```{r}
sumari$'Dim 2'$category
```
Veiem que per la dim1 les categories que més relacionades són f.education.num=f.education.num(10-12), f.MaritalStatusType=f.MaritalStatustyp-Never-Married, f.education.num=f.education.num(13-16), ho estàn positivament; Per altre banda tenim les ho estarien negativament que serien els f.education.num=f.education.num(9), els f.OccupationType=f.Occupationtyp-Emp-BaixCarrec i els f.EduType=f.Edutyp-HS-Graduate. Tot i que hem de tenir en compte que aquestes categories sobretot les que estàn relacionades positivament no estàn en el mateix grau de relació en els que ens manejavem en la dim1 fa un moment.

# Hierarchical Clustering
Treballem amb el clusterings, ara ja estem treballan amb unes variables qualitatives que hem passat a unes coordenad i aleshores aquestes coordenades son reals, podem aplicar exactament la mateixa tècnica, de K-means o de clustering jerarquic, per a fer els clusters corresponents per a la classificacio segons aquestes variables qualitatives.

Les comandes veiem que son practicament les mateixes que en PCA.
Podem treballar o bé en el context de variables numèriques PCA i fer classifcacio ,  o bé amb les variables categoriques i fer la classificació.
## K-Means amb MCAs
```{r}
res.mca <- MCA(df[,c(13,29,15:25,27:28)],quali.sup=2:3,quanti.sup = 1,ncp=21)
dclu <- res.mca$ind$coor[,1:21]   # Take scores (principal components)
#dclu
library("factoextra")
dcludist<- dist(dclu)
summary(dcludist)

kcla <- kmeans(dclu,6) #Utilitzem 6 clusters
#kcla
kcla$betweenss/kcla$totss #Veiem que la explicavilitat es foça baixa, de un 27% aproximadament.
df$kmclu<-kcla$cluster #Retain the cluster for every observation
#Fem factor dels clusters
df$kmclu<-factor(df$kmclu,labels=paste("kmclu",sep="-",levels(df$kmclu)))
#Per caracteritzar els diferents clusters utilitzem el catdes.
#catdes(df,num.var=which("kmclu"==colnames(df)))
```
Description of each cluster by the categories:

Description of each cluster by the categories 

CLUSTER 1:
Veiem que en aquest cluster estan molt sobrepresentats aquells individus que pertanyen a les categories: f.education.num=f.education.num(10-12)i  f.EduType=f.Edutyp-Colleges amb un 99.45 i 92,14 respectivament. Mentres que globalment només representen 29.6 i un 21,68 respectivament.
CLUSTER2:  
Veiem que en aquest cluster estan molt sobrepresentats aquells individus que pertanyen a les categories: f.EduType=f.Edutyp-Prof-school i  f.education.num=f.education.num(13-16)  amb un 100% els dos. Mentres que globalment només representen 1.58 i un 24,69 respectivament. Fem notar que aquest té aquestes característiques perque aquestes categories son poc freqüents i s'han ajuntat totes en aquest cluster.
CLUSTER3:
En aquest cluster a diferencia dels cluster 2 hi tenim la gent que menys anys ha estudiat, veiem que en aquest cluster estan molt sobrepresentats aquells individus que pertanyen a les categories: f.education.num=f.education.num(1-8) i  f.EduType=f.Edutyp-Dropout  amb un 100% els dos. Mentres que globalment només representen un 12,96% els dos.
CLUSTER4:
Veiem que en aquest cluster estan molt sobrepresentats aquells individus que pertanyen a les categories: f.education.num=f.education.num(9) i  f.EduType=f.Edutyp-HS-Graduate amb un 100% els dos. Mentres que globalment només representen 32.67.
CLUSTER5:
Veiem que en aquest cluster estan molt sobrepresentats aquells individus que pertanyen a les categories: f.education.num=f.education.num(13-16) i  f.EduType=f.Edutyp-Bachelors amb un 100% i un 70.99 respectivament. Mentres que globalment només representen 24.69 i un 16.42 respectivament. S'assembla bastant a les característiques remarcades al cluster 2.
CLUSTER6:
Veiem que en aquest cluster estan molt sobrepresentats aquells individus que pertanyen a les categories: f.education.num=f.education.num(10-12),  f.EduType=f.Edutyp-Associatesi f.EduType=f.Edutyp-Colleges amb un 100%, un 46.74 i un 53.25 respectivament. Mentres que globalment només representen 29.66, 7.98 i un 21.68 respectivament.

Per l'anàlisi del target binari Y.bin mirem com està representat en cadascun dels clusters en la següent taula elaborada:

genral: <=50k 76.66 	>50k 23.33
cluster1->  	97.16   	2.84
cluster2->	25.64     	74.35
cluster3->	95.13     	4.86
cluster4->	84.11     	15.88
cluster5-> 	54.46   	45.53
cluster6->	63.2     	36.79

Description of each cluster by quantitative variables

CLUSTER1: 
Veiem que el capital.gain està molt per sota de la mitja global, i el target hours.per.week està unes 5hores per sota de la mitjana global.
CLUSTER2:
Veiem que el capital.gain està molt per sobre de la mitja global, i el target hours.per.week està unes 6hores per sobre de la mitjana global.
CLUSTER3:
Tant l'education.num com el capital.gain es troben força per sota de la mitjana, i el target hours.per.week està unes 3hores per sota de la mitjana.
CLUSTER4:
Es un cluster on els individus es troben tot i que per sota de la mitjana capital.gain, eduaction.num i capital.loss estàn força propers a la mitjana global. 
CLUSTER5:
No hi ha gran cosa a destacar, tenen una education.num per sobre de la mitjana així com també ho està el capital.gain.
CLUSTER6:
No hi ha gran cosa a destacar, només que treballen unes 3hores més que la mitjana global i que estàn lleugerament per sobre de la mitjana d'edat.

Per veure més clarament els comentaris fets al target hours.per.week sobre la seva mitjana en cadascun dels clusters i sobre la mitjana global, mostrem a continuació els %'s en cada cluster per cadascuna de les categories de f.hours.per.week:
```{r}
prop.table(table(df$f.hours.per.week,df$kmclu),2)
```

HCPC from MCA
```{r}
res.mca <- MCA(df[,c(13,29,15:25,27:28)],quali.sup=2:3,quanti.sup = 1,ncp=21)
res.hcpc<-HCPC(res.mca,nb.clust=6,order=TRUE)
```
Goodness of fit: BetweenSS/TotalSS
Veiem que la explicavilitat es del 28,19% de la variabilitat de les dades.
```{r}
(res.hcpc$call$t$within[1]- res.hcpc$call$t$within[6])/res.hcpc$call$t$within[1]
```
Interprete the results of clustering 
```{r}
names(res.hcpc)
```
data.clust 
The original data with a supplementary row containing the partition ###
```{r}
summary(res.hcpc$data.clust)
```
El summary ens mostra el numero d'individus que hi ha en cada cluster i per categoria.
```{r}
colnames(res.hcpc$data.clust)
```
Counts of individuals in each cluster
```{r}
summary(res.hcpc$data.clust$clust)
```
Concretament en aquest summary veiem quants individus te cada cluster. Veiem que el cluster 3 està força més ple que tots els altres.

desc.var
A. The description of the clusters by the variables
```{r}
names(res.hcpc$desc.var)
```
desc.var$test.chi2
A.1. The categorical variables which characterizes the clusters
```{r}
res.hcpc$desc.var$test.chi2
```
Veiem que el factors més relacionats son: f.type, f.EduType,  f.OccupationType, f.cgain, f.closs, f.cvar, f.education.num.
Ens interesa anar mirant doncs lo representatius que son aquests clusters en relació a les variables targets que ens interesen.

desc.var$category
A.2. The description of each cluster by the categories
```{r}
res.hcpc$desc.var$category
```
CLUSTER1: 
Els que cobren menys de 50K són un 91.72% mentres que els que cobren més de 50K representen només el 8.27%.
Per tant veiem que els que guanyen menys de 50K estan sobrerepresentats, a que la mitjana global es que representen un 76.66%.

Veiem que la categoria f.OccupationType=f.Occupationtyp-Emp-AltCarrec està molt sobrerepresentada en aquest cluster, ja que representa un 99.62 d'aquest i únicament un 5.49 del global. També ho estàn f.type=f.typ-other, f.hours.per.week=hours.per.week-[1,39] i f.age=f.age-(50,90] amb un 100%, 49.62% i 38,34% respectivament en el cluster i un 5.51%, 24,20% i 19,56% globalment respectivament.

CLUSTER2: 
Els que cobren menys de 50K són un 96.08% mentres que els que cobren més de 50K representen només el 3.91%.
Per tant veiem que els que guanyen menys de 50K estan sobrerepresentats, ja que la mitjana global es que representen un 76.66%.

Veiem que la categoria f.EduType=f.Edutyp-Dropout està molt sobrerepresentada en aquest cluster, ja que representa un 100% d'aquest i únicament un 12.96 del global. També ho està f.OccupationType=f.Occupationtyp-Emp-BaixCarrec amb un 55.12% en el cluster i un 30.57% globalment.

CLUSTER3: 
Els que cobren menys de 50K són un 85% mentres que els que cobren més de 50K representen només el 15%.
Per tant veiem que els que guanyen menys de 50K estan sobrerepresentats, encara que lleugerament, ja que la mitjana global es que representen un 76.66%.

Veiem que la categoria f.EduType=f.Edutyp-HS-Graduate està  sobrerepresentada en aquest cluster, ja que representa un 53.048% d'aquest i únicament un 32.67% del global. Veiem que es un cluster amb molts individus i veiem que la representació de cada una de les categories no s'allunya massa de la mitja.

CLUSTER4:
Els que cobren menys de 50K són un 60.23% mentres que els que cobren més de 50K representen només el 39.82%.
Per tant veiem que els que guanyen més de 50K estan sobrerepresentats, ja que la mitjana global es que representen un 23.33%.

Veiem que la categoria f.EduType=f.Edutyp-Bachelors està força sobrerepresentada en aquest cluster, ja que representa un 69.64% d'aquest i únicament un 16.42% del global. En aquest cluster no hi ha gaire més cosa a destacar.

CLUSTER5: 
Els que cobren menys de 50K són un 48.38% mentres que els que cobren més de 50K representen només el 51.61%.
Per tant veiem que els que guanyen menys de 50K estan sobrerepresentats, ja que la mitjana global es que representen un 23.33%.

Veiem que la categoria f.cvar=f.cvar.gain està molt sobrerepresentada en aquest cluster, ja que representa un 100% d'aquest i únicament un 5.15% del global. Veiem que aquí es on es situen els individus que guanyen més diners. Els f.MaritalStatusType=f.MaritalStatustyp-Married-civ-spouse també estàn una mica sobrerepresentats ja que en el cluster representen un 63.70% i globalment un 45,80%.

CLUSTER6: 
Els que cobren menys de 50K són un 38.65% mentres que els que cobren més de 50K representen només el 61.34%.
Per tant veiem que els que guanyen més de 50K estan sobrerepresentats, ja que la mitjana global es que representin un 23.33%.genral: <=50k 76.66 	>50k 23.33
Veiem que la categoria f.cgain=f.cgain-Yes està  sobrerepresentada en aquest cluster, ja que representa un 100% d'aquest i únicament un 7.63% del global. No hi ha gran cosa a destacar més.

desc.var$quanti.var 
A.3. The quantitative variables which characterizes the clusters
```{r}
res.hcpc$desc.var$quanti.var
```
desc.var$quanti
A.4. The description of each cluster by the quantitative variables 
```{r}
res.hcpc$desc.var$quanti
```
CLUSTER1:
La mitjana de hours.per.week en aquest cluster es d'un 31.69 hores, mentres que en la mitjana global es de 39.91 hores. Veiem que en aquest cluster es troba aproximadament 8h per sota de la mitja.

CLUSTER2:
La mitjana de hours.per.week en aquest cluster es d'un 36.64 hores, mentres que en la mitjana global es de 39.91 hores. Veiem que en aquest cluster es troba lleugerament per sota de la mitja.

CLUSTER4:
La mitjana de hours.per.week en aquest cluster es d'un 42.65 hores, mentres que en la mitjana global es de 39.91 hores. Veiem que en aquest cluster es troba aproximadament 3h per sobre de la mitja.

CLUSTER5:
La mitjana de hours.per.week en aquest cluster es d'un 42.37 hores, mentres que en la mitjana global es de 39.91 hores. Veiem que en aquest cluster es troba aproximadament 3h per sobre de la mitja.

CLUSTER6:
La mitjana de hours.per.week en aquest cluster es d'un 43 hores, mentres que en la mitjana global es de 39.91 hores. Veiem que en aquest cluster es troba aproximadament 4h per sobra de la mitja.

Veiem que en els clusters que la gent, en general guanya més diners, també es la gent que treballa més hores.


desc.axes
B. The description of the clusters by the axes
```{r}
names(res.hcpc$desc.axes)
res.hcpc$desc.axes$quanti.var
res.hcpc$desc.axes$quanti
```
Les dimensions amb eta2 més elevada i amb un p-value més proper a 0 són: de la dim1 fins la dim8. (La dim2 i la dim6 a menys nivell). Aquestes dimensions son les que guarden més correlació amb els factors.

desc.ind
C. The description of the clusters by the individuals
```{r}
names(res.hcpc$desc.ind)
res.hcpc$desc.ind$para
res.hcpc$desc.ind$dist
```
Characteristic individuals
```{r}
para1<-which(rownames(res.mca$ind$coord)%in%names(res.hcpc$desc.ind$para[[1]]))
para2<-which(rownames(res.mca$ind$coord)%in%names(res.hcpc$desc.ind$para[[2]]))
para3<-which(rownames(res.mca$ind$coord)%in%names(res.hcpc$desc.ind$para[[3]]))
para4<-which(rownames(res.mca$ind$coord)%in%names(res.hcpc$desc.ind$para[[4]]))
para5<-which(rownames(res.mca$ind$coord)%in%names(res.hcpc$desc.ind$para[[5]]))
para6<-which(rownames(res.mca$ind$coord)%in%names(res.hcpc$desc.ind$para[[6]]))


dist1<-which(rownames(res.mca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[1]]))
dist2<-which(rownames(res.mca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[2]]))
dist3<-which(rownames(res.mca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[3]]))
dist4<-which(rownames(res.mca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[4]]))
dist5<-which(rownames(res.mca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[5]]))
dist6<-which(rownames(res.mca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[6]]))
dist6
para1
summary(df[402,])
```
Hem calculat els para's i els dist's de cada un dels clusters
```{r}
plot(res.mca,label="none",invisible=c("quali","ind.sup"))
plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2],col="grey80",cex=0.5,pch=16)
points(res.mca$ind$coord[para1,1],res.mca$ind$coord[para1,2],col="blue",cex=2,pch=16)
points(res.mca$ind$coord[dist1,1],res.mca$ind$coord[dist1,2],col="orange",cex=2,pch=16)
points(res.mca$ind$coord[para2,1],res.mca$ind$coord[para2,2],col="blue",cex=2,pch=16)
points(res.mca$ind$coord[dist2,1],res.mca$ind$coord[dist2,2],col="orange",cex=2,pch=16)
points(res.mca$ind$coord[para3,1],res.mca$ind$coord[para3,2],col="blue",cex=2,pch=16)
points(res.mca$ind$coord[dist3,1],res.mca$ind$coord[dist3,2],col="orange",cex=2,pch=16)
points(res.mca$ind$coord[para3,1],res.mca$ind$coord[para4,2],col="blue",cex=2,pch=16)
points(res.mca$ind$coord[dist3,1],res.mca$ind$coord[dist4,2],col="orange",cex=2,pch=16)
points(res.mca$ind$coord[para3,1],res.mca$ind$coord[para5,2],col="blue",cex=2,pch=16)
points(res.mca$ind$coord[dist3,1],res.mca$ind$coord[dist5,2],col="orange",cex=2,pch=16)
points(res.mca$ind$coord[para3,1],res.mca$ind$coord[para6,2],col="blue",cex=2,pch=16)
points(res.mca$ind$coord[dist3,1],res.mca$ind$coord[dist6,2],col="orange",cex=2,pch=16)
```
INSERIM PLOT FINAL AMB PUNTS DIBUIXATS
```{r}
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$para[[1]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$dist[[1]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$para[[2]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$dist[[2]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$para[[3]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$dist[[3]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$para[[4]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$dist[[4]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$para[[5]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$dist[[5]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$para[[6]])),]
res.hcpc$data.clust[which(rownames(res.hcpc$data.clust)%in%names(res.hcpc$desc.ind$dist[[6]])),]
```
Mirem el valor de les variables dels para's i dels dist's de cadascun dels clusters. Veiem que concorden molt els valors de les seves dades amb l'anàlisi que hem fet anteriorment de les característiques de cada cluster. La relació amb les caracteristiques dels clusters apareix més accentuada amb aquests individus ja que són els individus de cada cluster més allunyat de tots els altres clusters restants.
Podem veure per exemple, en el cluster 6, que els dist són individus que treballen hours.per.week força per sobre de la mitjana, que ots ells guanyen més de 50K i que tenen un nivell d’estudis molt elevat. L’edat d’aquests 5 individus també es troba per sobre de la mitjana. Veiem això comentat en les següents taules retallades del output:
             f.education.num clust
8722  f.education.num(13-16)     6
12716 f.education.num(13-16)     6
23256 f.education.num(13-16)     6
28326 f.education.num(13-16)     6
32204 f.education.num(13-16)     6

 f.hours.per.week       Y.bin 
hours.per.week-(45,98]  >50K 
hours.per.week-(39,45]  >50K 
hours.per.week-(45,98]  >50K 
hours.per.week-(45,98]  >50K 
hours.per.week-(39,45]  >50K 

Per veure com els nostres clusters estàn relacionats amb el nostre target binari (Y.bin), podem calcular quines son les proporcions dels que guanyen més o menys de 50k $ en cadascuna dels clusters.
Veiem que els clusters 4,5 i 6, com ja hem estat comentant en l’anàlisi anterior,  son els que contenien més proporció d'individus que guanyen més de 50k, tot i que son clusters amb poques observacions.
```{r}
prop.table(table(df$Y.bin,res.hcpc$data.clust$clust),2)
prop.table(table(df$Y.bin))
```
A destacar, veiem que a mesura que ens movem pels clusters la proporció de individus que treballen més hores per setmana per sobre de la mitjana va augmentan. Fins que arribem al cluster 6 on veiem que la proporció de gent que treballa entre [45-98] hores per setmana supera en un 10% la proporció d'individus que treballen aquest rang d'hores en la mostra global.
```{r}
prop.table(table(df$f.hours.per.week,res.hcpc$data.clust$clust),2)
prop.table(table(df$f.hours.per.week))
```
Comparant el Kmean amb el HCPC veiem que sobretot en el cluster 2 i 4 veuem que es troben treballarien i estarien organitzats d'una manera semblant ja que per exemple el cluster 2 en HCPC només té observacions que estàn ubicades unicament en el cluster 3 de Kmeans. Per altre banda tenim el cluster 6 de HCPC on els seus individus estan força repartits entre els diferents cluster Kmeans, per tant no haurien tingut exactament la mateixa organització.
```{r}
table(df$kmclu,res.hcpc$data.clust$clust)
```


#3 Deliverable
## Target hours.per.week
## Modelling using numeric variables (covariates)

```{r}
library(car)
library(FactoMineR)
library(effects)
library(lmtest)
library(rgl)
```
Comencem la feina de modelització, va adreçada a poder determinar en aquest cas per la variable target numerica hours.per.week quines son les variables amb les que esta relacionada i com s'han de combinar aquestes variables per tal de fer una predicció del target. És a dir del hours.per.week.
Aquests models son utils per veure les variables crítiques i veure les combinacions entre elles, pero sobretot per tal de fer prediccions en el futur.
Dintre de la estructura de modelatge, primer agafem la varibale target hours.per.week que ja coneixem bé gràcies al analisi descriptiu fet en les anteriors entregues.
```{r}
summary(df$hours.per.week)
```
Mirem la distribució d'aquest target, perque els models tinguin bones propietats, començarem pels models més simples, els de regresió. Aleshores la variable target ha de tenir condicionat a tot el que expliquen les variables explicatives ha de tenir unes característiques de normalitat. Fem un histograma per veure. El següent es el perfil de les hours.per.week i el podem recordar. Recordem que el perfil no es normal.
```{r}
hist(df$hours.per.week,30,freq=FALSE)
```
També sabem de les anteriors entregues les relacions dos a dos entre les variables explicatives numèriques i dels factors i el target. En aquesta fase del modelatge només ens interesa tot el que sigui la relació d'explicatives numeriques amb el target. Primer treballem la relació del target amb les numeriques. Un cop tenim les numeriques que son importants posades dins del model el que farem es veure si els factors son rellevants, és a dir, afegir-los com a més a més al model. Hi haurà moments on normalment afegirem el factor o el substituirem per el seu anàleg. I després veurem les interaccions entre variables numeriques i factors quan tenen el rol de variables explicatives.


Agafem les variables numeriques
[1] "age"            "education.num"  "capital.gain"   "capital.loss"  
[5] "hours.per.week" "capital.var" , la variable target es la cinquena

```{r}
res.condes<-condes(df[,vars_con],5)
res.condes$quanti
```
              correlation      p.value
education.num  0.14865402 1.136054e-25
capital.gain   0.08685800 1.077558e-09
capital.var    0.07668594 7.401486e-08
age            0.07664351 7.525259e-08
capital.loss   0.05119971 3.309228e-04

Veiem les relacions, veiem que com més education.num te un individu més hores treballa per setmana. Totes elles tenen una relació positiva, és a dir, a mesura que augmenten també augmenten les hours.per.week. La intensitat d'aquestes relacions pero es molt feble, tal com vam veure en la anterior entrega. Sabem que el model que es construeixi llavors serà un model predictiu no gaire bo.
Ara ja podem començar, som conscients de que la variable no es normal, aixo vol dir que tindrem problemes quan haguem de fer una validació de les hipotesis del model, i l'altre cosa que sabem es que no ho tindrem facil per explicar la variable target hours.per.week gràcies a l'analisi multivariant fet en la segona entrega.
El metode per calcular els models que relacionen la variable target amb un conjunt de variables explicatives es el metode "lm". Si tenim una correlacio de 0.14865402 entre education.num i la variable target i després amb el capital.gain una de 0.08685800 aleshores un model que tingui les dues variables com a explicatives no mostarara una correlacio de 0.235512 perque no son independents la variable eduaction.num i la variable capital.gain sino que estan relacionades entre elles. Aleshores la aportació de cadascuna de les variables explicatives numeriques que son les que estem treballant ara amb el target no es una relacio que sigui additiva. No es el mateix que explica una variable per ella mateixa, que quan està "en companyia" d'altres variables en un model. 
Com que tenim 5 variables numeriques en la bd, podem posar el primer model, que sera un model bastant gran en el sentit de que posarem les variables que sabem que estan relacionades amb el target.
Expliquem hours.per.week amb totes les variables explicatives que ens siguin posibles, per tant el data frame que li pasem el restringim a les numeriques que es el que volem.
```{r}
m1<-lm(hours.per.week~.,data=df[,vars_con])
summary(m1)
```
Si utilitzem totes les variables numeriques disponibles per explicar el target la explicavilitat que obtenim es del 3.79%, és a dir, no arribem a explicar ni el 4% de la variabilitat del target, es molt poc.
Mirem si tenim redundancies en aquestes variables, perque redundancies voldrà dir variables explicatives que estiguin molt relacionades entre elles, això ho podem veure en la segona entrega, i es fatal per els models.
Mirem primer quines son significatives: ens fixem en la columna Pr(>|t|), es un test d'hipòtesi, aleshores per cadascun de les variables explicatives que estan involucrades en el model veiem que es rebutja la hipotesi nula ja que el p-value en totes elles es molt proper a 0, per sota del llindar del 5% que normalment ens marquem, per tant, totes aquestes variables son rellevants, sobretot la de education.num amb un Pr(>|t|) de < 2e-16.
Fem un estudi dels efectes nets. Responem a la pregunta de si un cop tenim totes les variables menys una en el model afegir aquesta una aporta alguna millora al model? Fem per exemple una prova:
```{r}
bro1<-lm(hours.per.week~age+capital.gain+capital.loss+capital.var,data=df[,vars_con])
```
Aquest model te totes les variables numeriques menys la education.num, i en el m1 les te totes. Podem fer una h0 on els dos models son equivalents, si els dos models son equivalents el que vol dir es que un cop tenim les 4 variables que hem posat en el bro1 aleshores afegir-li la edication.num no aportaria res de nou. Això es un tests d'efectes nets. Per fer els tests el que fem es en cas de models encaixats (nested), el model bro1 esta dins de m1, aleshores el que fem es:
```{r}
anova(bro1,m1)
```
Test de fisher que es la eina bàsica de modelització i de tractament inferencial dels models.Per veure si son o no son equivalents.
el model bro1 te 4 variables, el m1 en te 5, un cop tenim afegides les 4 variables en el bro1 val la pena afegir el education.num? el test ens diu, h0 son equivalents, la h0 es rebutja ja que per < 2.2e-16, per tant, no son equivalents, el m1 es millor que bro1. Per tant education.num aporta un valor afegit.

Per no haver de fer aquest procediment per cadascuna de les variables, la llibreria car ens proporciona:
```{r}
Anova(m1)
```
tests d'efectes nets per a totes les variables implicades
Veiem que totes aporten al model. Aquesta conclusió seria una conclusió valida si totes les variables fòssin independents entre si, pero en aquest punt ja sabem que per exemple el capital.gain i el capital.loss no son independents del capital.var. Per tant, aixo vol dir que aquest model, com que te variables explicatives que estan relacionades i precisament aquestes, com veure estan molte correlacionades i fan gairabé la mateixa feina.

Una de les coses que hem de mirar per tant, es si les variables estan o no correlacionades, i per veure la colinealitat tenim "vif":

El vif es el variance inflation factor i el que ens diu es si el coeficient es un coeficient elevat vol dir que hi ha variables que estan molt correlacionades amb la variable que te aquest coeficient tant elevat. Veiem que capital.gain, capital.loss i capital.var son valors molt elevats i indiquen gran colinealitat. Han d'estar al voltant de 1, a la que tenim valors molt grans vol dir que tenim colinealitat. A la que trobem colinealitat les variables les hem de treure del model.
```{r}
vif(m1)
marginalModelPlots(m1)
```
Sembla que capital.var es la que tindriem que obviar. Pero com que el capital.var es un resum del capital.gain i el capital.loss doncs podem treure les altres dues. Això ho fem gracies també a que coneixem aquest arxiu i les seves dades. Per tant actualitzem m1:
```{r}
m1<-lm(hours.per.week~age+education.num+capital.var,data=df[,vars_con])
summary(m1)
```
Totes les variables son explicatives i els coeficients han canviat, hem canviat el model cap a millor. Ara veiem els efectes nets, i tots seràn significatius:
```{r}
Anova(m1)
```
El model que tenim ara es un model que hours.per.week = 31.7+(5.464e-02*age)+(5.953e-01*education.num)+(2.185e-04*capital.var), te una explicavilitat del R-squared:  0.0297, 2.97%.

El següent punt que ens hem de fixar es que aquest model només conté relacions lineals entre les variables numèriques i el target pero la relacio entre aquestes variables es realment lineal? Llavors la eina que tenim per d'alguna manera fer aquesta diagnosi es la marginalModelPlots de la llibreria car.
```{r}
marginalModelPlots(m1)
```
Obtenint aquests plots podem veure que hi ha una relació marginal entre les dades i el model, aleshores si mirem el blau que son les dades i el vermell que son el que diu el model podem detectar si hi ha discrepancies entre el  que diuen les dades i el que diu el model, si trobem que hi ha discrepancies això suggereix que la relació lineal no està vent agafada, la relació que hi ha entre la variable expplicativa i el target es no lineal, alguna transformació hem de fer.
Veiem que hi ha un petit desajust entre el blau i el vermell en els extrems, això ens fa pensar que aquest model logicament te molta variabilitat encara per explicar i es un model que d'entrada requerirà algun tipus de transformació per veure si podem millorar les propietats de explicavilitat en relació a les variables explicatives numèriques i al target. 

Intentem introduir alguna transformació, la transformació més senzilla es la transformació basada en la regresió polinòmica. La regresió polinòmica, estem dient que en el model 2 volem modelar les hours.per.week com la age numerica lineal, la education.num amb els termes lineals i quadràtics i amb el capital.var lineal. Aleshores quan utilitzem la comanda poly amb 2 volem dir que la modelització de la eductaion.num com a variable explicativa inclourà els termes lineals i quadràtics. Aquesta eina no ens permet interpretar directament els pesos o coeficients en el model.

## Include polynomial regressors
```{r}
m2<-lm(hours.per.week~age+poly(education.num,2)+capital.var,data=df[,vars_con])
m2llegible<-lm(hours.per.week~age+education.num+I(education.num^2)+capital.var,data=df[,vars_con])
summary(m2)
```
Veiem que el pvalue de poly education.num, el terme quadràtic es ortogonal al primer que es el lineal, si fem la h0 de coeficient = 0 aleshores te un pvalue de 0.009969, aleshores la probabilitat de que sigui 0 es força petita, aleshores si que li fa falta aquesta transformació.
Aquest coeficients no son directament interpretables, si el que volem fer es la equació hem de mirar el que hem construit com a m2llegible.
```{r}
summary(m2llegible)
```
El que veiem que es idèntic, pero el que veiem ara es que els coeficients son diferents, ara si que podem interpretar el model i dir que hours.per.week = 35.13+4.945e-02*age+(-1.351e-01)*education.num+3.821e-02*(education.num^2)+2.097e-04*capital.var
Amb el m2llegible el marginalModelPlots el R no enten que education.num i el quadrat de education.num siguin la mateixa variable, i llavors les va tractant per separat i això no es el que volem.
El que ens interesa es veure conjuntament si hem guanyat alguna cosa, per això fem sobre m2.
```{r}
marginalModelPlots(m2)
```
Tenim moltes informacions i el model es força dolent, no hem millorat gran cosa. Una mica si, ja que te una explicavilitat del R-squared:  0.03101, 3.1% respecte el 2.97% a m1. Com que el model m1 i m2 son encaixats podem provar el test de fisher.

 Nested models - Fisher test
```{r}
anova(m1,m2)
```
H0 es rebutja, m2 es una mica millor que m1
La probabilitat de que siguin equivalents es molt petita.

intenem proposar alguna millora més afegint alguna transformació incloent les variables explicatives numeriques amb els seus termes quadràtics.
```{r}
m3<-lm(hours.per.week~age+poly(education.num,2)+poly(capital.var,2),data=df[,vars_con])
summary(m3)
Anova(m3)
vif(m3)
```
Veiem que afegint el el terme quadràtic de capital.var no millorem ni un 0.1% del model. Inclòs posant el terme cúbic en education.num tampoc millora res, al voltant d'un 0.1%. Veiem que el terme quadràtic te un valor de Pr(>|t|) massa elevat, 0.305071.

Intentem fer una altre millora al nostre model posant el terme quadràtic de la variable age
```{r}
m4<-lm(hours.per.week~poly(age,2)+poly(education.num,2)+capital.var,data=df[,vars_con])
summary(m4)
anova(m4,m2) #H0 es rebutja, m4 es millor que m2.
```
La probabilitat de que siguin equivalents es molt petita.
Veiem que la explicavilitat millora notablement amb un total d'un 15.47%
```{r}
summary(df$capital.var)
```
Poposar canvia a factor de les variables numèriques presents actualment en el model:
Veiem que hi ha variables com la variable capital.gain o el capital.loss, recordem que hi havia molts pocs individus que tinguèssin capital.gain o capital.loss, per tant, en el nostre model ens interesarà que capital.var posiblement ens interesi treballar-la més com a variable factor. Podem considerar el model m1a que diem que tenim la age, education.num i considerem el capital.var com a factor (f.cvar). El que estem es mirant si ens surt més a compte utilitzar una variable explicativa factor per el concepte de capital.var.
```{r}
levels(df$f.cvar)
summary(df$f.cvar)
m1a<-lm(hours.per.week~poly(age,2)+poly(education.num,2)+f.cvar,data=df)
summary(m1a)
```
Veiem que te una explicavilitat del 0.1545, la empitjora una miqueta de res respecte m4, a part tant amb el summary com amb l'Anova podem veure per els seus Pr(>|t|), que son superiors al de m4 de capital.var. No ens el quedem com a factor.

Probem amb factor de age
```{r}
m4a<-lm(hours.per.week~f.age+poly(education.num,2)+capital.var,data=df)
summary(m4a)
```
Veiem que la exlicavilitat baixa dràsticament respecte el millor model que teniem fins ara, el model m4. m4a obtenim un Multiple R-squared:  0.07351. Veiem que no val la pena pasar a factor la variable age.
Ara amb f.education.num:
```{r}
m4b<-lm(hours.per.week~poly(age,2)+f.education.num+capital.var,data=df)
summary(m4b)
```
Veiem que millora molt poc la explicavilitat, de un 0.1547 a un 0.1563. Veiem que hi ha una categoria especifica, concretament la de f.education.numf.education.num(10-12) que te un Pr(>|t|) de 0.23269, per tant, ens està dient que aquesta concreta categoria del f.education.num doncs no valdría la pena. De totes maneres si fem el test d'efectes nets veiem que si val la pena en el seu conjunt, ja que f.education.num te un Pr(>F) de 3.303e-12. Ens quedem per tant amb el f.education.num.
```{r}
Anova(m4b)

m5<-lm(hours.per.week~poly(age,2)+f.education.num+capital.var,data=df)
summary(m5)
```
Per comparar aquests models no podem comparar amb els test de Fisher ja que no son nested. Podem utilitzar el BIC o el AIC, per tant, comparem els BIC's de m5, m4a, m4b=m5 i m1a i amb els anteriors.

 BIC / AIC : Minimum BIC is preferred
```{r}
BIC(m5,m1a)
```
Covariate use preferred
Busquem el model que te el BIC o el AIC que te un valor més petit, perque es el que ens està donant millor explicavilitat i minima complicació del model. El m5 te un BIC menor.

Continuem provant BIC's:
Ho mirem de manera gràfica:
```{r}
BIC(m5,m2)
BIC(m5,m4)
BIC(m5,m4a)
```
Ens quedem amb m5, perque te una major explicavilitat que totes les altres estudiades, amb una explicabilitat del 0.1563 veiem també que el BIC es el menor de tots els estudiats. El m5 reanomenem mBest.
```{r}
mBest<-lm(hours.per.week~poly(age,2)+f.education.num+capital.var,data=df)
summary(mBest)
```

## Residual Analysis
Abans de considerar sistemàticament la addició de variables explicatives factor pasem a fer una diagnosi per veure com tenim els residus en el nostre model.
```{r}
par(mfrow=c(2,2))
plot(mBest,id.n=0)
par(mfrow=c(1,1))
```
Mirem el gràfic residual vs fitted i en aquí hem de mirar els residus del model, es a dir, el que no queda explicat per la part sistemàtica te un soroll i que no té cap patró específic. En aquest cas es una mica dificil de veure pero la linia vermella ens ajuda a veure que si que hi ha una certa tendència descendent, és a dir, els residus son més positius quan les prediccions de les hours.per.week son petites que quan son grans. Això el que ens està dient es que els residus encara contenen informació, tenen una certa estructura, tenen un patró, no estàn centrats en el 0, no son soroll blanc. Cosa normal perque encara tenim un aprox 85% per explicar. 

Si ens fixem en el segon gràfic en el Normal Q-Q, es el gràfic dels residus, son les observacions del target menys les prediccions del target, es a dir, el que no està explicat per el model. Aquests residus han de tenir una distribució normal. Veiem que la recta que tenim en aquest gràfic es la recta sobre la que haurien d'estar la teoria(que es la normal estandard) vs la realitat dels residus, veiem clarament que hi ha doscrepancies a les cues. Veiem que per tant els residus no son normals, per tant no estem en les millors condicions per aplicar minims quadrats perque els estimadors no son suficients, si fèssim prediccions així no tindriem suficient fiabilitat.

El Scale-Location, ens fixem principalment amb la linia vermella, si la linia vermella es plana vol dir que la variabilitat dels residus es constant al llarg de tot el rang de valors de la predicció i això es bo. En aquest cas no es així, hi ha una més variabilitat en els extrems que no pas en els valors centrals.

Finalment, l'últim gràfic es el de les observacions influents, posem en ordenades els residus normalitzats, i en el eix de les X el factor d'anclatge, una mesura de quant lluny està la observació en qüestió del centre de gravetat de les variables explicatives. Podem veure que hi ha un parell d'observacions que son unes observacions que es troben molt lluny de les altres. El fet de que estiguin lluny vol dir que tenen valors de les variables explicatives que son bastant peculiars. 

Els residual plots son eienes de diagnosi de residus una mica més refinades
 residuals vs each explanatory variable
```{r}
residualPlots(mBest)
```
use order 2 polynomial for age
El que veiem molt clarament es que tots estàn molt centrats, vol dir que les decisions preses per construir el model han sigut força acertades, per exemple la de posar el teme quadràtic de la variable age.
La relació entre els residus i la variable age la linia blava ens diu que ja està capturada, per f.education.num tampoc veiem cap patró i cap desviació, i per capital.var igual.

Si volem entendre una mica el perfil fem servir la llibreria plot effects on hi veurem dibuixat l'efecte marginal de cadascuna de les variables en el model. 
```{r}
library(effects)
plot(allEffects(mBest))
```
El que ens diu basicament es que com més education.num més hour.per.week, com més capital.var també treballaràs més. Les sombres son la precisió que te aquest coeficient per valors molt alts de capital.var en la seva gràfica. Veiem que perdem molta precisió per aquests valors alts de capital.var perque hi ha molt poques observacions com veure a la entrega 2.

El influencePlot es un bubble plot. Podem veure quines observacions tenen un hatvalue determinat i te un residu que també son elevats. Ens indica quins son els residus student, els hat i la distancia de cook, que es el determinant si una observació es influent. En concret veiem que la 17040 es una observació molt influent amb una cookD de 0.042039239.
```{r}
influencePlot(mBest,id=list(method="identify"))  # Click the bubble you would like to identify
summary(mBest)

#Mirem que li pasa a la observació 17040
df["17040",]
```
Veiem que la variable capital.var es molt elevada, de 34095, per ser tant jove, 20 anys, es una observació influent, el fet de considerar aquesta persona dins de la mostra fa que els coeficients estimats per les variables explicatives canviin força, veiem que pertant a la categoria de f.education.num(10-12), una categoria que tenia un Pr(>|t|) més elevat.
Per treure els 5 individus que tenen alguna característica més rellevant fem:
```{r}
influencePlot(mBest,id=list(method="noteworthy",n=5))
```
Ens treu els que tenen la distancia de residu més gran o el que tenen el leverage més elevat o la cookD. Els que tenen les bombolles més grans son els que tenen una cookD major. Serien a priori les observacions més influents.
Mirem per exemple la observació 2902
```{r}
df["2902",]
#Veiem que es una observació que en totes les seves categories i relacions amb les altres doncs pertany a una minoria sempre, gent gran amb elevat numero d'estudis, treballant 75 hours.per.week, més de 50k, etc.
```

Ara el que farem es ampliar una mica les posibilitats de modelització, ampliarem una mica les variables explicatives que es poden posar en els models.

```{r}
vars_con
#m1<-lm(hours.per.week~.,data=df[,vars_con])
#mBest es el nostre model que hem fixat amb les nostres variables explicatives numèriques.
summary(mBest)
```
Hem d'interpretar uns resultats bàsics de tipus inferencial que tenen a veure amb els contrastos d'hipòtesis que surten en la taula de coeficients del summary. Aleshores per cadascun dels coeficients independenment, el que tenim es el contarast d'hipotesi coeficient = 0.

La valoració del model ja la vam fer mirant el valor de Multiple R-squared, i vam escollir el millor. Es el percentatge de la variabilitat del target que ve explicat pel nostre model de moment, un 15,63%.

El metode step es pot utilitzar per a una motorització segons el criteri de informacio vif, el que fa aquest metode es anar reduint un model gran eliminant les variables que no son rellevants segons aquest criteri, i aquest criteri de quantitat d'informació que el que fa es ponderar el que es la explicavilitat del model amb la seva complexitat, es a dir, es busca el model que sigui el més explicatiu posible pero que sigui el més simple posible.
```{r}
mBest2<-step(mBest,k=log(nrow(df))) # BIC
summary(mBest2)
```
Veiem que la explicavilitat es la mateixa.
obtenim AIC=22862.8, es el AIC que tindria el model després de suprimir algunes de les variables, pero veiem que no hem suprimit cap de les variables, el metode step s'ha quedat amb el model mBest. Per tant, ens fa veure que ja haviem fet correctament la modelització de mBest fins el moment.
El metode step proba una a una a suprimir totes les variables.
```{r}
vif(mBest2)
vif(mBest)
#(ens surt obviament el mateix ja que es exactament el mateix model)
```

## Addings factors
```{r}
summary(mBest)
```
Agafem el millor model fins el moment, mBest
Tenim amb mBest una explicavilitat del target del 15,63%, per intentar millorar el model mirem d'afegir variables que son factors. Fem servir el mateix mètode. La addició de factors es pot analitzar-se desde dos punts de vista, perimer es el d'afegir més informació nova al model, més variables explicatives noves, i segona de les maneres seria anem a veure si el tractament com a factor es un tractament més adequat per alguna de les variables que ja tenim en el model.

 Quins factors aporten alguna cosa nova al model?
La estretegia que seguim es que estem intentant fer una de les opcions mencionades anteriorment, quins factors aporten alguna cosa al model, com podem millorar el model que ja tenim a mBest afegint-hi factors.
```{r}
names(df)
```
Les variables factors més relacionades amb el target son:
No coloquem aquelles variables factor que ja hem estudiat anteriorment, la f.cvar que comporta a la substitució de f.cgain,f.closs. i f.education.num i obviament f.hours.per.week perque la variable numèrica es la nostra target, a més tampoc coloquem les que ja hem estudiat si substituir la variable numèrica pel seu factor tant si ho hem fet com si no.
```{r}
vars_fac<-names(df)[c(13,15:22)]
vars_fac
#Ara tornem a recordar quins son els factors més relacionats amb el target hours.per.week:
res.condes<-condes(df[,vars_fac],num.var=1)
#Agafem directament $quali
res.condes$quali
#Les que més relacionades estan son basicament: f.RelType, f.OccupationType, f.MaritalStatusType  
```
Pasem a contruir i a analitzar nous models a partir de l'addició de variables factor. 
Comencem amb un R2 més elevat, i en el nostre cas es: f.RelType amb R2 = 0.112074731, Aquest model li diem mfprova
```{r}
mfprova<-lm(hours.per.week ~ poly(age, 2) + f.education.num + capital.var+f.RelType, data = df)
summary(mfprova)
```
Veiem com s'han afegit les variables artificials per cadascun dels nivells del factor, hem d'anar en compte i no fer-li cas al pvalue que ens proporciona el summary d'aquestes variables, ja que realment formen part d'una de sola. Per deduïr si aquesta variable f.RelType es o no important, fem el métode Anova, un test d'efectes nets:
```{r}
Anova(mfprova)
```
En el resultats que ens mostra, podem veure com estan en el model les variables segons la variable nova afegida. Veiem que tenint f.education.num, capital.var i age amb termes quadràtics al nostre model, si li afegim f.RelType veiem que val la pena; tenim un Pr(>F) de < 2.2e-16. Per tant veiem que val la pena introduir la variable factor f.RelType en el nostre model.
A més a més, millorem la explicavilitat del model de un 15,63% a un 20,57%.

Per interpretar-ho primer fem una interpretació gràfica amb els efectes marginals:
```{r}
plot(allEffects(mfprova))
```
Veiem que aproximadament a partir dels 40 anys d'edat veiem que les hours.per.week van disminuint.
La eduaction.num, com més anys un individu hagi estudiat, més hours.per.week treballa.
Del f.RelType effect plot podem observar que ens treu uns intervals de confiança de les hours.per.week per cadascun dels grups que tenim definits segons f.RelType, veiem que els que més hours.per.week treballen son els husband, tenen un promig de hours.per.week treballades superior a 44, els que son wife-Other tenen un promig d'entre 38 i 40 hours.per.week, els child son els que tenen el promig més baix, i finalment els Not-inFamily que tenen un promig aproximat d'entre 42 i 43 hours.per.week aproximadament.
L'altre estudi que fem es la interpretació de les equacions.
```{r}
summary(mfprova)
#Com son models aniuats, mBest i mfprova
anova(mfprova,mBest)
BIC(mBest)
BIC(mfprova)
```
Per tant, com veiem que mfprova es millor actualitzem mBest amb aquest nou model:
```{r}
mBest<-lm(hours.per.week ~ poly(age, 2) + f.education.num + capital.var+f.RelType, data = df)
summary(mBest)
```
continuem intentant afegir-hi factors que ens ajudin a millorar el nostre model, els anem afegint un a un:
Ara tornem a recordar quins son els factors més relacionats amb el target hours.per.week:
```{r}
res.condes$quali
```
Les que més relacionades estan son basicament: f.RelType, f.OccupationType, f.MaritalStatusType, f.type. Ja hem afegit f.RelType on ens sortia a compte i millorava el model. 
Continuem amb f.OccupationType:
```{r}
mprovaContFact<-lm(hours.per.week ~ poly(age, 2) + f.education.num + capital.var+f.RelType+f.OccupationType, data = df)
summary(mprovaContFact)
#Veiem que afegint-hi f.OccupationType millorem la explicavilitat fins al 23.72%.
vif(mprovaContFact)
#Veiem que no hi ha colinealitats, podem procedir.
anova(mprovaContFact,mBest) #veiem que no son equivalents
BIC(mprovaContFact)
BIC(mBest)
#fem test d'efectes nets, veiem que l'addició del factor f.OccupationType si que es significatiu.
Anova(mprovaContFact)
```
Veiem a més que el BIC es menor amb l'addició de f.OccupationType, per tant actualitzem mBest amb mprovaContFact, a més te una major explicavilitat.
```{r}
mBest<-lm(hours.per.week ~ poly(age, 2) + f.education.num + capital.var+f.RelType+f.OccupationType, data = df)
```
Continuem amb f.MaritalStatusType:
```{r}
mprovaContFact<-lm(hours.per.week ~ poly(age, 2) + f.education.num + capital.var+f.RelType+f.OccupationType+f.MaritalStatusType, data = df)
summary(mprovaContFact)
#Millorem poc, menys d'1% la explicavilitat, pero la millorem fins a un 23,94%.
vif(mprovaContFact) #veiem que el vif de f.RelType i f.MaritalStatusType estàn força elevats. son distants a 1.
BIC(mprovaContFact)
BIC(mBest)
```
El BIC es major al afegir f.MaritalStatusType, a més, veiem que la explicavilitat no millora gairabé res. No actualitzem llavors mBest.

Continuem amb f.type:
```{r}
mprovaContFact<-lm(hours.per.week ~ poly(age, 2) + f.education.num + capital.var+f.RelType+f.OccupationType+f.type, data = df)
summary(mprovaContFact)
#Millorem la explicavilitat fins al 24.41%.
vif(mprovaContFact)
```
Veiem que tenim valors força elevats per f.OccupationType i per f.type, per tant veiem colinealitat amb aquestes. No podem seguir així i per tant prescindim millor de f.type. Per tant no actualitzem amb f.type.
```{r}
summary(mBest)

mprovaContFactStep<-step(mBest,k=log(nrow(df)))
summary(mprovaContFactStep)
```
tot i que veiem que el métode step redueix el model al treure capital.var i f.education.num que el que fa es ponderar el que es la explicavilitat del model amb la seva complexitat, considerem que poden donar més a alguna explicació.
```{r}
plot(allEffects(mBest))
```
Provem d'afegir totes les altres variables factor i procedir a executar el métode step per veure si les acabem afegint al nostre model o no.
```{r}
mprovaContFact2<-lm(hours.per.week ~ poly(age, 2) + f.education.num + capital.var+f.RelType+f.OccupationType+f.RaceType, data = df)
summary(mprovaContFact2)
vif(mprovaContFact2)
```
Veiem que el f.EduType tindria colinealitat amb f.education.num en el nostre model i la suprimim. f.RaceType no te colinealitat amb ningú pero ens aporta molt poc al model. Tot i axí fem el métode step per veure que ens proposa:
```{r}
mprovaContFactStep2<-step(mprovaContFact2,k=log(nrow(df)))
summary(mprovaContFactStep2)
```
Veiem que l'step precindeix de f.RaceType, fa el procediment com anteriorment haviem analitzat.

Recordem com tenim el model actual mBest:

hours.per.week ~ poly(age, 2) + f.education.num + capital.var + f.RelType + f.OccupationType, data = df)

Pasem a entendre millor aquest model obtingut:
De forma gràfica amb el allEffects:
```{r}
plot(allEffects(mBest))
```
Veiem que amb l'edat, a partir dels més o menys 40 anys, com més edat, es van baixant les hours.per.week treballades. Amb el f.RelType effect plot podem observar que ens treu uns intervals de confiança de les hours.per.week per cadascun dels grups que tenim definits segons f.RelType, veiem que els que més hours.per.week treballen son els husband, tenen un promig de hours.per.week treballades superior a 44, els que son wife-Other tenen un promig d'entre 38 i 40 hours.per.week, els child son els que tenen el promig més baix, estàn entre 0 i unes 39 hours.per.week i finalment els Not-inFamily que tenen un promig aproximat d'entre 42 i 43 hours.per.week.
Els f.type que més hours.per.week treballen son els f.typ-self-emp-inc.
```{r}
summary(mBest)
```
Veiem que els residus sembla que segueixen tenint algun patró, tot i que veiem que hem millorat, sobretot es veu força visible en els gràfics de Residuals vs Fitted i en el de Residuals vs Leverage.Tenen una tendència positiva per valors baixos i una de negativa per valors alts en la gràfica Residuals vs Fitted. Sobre la normalitat dels residus seguim tenint un desajust, sobretot pels valors negatius. Scale-Location seguim veient la linia vermella no recta, per tant seguim tenint problemes. La validació dels residus ens diu que amb aquest model tenim problemes. Això el que ens està dient es que els residus encara contenen informació, tenen una certa estructura, tenen un patró, no estàn centrats en el 0, no son soroll blanc.
```{r}
par(mfrow=c(2,2))
plot(mBest,id.n=0)
par(mfrow=c(1,1))
```


## Residual analysis: Heterokedasticity - Non-normality
## Transformations
## Interactions

Ara ja hem fet la introducció de variables numèriques i de factors necessaris en el nostre model, a continuació continuem intentant millorar el model tot buscant interaccions entre variables numèriques i factors, la interacció entre els diferents factors.

mBest<-lm(hours.per.week ~ poly(age, 2) + f.education.num + capital.var+f.RelType+f.OccupationType, data = df)

# Interactions between numeric variables and factors:
```{r}
summary(mBest)
mBestI1<-lm(hours.per.week ~ (poly(age, 2) + capital.var)*(f.RelType+f.OccupationType+f.education.num),data=df)
summary(mBestI1)
```
Veiem que ara, tenim una explicavilitat millorada de un 23,72% fins a un Multiple R-squared:  0.2633 (26,33%)

Apliquem el métode step
```{r}
mBestI12<-step(mBestI1,k=log(nrow(df)))  # BIC
```
El resultat d'aplicar el mètode step sobre el model mBestI1 es:
hours.per.week ~ poly(age, 2) + f.RelType + f.OccupationType + f.education.num + poly(age, 2):f.education.num
Veiem que precindeix de capital.var i age només interacciona amb f.education.num
```{r}
summary(mBestI12)
#Tot i haver simplificat el model amb el mètode step, explicavilitat baixa fins a un Multiple R-squared:  0.2454
BIC(mBestI1)
BIC(mBestI12)
#Veiem que el BIC de mBestI12 es considerablement menor al de mBestI1.
anova(mBestI12,mBestI1)   # no equivalents, but mBestI12 best
Anova(mBestI12)
#Gracies al test d'efectes nets veiem que totes les variables aporten al model.
vif(mBestI12)
#De moment, per tant ens quedem amb mBestI12, ja que un AIC menor.
##hours.per.week ~ poly(age, 2) + f.RelType + f.OccupationType + f.education.num + poly(age, 2):f.education.num, de moment ens quedem amb aquest model.

plot(allEffects(mBestI12))
plot(effect("poly(age, 2)*f.education.num",mBestI12))
```
Tot i que no estiguin en el model veiem els gràfics
Es veu clarament com les persones que tenen més estudis (f.education.num(13-16)), quan son joves son els que es veu clarament que més treballen, en canvi un cop es vei que superen una edat i ja superen els seus estudis acaben sent els que treballen més hores. Tot el contrari pasa en el grup de f.education.num(9), on comencen a treballar moltes més hores de joves i a mesura que es van fent grans van dismininuint més el hours.per.week respecte el grup anteriorment comentat.
veiem com ja hem vist en numerosos casos, que els husband son els que més hores treballen i que segons la edat, va disminuint segons s'augmentem els anys. Veiem també que els que més hores treballen son els self-emp, i a més també els que treballen en general son els que tenen una age d'entre 30 i 50 anys.
Aquest gràfics son els que no apareixen en el nostre model optimitzat per el mètode step, tot i així els posem perque trobem interesant les petites conclusions anteriorment esmentades.
```{r}
plot(effect("poly(age, 2)*f.RelType",mBestI12))
plot(effect("poly(age, 2)*f.OccupationType",mBestI12))
 
#Actualitzem mBest amb el model mBestI12
mBest<-mBestI12
#ANEM PER AQUI
# Interpretation of mBest realitzant alguna equació.
summary(mBest)
```
hours.per.week ~ poly(age, 2) + f.RelType + f.OccupationType + f.education.num + poly(age, 2):f.education.num, data = df)
equacions:
Tot i com ja hem comentat anteriorment en aquest estudi algunes de les categories de f.education.num no tenen un Pr(>|t|) suficientment petit, però vàrem decidir quedarnos amb la variable en el model, a més el mètode step en aquest cas el preserva.
Fem una interpretació del nostre model, on un individu tingui caracterítiques de husband, Emp-AltCarrec i education.num(13-16), la més elevada, les hours.per.week d'aquest individu tindria vindria donada per la següent equació:
 y=(39.7628+0-2.4049+1.3031)+(-246.6231+26.8566)*age
```{r}
par(mfrow=c(2,2))
plot(mBest,id.n=0)
par(mfrow=c(1,1))
```
Veiem que els residus contenen més informació de la que contenien abans de fer la interacció entre variables numèriques i factors. Veiem que continuem tenint desviacions sobre la linia recta, degut a que hi ha uns residus estanderitzats que son més negatius del que haurien de ser. Continuen tenint un patró, no compleixen amb la normalitat. Veiem que la variança no es constant en el gràfic d'Sacle-Location. Per veure els indiviud més influents fem servir el influence plot.
```{r}
influencePlot(mBest,id=list(method="identify")) 
#Treiem dos de les observacions amb CookD més elevat:
#        StudRes        Hat       CookD
#15377  1.452627 0.03506796 0.004509978
#17040 -2.382310 0.04085416 0.014206424
```

## Interactions between factors
Prenem el model mBest: hours.per.week ~ poly(age, 2) + f.RelType + f.OccupationType + f.education.num + poly(age, 2):f.education.num, data = df)
mBestI1<-lm(hours.per.week ~ poly(age, 2)*f.education.num + f.RelType + f.OccupationType + f.education.num, data = df)
```{r}
mBestIF1<-lm(hours.per.week ~ poly(age, 2)*f.education.num + f.RelType + f.OccupationType, data = df)
summary(mBestIF1)
BIC(mBestIF1)
```
El que estem intentant veure es si dos a dos son factibles i de fet si ens aporten alguna cosa bona en l'explicavilitat del model aquestes interaccions entre dos factors.

El següent es model sobreparametritzat, veiem que obtenim major explicavilitat, pero major BIC que a mBestIF1, que es el anteriorment tractat, la interacció de poly(age, 2)*f.education.num es una interacció que ja hem resolt anteriorment que aporta al nostre model. Per tant, ara ens ocupem dels factors f.RelType i f.OccupationType.
```{r}
mBestIF12<-lm(hours.per.week ~ poly(age, 2)*f.education.num + f.education.num*(f.RelType + f.OccupationType), data = df)
summary(mBestIF12)
BIC(mBestIF12)
#Fem alguna prova més
mBestIF3<-lm(hours.per.week ~ poly(age, 2)*f.education.num + f.RelType*(f.education.num + f.OccupationType), data = df)
summary(mBestIF3)
BIC(mBestIF3)
#Pitjor explicavilitat i BIC que mBestIF12, ens seguim quedant amb aquest.
mBestIF4<-lm(hours.per.week ~ poly(age, 2)*f.education.num + f.OccupationType*(f.education.num + f.RelType), data = df)
summary(mBestIF4)
BIC(mBestIF4)
```
Tot i ser molt poc millor l'explicavilitat en aquest model mBestIF4 que en el mBestIF12, el BIC es superior en mBestIF4, ens continuem quedant amb mBestIF12 per fer el mètode step.
Simplifiquem el model amb el mètode step, ja que el tenim sobreparametritzat i necessitem simplificar-lo.
```{r}
mBestIFstep<-step(mBestIF12,k=log(nrow(df)))  # BIC
BIC(mBestIFstep)
summary(mBestIFstep)
```
Veiem que el mètode step ens diu que ens quedem amb un model com el inicial amb el que hem començat l'apartat de interacció entre factors, és a dir, el model amb el que continuarem serà el següent:
hours.per.week ~ poly(age, 2) + f.education.num + f.RelType + f.OccupationType + poly(age, 2):f.education.num
Es un model amb un BIC de 36389.78 i una explicavilitat de 24,54%.
No hem trobat per tant cap interacció entre factors nova que ens aporti al model, tot tenint en compte la solució proposada pel mètode step.

Comparem amb un test anova, el nostre mBest amb el millor que haviem pogut treure de interaccions entre factors abans d'aplicar-hi el mètode step, aquest ha sigut el model mBestIF12:
```{r}
anova(mBest,mBestIF12) #Veiem que no son equivalents, a més el model mBest es millor segons el mètode step.
Anova(mBest)
```
Veiem que totes les variables tenen rellevància en el model, la única interacció que es conserva es la de poly(age, 2):f.education.num, alhora d'interpretar el test d'efectes nets sempre hem de tenir en compte el que tingui jerarquia més alta que corrovorarà que el de menor pes també serà necessari (sempre i quan òbviament ens estiguem referint a la mateixa variable).
```{r}
vif(mBest)
mBest<-lm(hours.per.week ~ poly(age, 2)*f.education.num + f.RelType + f.OccupationType, data = df)
#Els plots effects corresponents a aquest model mBest ja han sigut per tant mostrats anteriorment, i els plots referents a l'anàlisis dels residus també.
```

## Transformation of numeric target
Ara pasem al tema de transformacions, que te a veure amb la transformada boxcox, la transformada boxcox es aplicable a variables de resposta que son numèriques i no negatives.
Aleshores, la transformació sobre el target es una transformació que ek que intenta es millorar les propietats del model lineal, és a dir, que els residus siguin més normals, almenys que siguin menys simètrics.

El model fins ara tenim que es:
(hours.per.week ~ poly(age, 2)*f.education.num + f.RelType + f.OccupationType, data = df)
```{r}
summary(mBest)
library(MASS)
boxcox(hours.per.week ~ poly(age, 2)*f.education.num + f.RelType + f.OccupationType, data = df) 
```
Veiem que crea un diagrama on en aquest gràfic veiem que lambda, el que es veu es que va millorant a mesura que la tranformació que se li aplica, la lambda que li sembla recomanable està al voltant de 1,2. Es la millor transformació per assolir unes millors propietats del model. seria y^1.2. Si el model que tenim entre mans es un model que te una explicavilitat alta i veiem que encara el podem millorar doncs aleshores hem de fer tot lo posible, pero introduir en un model tant dolent com aquest una transformació per millorar una mica les propietats pero de fet sabem que no es posible explicar be aquest target aleshores ens resistim a introduir variables explicatives o transformacions que siguin extranyes.

Amb això hem acabat la modelització utilitzant un patró normal del target, que es aplicable a variables amb un significat numèric.

Ara un cop tenim el model definitiu hem de fer la part del model validation, i no podem donar per vàlid cap model que tingui variables que tinguin outliers dels residus o observacions molt influents. Aquestes observacions influents venen per la distancia de cook. Tot i que ja hem estat fent validacions al llarg de la construcció del model final, en fem a continuació una última:
```{r}
influencePlot(mBest,id=list(method="identify"))
```
Ens treu els que tenen la distancia de residu més gran o el que tenen el leverage més elevat o la cookD.
```{r}
influencePlot(mBest,id=list(method="noteworthy",n=5))
```
Veiem que tenim observacions amb un residu estanderitzat gran, i els hatvalues, el factor d'anclatge hi ha observacions que estan fora del llindar que ens diria quan el factor d'apalancament comença a ser notable, és a dir, les particularitats que tenen les variables explicatives que estan lluny del centre de gravetat del núvol d'observacions. Veiem que hi ha clarament observacions que son problemàtiques. Per posar un exemple veiem com la observació 28369 té una distància de cook de 1.704652e-02, i te un residu StudRes molt elevat, concretament de 4.3513923. Aquestes observacions hem de veure que els hi pasa i veure si son influents. Determinar si una observació es influent en difinitiva es determinar-les utilitzant un boxplot, on les determinem mitjançant les cookD.
```{r}
boxplot(cooks.distance(mBest))
abline(h=0.010,col="blue",lwd=2)
```
sent estrictes marquem el limit de cookD en 0.010, li posem la cota en aquest valor. Les que ens preocupen son les que estan notablement més lluny de les demés.
Veim que tenim dues observacions per sobre de la distancia de cook superior a 0.010. Anem a veure aquestes dues observacions:
Veiem que son les observacions 11733 que està just al límit de la cookD marcada ja que te una cookD=1.036282e-02, i l'altre es l'observació 28369 amb una cookD=1.704652e-02. Anem a veure les seves característiques:
```{r}
df["11733",]
```
Veiem que es una dona de 76 anys, unmarried, en concret es widowed, que treballa 40 hores setmanals, es f.Occupationtyp-Emp-AltCarrec, amb poc nivell d'estudis f.education.num(1-8), f.Edutyp-Dropout. Sería un individu atípic, pero no tant com el següent.
```{r}
df["28369",]
```
Veiem que la característica a destacar més es que es un home de 73 anys i treballa 75 hours.per.week, un número molt elevat per la seva edat, i per a qualsevol. Està Married-civ-spouse, en concret es husband, workclass Private amb un elevat nivell d'estudis f.education.num(13-16), en concret 13. Sería un individu força atípic.

Veiem que si incloem aquestes dues variables al model aleshores fem variar els coeficients del model. No hem de tolerar que dos observacions canviin les relacions entre les explicatives i el target. Les hem de suprimir.
```{r}
qnorm(0.995)
```
De moment tenim dues observacions a suprimir: la 28369 i la 11733, segons la cookD, ara fem una altre repasada tot utilitzant llindar per els StudRes, on ens surt com a limit el [-2.575829,2.575829] a un interval de confiança del 99%, totes les observacions que es trobin fora d'aquest rang es consideren com a misfit i seràn eliminades.
```{r}
ll<-which(abs(rstudent(mBest))>2.575829);ll
```
Mirem el total de les observacions que tenen un StudRes superior a 2.575829 en valor absolut. Veiem que dins d'aquesta llista estàn les que haviem tret en amb la comanda influencePlot(mBest,id=list(method="noteworthy",n=5)), aquestes seràn les que eliminarem del df, en concret son: 1471, 7776, 9165, 16065, 27126, 28357 i 28369. Aquestes observacions a més, estàn molt per sobre del valor 2.575829, algunes d'elles superan el valor 4, decidim eliminar-les.
De moment les observacions que eliminem son: 28369, 11733, 1471, 7776, 9165, 16065, 27126, 28357 i 28369.

Ara continuem l'analisi de quines variables hauriem d'eliminar tot utilitzant els hatvalues:
Els hatvalues son valors que estan entre 0 i 1 i son els que es coneixen com a Factor d'apalancament/anclatge o leverage. Aleshores totes les observacions que tenen el factor d'anclatge molt elevat vol dir que son extranyes i poden ser influents.
```{r}
3*mean(hatvalues(mBest))
llhii<-which(hatvalues(mBest)>3*mean(hatvalues(mBest)));length(llhii)
llhii
```
Potential inﬂuent observations are those whose leverage is above three times the mean leverage (or 3p/n); 0.01343648. Veiem que hi ha 84 observacions cumplint aquest criteri, les mes influents del model apareixen en el resultat de la comanda influencePlot(mBest,id=list(method="noteworthy",n=5)), mirem quines d'elles apareixen en la llista dels 84 obtinguts en la llista llhii i els eliminem del nostre df, per tant al final d'aquest analisis ens queda que les observacions potencialment influents i hem d'eliminar son: 28369, 11733, 1471, 7776, 9165, 16065, 27126, 28357, 6001, 8695, 10140, 11533 i 18273. Per tant, acabem havent d'eliminar totes aquelles que ens havia dit la comanda influencePlot(mBest,id=list(method="noteworthy",n=5)) que eren les més influents segons els valors de StudRes, hatvalues i cookD.

Les eliminem:
```{r}
par(mfrow=c(2,2))
plot(mBest,id.n=0)
par(mfrow=c(1,1))
```
eliminar i actualitzar mBestEliminar
creem un nou df de proba, veiem que la observació 6001 es la unica que te un fnlwgt de 84616, borrem les observacions anteriorment mencionades buscant-les pel seu fnlwgt.
```{r}
ll<-which((df$fnlwgt=="84616")|(df$fnlwgt=="105886")|(df$fnlwgt=="312500")|(df$fnlwgt=="120939")|(df$fnlwgt=="280169")|(df$fnlwgt=="157593")|(df$fnlwgt=="323627")|(df$fnlwgt=="107814")|(df$fnlwgt=="321824")|(df$fnlwgt=="29020")|(df$fnlwgt=="152900")|(df$fnlwgt=="142370")|(df$fnlwgt=="86111")) ;length(ll)
if( length(ll)>0) dfBestEliminar<-df[-ll,]
mBestEliminar<-lm(hours.per.week ~ poly(age, 2)*f.education.num + f.RelType + f.OccupationType, data = dfBestEliminar)
```
un cop eliminades mirem la situació des residus en del nostre model mBest i mirem si ens diu es que en els residus encara contenen informació, tenen una certa estructura, tenen un patró, no estàn centrats en el 0, no son soroll blanc.
```{r}
residualPlots(mBestEliminar)

par(mfrow=c(2,2))
plot(mBestEliminar,id.n=0)
par(mfrow=c(1,1))

#Actualitzem el nostre df amb les 13 observacions esborrades.:
df<-dfBestEliminar
```
Veiem que tot i eliminar les observacions que ens ha indicat influencePlot(mBest,id=list(method="noteworthy",n=5)), veiem que els residus segueixen tenint un patró, no estàn centrats en el 0, continuem tenint problemes, no es soroll blanc, continuen tenint informació. També es causat a que el model es dolent ja que te una baixa explicavilitat. Hem de tenir en compte que no estem eliminant totes les observacions que no cumplien amb els requisits dels limits marcats per StudRes, hatvalues i cookD, sino que hem eliminat només aquells que estaven fora dels limits i a més eren els més influents mencionats per el influencePlot.

A continuació el que fem es com modelar una variable de resposta quan aquesta variable de resposta es un factor, en el nostre cas es un factor que ens diu si guanya o no guanya més de 50k $ l'any, la variable Y.bin.
 
# Binary target

Primer farem una dicisió, per finalitats de validació de models, aleshores el que fem es fer un split, dividir entre dos mostres, la mostra de treball i la mostra de test.
Per fer-ho considerem el 75% de les observacions triades a l'atzar com la mostra de treball i el 25% restant com la mostra de test, ens ajuda a fer una validació.
## Split into 2 samples: Work and Test
```{r}
names(df)
# 75% to Working Set and 25% Test
# Útil per les confusion tables
set.seed(14031997)
ll<-sample(1:nrow(df),nrow(df)*0.75)
ll<-sort(ll)
dfwork<-df[ll,]
dftest<-df[-ll,]
```
Treballarem exclusivament amb la mostra de treball, amb dfwork, per fer el disseny de quin es el millor model per explicar el target. I després un cop tinguem el millor model aleshores per veure la capacitat predictiva i si hi ha sobreparametrització calcularem la capacitat predictiva en la mostra de treball i en la mostra de test, això ho farem mitjançant confusion tables.

## Modelling
## Using only covariates as explanatory vars
Dintre del modelatge no tenim cap particularitat, comencem com en el tema pasat, comencem volent veure la relació entre la variable de resposta, que ara es binaria, i abans era la numèrica hours.per.week, segons les variables que eren explicatives numèriques.
```{r}
names(df)
#Agafem les variables numèriques, les posem en la llista de vars_exp
vars_exp<-names(df)[c(1,5,11:13,26)];vars_exp
#> vars_exp<-names(df)[c(1,5,11:13,28)];vars_exp
#[1] "age"            "education.num"  "capital.gain"   "capital.loss"  
#[5] "hours.per.week" "capital.var"   
summary(dfwork[,vars_exp])
```
Primeres de les coses que fem, tenim el target, que fiquem el target, que es el Y.bin. Veiem un resum de la cantitat d'observacions que guanyen <=50K i els que guanyen >50K, després també ho veiem en %, on veiem que els que guanyen >50K representen 23.43% i els que guanyen <=50K representen la resta.
```{r}
summary(dfwork$Y.bin)
prop.table(table(dfwork$Y.bin))
# In general
```
Fem servir catdes per veure quines variables numèriques estan relacionades amb el target.
```{r}
#catdes(dfwork[,c("Y.bin",vars_exp)],1)
```
Veiem que globalment les variables numeriques que estan relacionades amb el target, mirem Eta2. Com més intensa es la relació entre la variable numèrica i el target més alt serà Eta2, per tant, veiem que les variables que estan més relacionades son education.num, capital.gain, capital.var, hours.per.week, age i el capital.loss. Com que en tenim poquetes el que ens està dient aquest output es que totes les que hem triat a vars_exp estàn relacionades amb la variable target binaria.

 Les agafem totes

Una de les coses que canviem respecte a la modelització de hours.per.week es que ara treballem amb el mètode glm.
El primer paràmetre es la equació que defineix el target (Y.bin). Li posem family=binomial perque li hem de dir al model quina es la distribució, quin es el model probabilista que volem per al target. No hi posem capita.var, que te una dependencia lineal perfecte amb altres variables.
```{r}
mYbin<-glm(Y.bin~age+education.num+capital.gain+capital.loss+hours.per.week,family=binomial,data=dfwork)
summary(mYbin)
```
Veiem que tenim una primera part que es la fórmula, després veiem un aparatt que te a veure amb els residus, deviance residuals, la deviance es una mesura de discrepància entre la observació i la predicció, la predicció que farà aquest model es una probabilitat, és a dir, la probabilitat de resposta positiva que es guanyar >50k $ l'any.

Ens fixem en la part corresponent als coefficients, aqui tenim la taula de resultats que per totes les variables seleccionades en el model més un terme que es el de la constant, veiem l'estimador del paràmetre, els seu standard error. A partir de tenir l'estimador i Std. Error doncs ja es pot tenir quin es el z value de la hipòtesi nula coeficient = 0. 

A la part final del summary veiem que ens diu un  Null deviance que seria la mesura de discrepància corresponent al model null, que son 4000.8  on 3673  degrees of freedom.

Després tenim el Residual deviance, que es una discrepància que es de 2995.7  on 3668  degrees of freedom (amb tants graus de llibertat com numero d'observacions del model menys numero de paràmetres en el model).
Finalment tenim el AIC.
La principal diferència amb el modelatge de hours.per.week es que ara no obtenim el coeficient de determinació del model. 

Mirem si hi ha un altre model que tingui menys variables i es pugui donar la mateixa explicavilitat, intentem fer comparacions de models.
El nou model es canviar capital.gain i capital.loss per la variable que abans hem tret capital.var.
```{r}
mYbinA<-glm(Y.bin~age+education.num+capital.var+hours.per.week,family=binomial,data=dfwork)
summary(mYbinA)
```
Veiem que el residual deviance es més alt, Residual deviance: 3091.8, desde aquest punt de vista sembla que la discrepancia queda millor capturada en aquest segon model. 
 mYbinA ? nested into mYbin?
```{r}
anova(mYbinA,mYbin,test="Chisq")  # No és correcte ja que aquest models no son encaixats, el model petit hauria d'estar inclòs en el model gran, i això no és així.
#Per tant comparem els dos models que no son nested utilitzem el BIC:
BIC(mYbin,mYbinA) # Best mYbin, minimum BIC, el model que te les dues variables per separat i no capital.var.
#       df      BIC
#mYbin   6 3044.961
#mYbinA  5 3132.865
```
Mirem si totes les variables incloses son significatives amb el test d'efectes nets, en aquest cas com les variables explicatives son numèriques doncs seria un test d'efectes nets en el que el p-value que ens dona es el mateix p-value que ens surt en la taula anova, no es massa informatiu. El test Anova d'efectes nets funciona molt bé quan hi ha variables explicatives que son factors.
```{r}
Anova(mYbin,test="LR") #Veiem que totes les variables aporten al model.
#Utilitzem el métode step per buscar el model que sigui el més explicatiu posible pero que sigui el més simple posible. 
mYbinB<-step(mYbin,k=log(nrow(dfwork))) # Use k=log(nrow(dfwork)) for BIC
```
Veiem que totes les variables son útils i que no poden desaparèixer del model. Per tant, el millor model continua sent el mYbin.
De totes maneres veiem si tenim colinealitat abans de deixar totes les variables:
```{r}
vif(mYbinB) 
#Veiem que no tenim colinealitat, estan al voltant de 1:
#          age  education.num   capital.gain   capital.loss hours.per.week 
#     1.020456       1.007595       1.006165       1.005288       1.016327
```
Check colinearity, sembla que ens les hem de quedar totes, pero podem utilitzar-les tal com estàn o potser seria millor aplicar algun tipus de transformació. Per això mirem la relació marginal entre la resposta i les variables explicatives, això ho fem am el matginalModelPlots:
```{r}
marginalModelPlots(mYbinB)
```
Primer mirem la relació que hi ha entre la probabilitat de resposta positiva de Y.bin i la variable age, veiem que sembla que el blau (les dades), el que veiem es que tenim un increment de la probabilitat de guanyar més diners que estaria més o menys en els 50 anys. Segons el que ens diu el model (la linia vermella),aleshores veiem que en funció de l'edat veiem que el que fa el model es fixar un màxim al voltant dels 60 anys i que disminueix un cop arribat al seu màxim de manera molt més relaxada del que ho fan les dades.
La relació entre la variable age i la probabilitat es no lineal. Veiem que hi ha un cert desajust en l'edat entre el model i les dades.

En el education.num sembla que no hi ha gairebé gens de desajust. Com més anys estudiin els individus més probabilitat de guanyar més diners tenen.

Sembla que també hi ha una certa correspondència, com més capital.gain més probabilitat tenen de guanyar +50k $. Tot i això veiem que hi ha observacions extranyes.

En el capital.loss també veiem que tenim un cert desajust i en les hours.per.week no en tenim gairabé gens, similar al que pasa amb education.num, és a dir, com més hores treballi un individu més probabilitat té de guanyar més diners.

Globalment veiem que entre el model i les dades hi ha una certa correspondència, això ens indica que el model no funciona del tot malament. Veiem que a mesura que augmenten de forma general els valors de les variables explicatives també augmenta la probabilitat de guanyar més de 50k $.

Intententem fer alguna transformació en les variables on tenim més desajust per així millora-la i tenir més correspondència entre el model i les dades.
Fem per la variable age:
El capital.loss també es una variable juntament amb la variable age que te menor corresppondencia les dades amb el model. Però capital.loss es una variable força difícil, perque només te uns quants valors que son diferents a 0, i potser interesa més utilitzar-la com a factor, que ja ho veurem més endavant. El que si que veiem es que a major capital.loss vol dir que també es tenien més diners i que per tant tambe la probabilitat de guanyar més de 50k $ l'any es més elevada.
```{r}
mYbinC<-glm(Y.bin ~ poly(age,2) + education.num + capital.gain + capital.loss + hours.per.week, family=binomial,data=dfwork)

summary(mYbinC)
```
Veiem que te Residual deviance: 2891.1, apliquem ara el test de la deviance:
 Deviance test
```{r}
anova(mYbin,mYbinC,test="Chisq")  # rebutjem la H0, com que la rebutjem vol dir que els dos model no son equivalents, el gran fa millor feina que el petit, ens quedem amb el model mYbinC.
```
Analysis of Deviance Table

Model mYbin: Y.bin ~ age + education.num + capital.gain + capital.loss + hours.per.week
Model mYbinC: Y.bin ~ poly(age, 2) + education.num + capital.gain + capital.loss + hours.per.week
Actualitzem llavors mYbin amb mYbinC:
```{r}
mYbin <- mYbinC
#  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    
#1      3668     2995.7                          
#2      3667     2891.1  1   104.65 < 2.2e-16 ***
marginalModelPlots(mYbin)
```
Veiem que la age ara està molt més ajustada, veiem que hem millorat molt el model. La capital.loss que ara veiem que es la que més desajustada està, la preferim provar de treballar com a factor binari i no fer-li una transformació com li hem fet a la variable age per les seves característiques.

Mirem els residus: fem servir l'eina de residualPlots
```{r}
residualPlots(mYbin)
```
Veiem que hi ha observacions que son outliers dels residus. Aquestes observacions son interesants de ser detectades, ja vam detectar i eliminar observacions massa influents en el pasat tema (amb hours.per.week). Aquí no mirem patrons, a diferencia del pasat tema. Ens hem de fixar amb la linia de color rosa, quan aquesta es plana es que el model està força bé, que es el nostre cas. Veiem que el gràfic més important, el que te el predictor lineal per una banda i després els residus per una altre, veiem que la linia rosa es plana, cosa molt positiva, es un model molt satisfactori de moment utilitzant només variables explicatives numèriques.

A continuació podem veure la sortida que ens ha donat l'execució de residualPlots(mYbin) per consola:
               Test stat Pr(>|Test stat|)    
poly(age, 2)                                 
education.num     0.4909           0.4835    
capital.gain     27.9129        1.269e-07 ***
capital.loss      5.4559           0.0195 *  
hours.per.week    1.4101           0.2350 

Ens fixem que per cadascuna de les variables explicatives numèriques sense transformacions polinòmiques podem veure una h0 que acaba treient un p-value, aquesta h0 diu que els residus son prou aleatoris marginalment segons els valors de les variables explicatives, és a dir, en tots els casos que s'accepta la h0 vol dir que el tractament que s'ha donat a la variable numèrica es l'adequat per la modelització, almenys desde el punt de vista dels residus. Veiem que els que rebutjen més la h0 son el capital.gain i el capital.loss, per tant estaria bé buscar-hi una solució.

Ara ja tenim el millor dels models pel que fa a les variables explicatives que son numèriques, pasem a analitzar si l'addició de variables explicatives factor poden millorar-lo.

## Adding factors
El primer que fem es substituir capital.gain i capital.loss per el seus fatcors corresponents. Recordem que un cop substituim per factors haurem de comparar per l'indicador BIC, ja que no son model encaixats.

 De moment el millor model fins al moment es el mYbin:
(Y.bin ~ poly(age,2) + education.num + capital.gain + capital.loss + hours.per.week, family=binomial,data=dfwork)
```{r}
names(df)
#Probem amb f.cvar i amb l'altre opció de f.cgain i f.closs
mYbinFactor1<-glm(Y.bin ~ poly(age,2) + education.num + f.cvar + hours.per.week, family=binomial,data=dfwork)
mYbinFactor2<-glm(Y.bin ~ poly(age,2) + education.num + f.cgain + f.closs + hours.per.week, family=binomial,data=dfwork)

BIC(mYbin,mYbinFactor1,mYbinFactor2)
#Veiem que el model amb millor BIC segueix sent mYbin, on no utilitzem cap dels factors.
#             df      BIC
#mYbin         7 2948.524
#mYbinFactor1  7 3020.979
#mYbinFactor2  7 3020.979
```
Probem amb f.age i f.education.num:
```{r}
mYbinFactor3<-glm(Y.bin ~ f.age + education.num + capital.gain + capital.loss + hours.per.week, family=binomial,data=dfwork)
mYbinFactor4<-glm(Y.bin ~ poly(age,2) + f.education.num + capital.gain + capital.loss + hours.per.week, family=binomial,data=dfwork)
BIC(mYbin,mYbinFactor3,mYbinFactor4)
#             df      BIC
#mYbin         7 2948.524
#mYbinFactor3  8 2991.185
#mYbinFactor4  9 2981.745
```
Veiem que continua sent millot mYbin
Intentem en el model de mYbinFactor2 aplicar-li un mètode step
```{r}
mYbinFactor2Step<-step(mYbinFactor2,k=log(nrow(dfwork)))
```
Veiem que el mètode step ens deixa tal qual el model, per tant deixem el capital.loss i el capital.gain com a numèriques dins del nostre model tal i com hem comprovat just abans amb els BIC's.

Continuem intentant afegir-hi factors en el millor model fins al moment, només considerem aquelles variables que son factors i els efectes principals.
```{r}
names(df)
vars_edis<-names(df)[c(10,16:25,27,28)]
vars_edis
```
 [1] "sex"                 "f.type"              "f.RelType"          
 [4] "f.CountryType"       "f.EduType"           "f.MaritalStatusType"
 [7] "f.OccupationType"    "f.RaceType"          "f.age"              
[10] "f.cgain"             "f.closs"             "f.cvar"             
[13] "f.education.num" 

 Utilitzem catdes per veure quines son les variables factor amb més relació amb les categories del target (guanyar més o menys de 50k $ a l'any) i les que hauriem de provar abans d'afegir al nostre model.

catdes(dfwork[,c("Y.bin",vars_edis)],1)

Veiem que les que globalment estan més associats amb el target son: f.MaritalStatusType, f.RelType, f.education.num(tot i que no ens interesa ja que en el model tenim la variable numèrica educació), f.EduType (tampoc seria del tot necesaria perque es trepitja amb education.num), f.OccupationType, f.cvar, f.age (no el tenim en compte perque ja tenim l'edat tractant-la amb els termes lineals i quadràtics (poly(age,2))), f.cgain, sex, f.type,.. A continuació ordenats de més a menys:


                          p.value df
f.MaritalStatusType 1.169160e-165  3
f.RelType           2.627218e-152  3
f.EduType           5.195629e-100  7
f.education.num      1.487420e-88  3
f.OccupationType     1.578939e-87  7
f.cvar               1.201203e-76  2
f.age                2.548208e-66  3
f.cgain              9.743318e-60  1
sex                  8.111948e-37  1
f.type               1.145503e-20  4
f.closs              6.133173e-16  1
f.RaceType           5.308757e-11  2
f.CountryType        2.980946e-05  1

Use more significant gross effect factors
```{r}
#mYbinFactor3<-glm(Y.bin ~ (poly(age,2) + education.num + capital.gain + capital.loss + hours.per.week)+(f.MaritalStatusType+f.RelType +f.EduType+f.OccupationType+sex+f.type+f.RaceType+f.CountryType), family=binomial,data=dfwork)

#summary(mYbinFactor3)

#Veiem que Residual deviance: 2257.0 , ha baixat força respecte.
#Null deviance: 4000.8  on 3673  degrees of freedom
#Residual deviance: 2257.0  on 3640  degrees of freedom
#AIC: 2325

#vif(mYbinFactor3)
#Mirem si realment afegint-hi aquests factors al model implica una millora:

#anova(mYbin,mYbinFactor3,test="Chisq")  #No son equivalents
#  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    
#1      3667     2891.1                          
#2      3640     2257.0 27   634.05 < 2.2e-16 ***
```
Veiem que al afegir els factors el model millora globalment.
Ara mirem si tots aquests factors afegits son realment significatius amb el test d'efectes nets:
```{r}
#Anova(mYbinFactor3,test="LR")

#Response: Y.bin
#                    LR Chisq Df Pr(>Chisq)    
#poly(age, 2)          74.895  2  < 2.2e-16 ***
#education.num          1.589  1   0.207487    
#capital.gain         182.885  1  < 2.2e-16 ***
#capital.loss          38.348  1  5.920e-10 ***
#hours.per.week        15.600  1  7.824e-05 ***
#f.MaritalStatusType   51.733  3  3.414e-11 ***
#f.RelType             21.140  3  9.848e-05 ***
#f.EduType              3.632  7   0.821073    
#f.OccupationType      33.192  6  9.630e-06 ***
#sex                    4.243  1   0.039417 *  
#f.type                 5.132  3   0.162376    
#f.RaceType             2.090  2   0.351637    
#f.CountryType         10.037  1   0.001534 **
```
Veiem que no, les que no son importants son les següents: f.EduType, f.type, f.RaceType.

Utilitzem el mètode step perque ens ajudi més ràpidament a netejar el model:
```{r}
#mYbinFactor4<-step(mYbinFactor3,k=log(nrow(dfwork)))
#Y.bin ~ poly(age, 2) + education.num + capital.gain + capital.loss + hours.per.week + f.MaritalStatusType + f.CountryType

#                      Df Deviance    AIC
#<none>                     2328.5 2418.8
#- f.CountryType        1   2345.0 2427.1
#- hours.per.week       1   2353.7 2435.8
#- capital.loss         1   2375.5 2457.6
#- poly(age, 2)         2   2419.0 2492.8
#- capital.gain         1   2535.9 2618.0
#- education.num        1   2615.4 2697.5
#- f.MaritalStatusType  3   2879.0 2944.7
```
Segons aquest criteri step, de tots els factors que hem intentat afegir en el model mYbinFactor3 veiem que només troba importants el f.MaritalStatusType i curiosament el f.CountryType.
Veiem que mYbinFactor4: Y.bin ~ poly(age, 2) + education.num + capital.gain + capital.loss + hours.per.week + f.MaritalStatusType + f.CountryType
```{r}
#anova(mYbinFactor4,mYbinFactor3,test="Chisq")  # No son equivalents,
#  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    
#1      3663     2328.5                          
#2      3640     2257.0 23   71.505 7.108e-07 ***
```
Tot i tenir el resultat obtingut per el mètode step, fem un model amb només aquelles variables factor que tenen uns efectes nets en mYbinFactor3 que son significatius, treiem 3 variables factor respecte el model mYbinFactor3, concretament: f.RaceType, f.type i f.EduType.
```{r}
mYbinFactor5<-glm(Y.bin ~ (poly(age,2) + education.num + capital.gain + capital.loss + hours.per.week)+(f.MaritalStatusType+f.RelType+f.OccupationType+sex+f.CountryType), family=binomial,data=dfwork)

#anova(mYbinFactor4,mYbinFactor5,test="Chisq")# No son equivalents
#  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    
#1      3663     2328.5                          
#2      3652     2267.4 11   61.093 5.805e-09 ***
```
Ens quedem amb el model mYbinFactor5, tot i que les variables afegides respecte el model mYbinFactor4 tenen força categories , concretament els f.RelType+f.OccupationType, creiem que al no haver acabat la modelització poden arribar a ser útils, ens ho podem permetre i tampoc porten una exagerada complexitat al model en termes de número de categories, i molt menys obviament la variable sex.

Mirem que no tinguem colinealitats
```{r}
#vif(mYbinFactor4)
#                        GVIF Df GVIF^(1/(2*Df))
#poly(age, 2)        1.140980  2        1.033522
#education.num       1.067905  1        1.033395
#capital.gain        1.043081  1        1.021314
#capital.loss        1.016487  1        1.008210
#hours.per.week      1.057020  1        1.028115
#f.MaritalStatusType 1.199714  3        1.030812
#f.CountryType       1.007150  1        1.003568

#vif(mYbinFactor5)
#                         GVIF Df GVIF^(1/(2*Df))
#poly(age, 2)         1.227012  2        1.052476
#education.num        1.414080  1        1.189151
#capital.gain         1.049536  1        1.024468
#capital.loss         1.026532  1        1.013179
#hours.per.week       1.175306  1        1.084115
#f.MaritalStatusType 32.506163  3        1.786464
#f.RelType           70.578492  3        2.032881
#f.OccupationType     1.734326  7        1.040114
#sex                  3.123928  1        1.767464
#f.CountryType        1.014009  1        1.006980
```
Veiem que no tenim colinealitats en el model que agafem finalment, el model mYbinFactor5, tot i que veiem que f.RelType i el f.MaritalStatusType tenen una certa relació com es obvi, inflen una mica la variança, pero res que sigui rellevant, no pasa de 3.
Actualitzem mYbin
```{r}
mYbin <- mYbinFactor5
summary(mYbin)
#Per tant, de moment el model agafat fins el moment es:
#(Y.bin ~ (poly(age, 2) + education.num + capital.gain + capital.loss + hours.per.week) + (f.MaritalStatusType + f.RelType + f.OccupationType + sex + f.CountryType), family = binomial, data = dfwork)

library(effects)
plot(allEffects(mYbin))
```
Comentem una mica els diferents gràfics per entendre millor el model.

age: A mesura que s'augmenta l'edat, la probabilitat de resposta positiva i guanyar més de 50k$ doncs es va incrementant, el pic està sobre els 50 anys, a partir d'aquest valor torna a disminuir. 

education.num: L'education.num pasa que a mesura que augmenten els anys d'estudis doncs aumenten la probabilitat de tenir uns ingressos per sobre de 50K

capital.gain: Si el capital.gain va creiexnt també ho fan els ingressos i com més doncs mé probabilitat de que ho siguin per sobre de 50k

hours.per.week:el capital.loss si tens pèrdues de una certa magnitud també es normal que passi perquè tens un ingressos més alts. com més capital.loss més probabilitat de guanyar 50k, tenim efecte lineal també. Pero veiem que hi han unes bandas una mica més amples i distorsionades.

sex:veiem una clara diferència en que si ets home tens més probabilitat de cobrar més de 50k de les que te una dona.

f.CountryType:Tenim que per individus que tinguin nacionalitat d'Estats Units hi han més probabilitat de que aquests guanyin més de 50k comparat amb els que no ho són

els gràfics per les variables f.MaritalStatusType, f.RelType, f.OccupationType no es veuen massa clars tal i com ens els treu el R i no podem massa cosa a partir dels gràfics que se'ns proporcionen, tot i això podem fer un petit comentari on els married comparats amb les altres categories de f.MaritalStatusType son els que tenen més probabilitats de guanyar +50k$ l'any. Per altre banda els que son f.Reltyp-Rel-NotFamily son dels que tenen més probabilitats i els Child com es obvi els que menys. Tot i així podem veure que en totes aquestes variables tenen una certa amplitud de forquilla.


Treballem el afegir interactions, seleccionarem el millor model, validarem, i veurem la seva capacitat predictiva

Per començar a afegir les interaccions, prendrem millor un model més simplificat que el últim mYbin actualitzat, que contenia més variables factors de les que proposava el mètode step. Des del punt de vista tècnic tenim una discrepància, tenim una estratègia de simplificació de models que està basada en el mètode step que ens recomana un determinat model, pero aquest model no es consistentm no es el mateix del que ens donen les eines d'inferència. Prenem la decisió de prendre el criteri de simplificació proposat per el mètode step per els pasos següents per simplificar el procès a part de ser el més aconsellat.

```{r}
mYbin <- mYbinFactor4
summary(mYbin)
#Per tant, de moment el model agafat fins el moment es:
#(Y.bin ~ poly(age, 2) + education.num + capital.gain + capital.loss + hours.per.week + f.MaritalStatusType + f.CountryType, family = binomial, data = dfwork)

library(effects)
plot(allEffects(mYbin))
```

##Afegint interaccions
```{r}
#(Y.bin ~ poly(age, 2) + education.num + capital.gain + capital.loss + hours.per.week + f.MaritalStatusType + f.CountryType, family = binomial, data = dfwork)
mYbinI15<-glm(Y.bin ~ (poly(age,2) + education.num + capital.gain + capital.loss + hours.per.week)*(f.MaritalStatusType + f.CountryType), family=binomial,data=dfwork)
summary(mYbinI15)
mYbinI16<-step(mYbinI15,k=log(nrow(dfwork)))
```
El millor model que obté es sense interaccions entre variables numèriques i factor, no son rellevants cap de les interaccions que hem intentat introduir, Y.bin ~ poly(age, 2) + education.num + capital.gain + capital.loss + hours.per.week + f.MaritalStatusType + f.CountryType

Afegim interaccions entre aquells factors que son rellevants en el model simplificat son el f.MaritalStatusType+f.CountryType, segons el metode de simplificació com hem comentat just al principi de l'apartat de "Afegint interaccions":
```{r}
mYbinI17<-glm(Y.bin ~ (poly(age,2) + education.num + capital.gain + capital.loss + hours.per.week)+(f.MaritalStatusType+f.CountryType)^2, family=binomial,data=dfwork)
mYbinI18<-step(mYbinI17,k=log(nrow(dfwork)))
```
Veiem que el mètode step precindeix en aquest cas de les interaccions entre factors.
```{r}
vif(mYbinI18)  # No tenim colinearitat
```
El model amb el que acabem l'estudi de afegir o no interaccions per tant es el següent:
(Y.bin ~ poly(age, 2) + education.num + capital.gain + capital.loss + hours.per.week + f.MaritalStatusType + f.CountryType, family = binomial, data = dfwork)

 Model Validation
En aquest cas no podem veure el coeficient de determinació ja que simplement no hi és.
```{r}
summary(mYbin)
```
Veiem que tenim un Residual deviance: 2328.5, te una distribució de shi quadrat 3663  degrees of freedom.
Si la Residual deviance de un model es de més o menys els graus de llibertat aleshores vol dir que el model s'ajusta bé a les dades. Veiem que tenim força diferencia, per tant el que ens està dient es que aquest model desde el punt de vista pràctic sembla que ajusta bé les dades, es un bon model per les dades.
Primer en la validació del model mirem si tenim desajustos entre les nostres dades i el model.
```{r}
marginalModelPlots(mYbin)
```
Veiem que per age, education.num i hours.per.week les dades s'ajusten perfectament al model. El capital.gain i el capital.loss no ho acaben de fer. Veiem que la capital.gain tenim un individu bastant especial a la part inferior dreta, on tenim un capital.gain elevat, i potser en part, es aquest individu que causa desajustos entre el model i les dades. Per altre banda com ja hem comentat foça cops te un comportament extrany. 
Globalment el model te un fit adequat.

Mirem els residual plots
```{r}
residualPlots(mYbin)
```
Aquí no mirem tema de patrons, cap normalitat. El que veiem es que hi ha una observació que te un residu atípic, un outlier dels residus. No sabem si serà influent o no, pero s'ha de mirar, ja que es una observació atípica. L'haurem de probablament descartar, ja que no podem construir un model amb aquestes variables que s'ajusti a la gran majoria de les observacions si aquesta observació està present.
```{r}
res.ii<-influencePlot(mYbin,id=list(method="noteworthy",n=5))
res.ii
dfwork[rownames(res.ii),]
```
Ens fixem per exemple amb l'indiviud 17040, que es el que te una cookD major i també un dels que te major StudRes.

Veiem que te 20 anys, es un home de raça Black i on veiem clarament el perque es una observació atípica es perque principalment te uns grans guanys amb una edat molt jove, en concret te exce ssius guanys (capital.gain = 34095), f.Edutyp-Colleges, f.Occupationtyp-Emp-AltCarrec, només treballa 10 hores per setmana, veiem que es en general una observació amb lack of fit en el conjunt de les dades.

Comentem una altre, per exemple el 15377, que te un cookD el segon més elevat amb un valor de 0.088891414 i un StudRes de -4.1501389. Veiem que es una observació que es un home de 55 anys, divorciat, veiem que te un capital.gain molt elevat de 34095, basicament per aquest capital.gain tant elevat es una observació a tenir molt en compte.

Seguim mirant i veiem que les observacions 10966 (dona de 52 anys, widowed, treballa només 20 hores i guanya més de 50k$ a l'any). 

Algunes de les observacions que hem estat analitzant no les considerem a eliminar perque no les hem trobat amb valors gens extranys excessivament.
```{r}
ll<-which(rownames(dfwork) %in% c("17040","15377","10966"))
#Recalculem el model:
mYbinI19<-glm(Y.bin ~ poly(age, 2) + education.num + capital.gain + capital.loss + hours.per.week + f.MaritalStatusType + f.CountryType, family=binomial,data=dfwork[-ll,])
mYbinI20<-step(mYbinI19,k=log(nrow(dfwork)))
res.ii2<-influencePlot(mYbinI20,id=list(method="noteworthy",n=5))
#Veiem que ja hem eliminat les observacions que voliem.
mYbin<-mYbinI20
residualPlots(mYbin)
```
Ara veiem que la gràfica de capital.gain millora molt clarament, ens hem de fixar amb la linia de color rosa, quan aquesta es plana es que el model està força bé, que es el nostre cas en totes les variables explicatives.

Mirem el marginalModelPlots i ara si que veiem que hem millorat molt l'ajustament de les dades i el model en capital.gain que es on teniem més desajust, ja que age, education.num i hours.per.week ja estàven molt ben ajustades.
```{r}
marginalModelPlots(mYbin)
```

## Confusion table
Ara el que fem es un cop tenim el millor model validat i veiem que les dades s'ajusten bé a aquest el que fem es quantificar-ho, la millor manera de fer-ho es utilitzar confusion table. Fem una confusion table amb la qual cosa el que fem es la predicció per totes les observacions que tenim en la mostra de treball dfwork de les probabilitats tipo resposta que tenen de guanyar més de 50k$ a l'any.

millor i final model: mYbin

Confusion table per la mostra dfwork utilitzant el millor model fins al moment mYbin:

Agafem les 10 primeres observacions de la nostra mostra de treball i mirem quina predicció fa el nostre model final mYbin sobre aquestes:
```{r}
predict(mYbin,type="response")[1:10]
#          1           2           3          20          37          40 
#0.206493323 0.548593136 0.027476397 0.206229598 0.005747426 0.628794236 
#         42          65          85          95 
#0.707247241 0.029336326 0.897523450 0.559212057 
```
Hem de definir un llindar a partir del qual transformem aquestes probabilitats en una resposta positiva o negativa. Diem que els que tenen una probabilitat inferior de 0.5 aleshores son observacions que no tenen resposta positiva, per tant guanyen menys de 50k$, els que tenen una probabilitat més gran de 0.5 pertanyen al grup de +50k$.
Creem un factor amb tantes observacions com dimensió te la dfwork.
```{r}
premYbin<-factor(ifelse(predict(mYbin,newdata=dfwork,type="response")<0.5,0,1),labels=c("<50k$/any Pred","+50k$/any Pred"))
tYbin<-table(premYbin,dfwork$Y.bin)
tYbin
```
Veiem que en columnes tenim la realitat de la mostra i les files son les prediccions del nostre model mYbin. Veiem que tenim 2643 observacions que guanyen en la realitat menys de 50k$ i el model també prediu que guanyen menys de 50k$, per tant, l'encerta. Després veiem la segona de les columnes que son les que guanyen més de 50k$ i tenim que el model també diu que guanyen més de 50k$ en 494 observacions, també l'encerta. 
El problema està fora d'aquesta diagonal, el 367 son gent que guanya realment més de 50k pero que el nostre model prediu que en guanyen menys, per tant s'ha equivocat, el mateix pasa amb els 170 que son gent que guanya menys de 50k$ l'any i que el nostre model prediu que en guanyen més.

premYbin         <=50K >50K
  <50k$/any Pred  2643  367
  +50k$/any Pred   170  494

La qualitat predictiva del model la podem obtenir suman les diagonals i dividint per el nombre d'observacions:
La capacitat predictiva del model mYbin es del 85.38378%
```{r}
capamYbin<-100*(sum(diag(tYbin))/nrow(dfwork));capamYbin
```
Tot i això el que ens interesa saber es si aquesta capacitat predictiva es casualitat o de fet es una conseqüència de que hem treballat bé el model.

Veiem quin seria el valor corresponent al encertar en el cas del model null. És aquell model que fa la predicció per tots els individus que no guanyen més de 50k$ l'any (representen la majoria).
```{r}
mYbinNULL<-glm(Y.bin~1,family=binomial,data=dfwork)
predict(mYbinNULL,type="response")[1:10]
premYbinNULL<-factor(ifelse(predict(mYbinNULL,type="response")<0.5,0,1),labels=c("<50k$/any Pred"))
tYbinNULL<-table(premYbinNULL,dfwork$Y.bin)
tYbinNULL
```
Veiem que tenim 2813 que guanyen a la realitat menys de 50k$, i la predicció del model null encerta, ens diu el mateix, pero per altre banda, tots els que guanyen més estan mal predits.
premYbinNULL     <=50K >50K
  <50k$/any Pred  2813  861

Mirem la capacitat predictiva d'aquest model null, calculem els que encerta respecte el total:
```{r}
capamYbinNULL<-100*(tYbinNULL[1,1]/nrow(dfwork));capamYbinNULL
```
Veiem que encerta el 76.56505% dels casos. Concloem que després de la feina de modelització hem aconseguit introduir una millora en la capacitat predictiva del model del 10%.

Veiem com es comportaria el nostre model si haguès de fer la predicció sobre el data frame dftest. Fem el exactament el mateix procediment que anteriorment pero amb dftest en comptes de dfwork.

 Confusion table for work sample, using Final Model
```{r}
predict(mYbin,newdata=dftest,type="response")[1:10]
premYbinTEST<-factor(ifelse(predict(mYbin,newdata=dftest,type="response")<0.5,0,1),labels=c("<50k$/any Pred","+50k$/any Pred"))
tYbinTEST<-table(premYbinTEST,dftest$Y.bin);tYbinTEST
#premYbinTEST     <=50K >50K
#  <50k$/any Pred   871  122
#  +50k$/any Pred    71  161
capamYbinTEST<-100*(sum(diag(tYbinTEST))/nrow(dftest));capamYbinTEST
```
Veiem que encerta el 84.2449% dels casos, ha minvat al voltant del 1%.
Ens fixem que ara el percentatge d'encert en la mostra dftest que no s'ha utilitzat en la construcció del model, si hi ha grans diferencies entre la mostra de test i la mostra de treball vol dir que el model està sobreparametritzat, en el nostre cas no es així.

Després de l'anàlisi de capacitat predictiva veiem les ROC curve, ens permeten evaluar la capacitat predictiva d'un model en termes de discriminar un individu positiu i un de negatiu.
 
Utilitzem ROC curve
```{r}
# ROC Curve
library("ROCR")
#Transformem les dades per tenir-les llestes per aplicar els mètodes perfomance de la library ROCR.
dadesROC<-prediction(predict(mYbin,newdata=dfwork,type="response"),dfwork$Y.bin)
```
Volem obtenir el plot performance de l'error segons la probabilitat, el cutoff es el llindar que ens diu quan determinem que es resposta positiva o negativa. Veiem que es en el valor sobre el 0.4 on obtenim la millor de les clasificacions. Nosaltres anteriorment hem posat 0.5, que igualment a simple vista tindria un error rate baix, molt semblant al de 0.4.
```{r}
par(mfrow=c(1,2))
plot(performance(dadesROC,"err"))
```
Ara el que volem veure es el gràfic que es veritablament la corva ROC, que bàsicament el que ens fa es que relaciona els falsos positius amb els true positius. Dóna una idea de la capacitat discriminant del model.
L'àrea sota la ROC curve indica la capacitat discriminant del model, finalitzem dient que veiem que en el nostre cas es força bo.
```{r}
plot(performance(dadesROC,"tpr","fpr"))
abline(0,1,lty=2)
```






